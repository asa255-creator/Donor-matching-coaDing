/****************************************************
 * Donor Matcher v3 + Local Runner Adapter
 * Safe to include without renaming your existing menu glue
 * Exposes:
 *   donor_createTrainingPairs
 *   donor_trainMatcher
 *   donor_assignDonorIds
 *   donor_addUncertainPairs
 *   donor_createTrainingPairs_debug
 *   donor_createTrainingPairs_lite
 *   dl_prepareLocalJobAndShowCommand_
 *   dl_showProgressSidebar
 *   diagnostic_analyzeMatching (NEW - add to your menu)
 *   cd_uploadDialog (NEW - add to your menu)
 *   cd_prepareMatchingJob_ (NEW - add to your menu)
 *   doGet
 *   doPost
 ****************************************************/

/** =========================
 * Donor Matcher v3 config
 * ========================= */
const DONOR_CFG = {
  krefSheet: 'KREF_Exports',
  fecSheet: 'FEC_Exports',
  trainingSheet: 'Training',
  idColumnName: 'DonorID',
  sampleTrainingPairs: 20,
  initialSamplePairs: 20,
  uncertainBatchSize: 10,
  uncertaintyBand: 0.15,
  predictThreshold: 0.7,
  maxPairsPerBlock: 4000,
  maxTotalPairs: 200000,
  learningRate: 0.1,
  maxTrainIterations: 400
};

/** =========================
 * Training sheet header
 * ========================= */
const DONOR_TRAINING_HEADER = [
  'Label',
  'sheet_a','row_a','first_a','last_a','addr_a','city_a','state_a','zip_a','emp_a','occ_a',
  'sheet_b','row_b','first_b','last_b','addr_b','city_b','state_b','zip_b','emp_b','occ_b',
  'feat_name','feat_last','feat_addr','feat_emp','feat_occ','feat_zip','feat_city',
  'feat_name_last','feat_name_addr','feat_name_emp','feat_name_occ',
  'feat_last_addr','feat_last_emp','feat_last_occ',
  'feat_addr_emp','feat_addr_occ','feat_emp_occ'
];

/** =========================
 * UNIFIED MATCHING MODULE
 * This Python code is injected into all matching scripts
 * ========================= */
const UNIFIED_MATCHING_MODULE = [
  '# ============================================',
  '# UNIFIED MATCHING MODULE',
  '# Shared by: Training, Diagnostic, Campaign Deputy',
  '# ============================================',
  '',
  'import re',
  'import math',
  'from collections import defaultdict',
  '',
  '# ----------------------------',
  '# STATE ABBREVIATIONS',
  '# ----------------------------',
  'STATE_ABBREV = {',
  '    "ALABAMA": "AL", "ALASKA": "AK", "ARIZONA": "AZ", "ARKANSAS": "AR", "CALIFORNIA": "CA",',
  '    "COLORADO": "CO", "CONNECTICUT": "CT", "DELAWARE": "DE", "FLORIDA": "FL", "GEORGIA": "GA",',
  '    "HAWAII": "HI", "IDAHO": "ID", "ILLINOIS": "IL", "INDIANA": "IN", "IOWA": "IA",',
  '    "KANSAS": "KS", "KENTUCKY": "KY", "LOUISIANA": "LA", "MAINE": "ME", "MARYLAND": "MD",',
  '    "MASSACHUSETTS": "MA", "MICHIGAN": "MI", "MINNESOTA": "MN", "MISSISSIPPI": "MS", "MISSOURI": "MO",',
  '    "MONTANA": "MT", "NEBRASKA": "NE", "NEVADA": "NV", "NEW HAMPSHIRE": "NH", "NEW JERSEY": "NJ",',
  '    "NEW MEXICO": "NM", "NEW YORK": "NY", "NORTH CAROLINA": "NC", "NORTH DAKOTA": "ND", "OHIO": "OH",',
  '    "OKLAHOMA": "OK", "OREGON": "OR", "PENNSYLVANIA": "PA", "RHODE ISLAND": "RI", "SOUTH CAROLINA": "SC",',
  '    "SOUTH DAKOTA": "SD", "TENNESSEE": "TN", "TEXAS": "TX", "UTAH": "UT", "VERMONT": "VT",',
  '    "VIRGINIA": "VA", "WASHINGTON": "WA", "WEST VIRGINIA": "WV", "WISCONSIN": "WI", "WYOMING": "WY",',
  '    "DISTRICT OF COLUMBIA": "DC", "PUERTO RICO": "PR", "GUAM": "GU", "VIRGIN ISLANDS": "VI"',
  '}',
  '',
  '# ----------------------------',
  '# ADDRESS ABBREVIATIONS',
  '# ----------------------------',
  'ADDRESS_ABBREV = {',
  '    "RD": "ROAD", "ST": "STREET", "AVE": "AVENUE", "BLVD": "BOULEVARD", "DR": "DRIVE",',
  '    "LN": "LANE", "CT": "COURT", "CIR": "CIRCLE", "PL": "PLACE", "PKWY": "PARKWAY",',
  '    "HWY": "HIGHWAY", "APT": "APARTMENT", "STE": "SUITE", "FL": "FLOOR", "TER": "TERRACE",',
  '    "TRL": "TRAIL", "WAY": "WAY", "EXPY": "EXPRESSWAY", "FWY": "FREEWAY",',
  '    "N": "NORTH", "S": "SOUTH", "E": "EAST", "W": "WEST",',
  '    "NE": "NORTHEAST", "NW": "NORTHWEST", "SE": "SOUTHEAST", "SW": "SOUTHWEST",',
  '    "PO": "POST OFFICE", "MT": "MOUNT", "FT": "FORT"',
  '}',
  '',
  '# ----------------------------',
  '# NORMALIZATION FUNCTIONS',
  '# ----------------------------',
  'def normalize_state(state):',
  '    """Normalize state to 2-letter uppercase code."""',
  '    if not state:',
  '        return ""',
  '    state = str(state).strip().upper()',
  '    # Already 2-letter code',
  '    if len(state) == 2 and state.isalpha():',
  '        return state',
  '    # Full name lookup',
  '    return STATE_ABBREV.get(state, state[:2] if len(state) >= 2 else state)',
  '',
  'def normalize_zip(z):',
  '    """Normalize ZIP to 5 digits."""',
  '    z = str(z or "").strip()',
  '    # Remove ZIP+4',
  '    if "-" in z:',
  '        z = z.split("-")[0]',
  '    # Extract digits only',
  '    digits = "".join(c for c in z if c.isdigit())',
  '    return digits[:5] if len(digits) >= 5 else digits',
  '',
  'def normalize_address(addr):',
  '    """Normalize address: uppercase, remove periods, expand abbreviations."""',
  '    if not addr:',
  '        return ""',
  '    addr = str(addr).strip().upper()',
  '    # Remove periods and common punctuation',
  '    addr = re.sub(r"[.,#;]", "", addr)',
  '    # Expand abbreviations',
  '    words = addr.split()',
  '    normalized = []',
  '    for word in words:',
  '        normalized.append(ADDRESS_ABBREV.get(word, word))',
  '    return " ".join(normalized)',
  '',
  'def normalize_name(name):',
  '    """Normalize name: uppercase, strip whitespace."""',
  '    if not name:',
  '        return ""',
  '    return str(name).strip().upper()',
  '',
  '# ----------------------------',
  '# SIMILARITY ALGORITHMS',
  '# ----------------------------',
  'def jaro_winkler(a, b):',
  '    """Jaro-Winkler similarity (good for typos, prefix matching)."""',
  '    a = normalize_name(a)',
  '    b = normalize_name(b)',
  '    if not a and not b:',
  '        return 1.0',
  '    if not a or not b:',
  '        return 0.0',
  '    if a == b:',
  '        return 1.0',
  '    m_dist = max(0, max(len(a), len(b)) // 2 - 1)',
  '    a_m = [False] * len(a)',
  '    b_m = [False] * len(b)',
  '    matches = 0',
  '    for i in range(len(a)):',
  '        start = max(0, i - m_dist)',
  '        end = min(i + m_dist + 1, len(b))',
  '        for j in range(start, end):',
  '            if b_m[j] or a[i] != b[j]:',
  '                continue',
  '            a_m[i] = True',
  '            b_m[j] = True',
  '            matches += 1',
  '            break',
  '    if matches == 0:',
  '        return 0.0',
  '    t = 0',
  '    k = 0',
  '    for i in range(len(a)):',
  '        if not a_m[i]:',
  '            continue',
  '        while not b_m[k]:',
  '            k += 1',
  '        if a[i] != b[k]:',
  '            t += 1',
  '        k += 1',
  '    t = t / 2.0',
  '    j = (matches / len(a) + matches / len(b) + (matches - t) / matches) / 3.0',
  '    l = 0',
  '    while l < 4 and l < len(a) and l < len(b) and a[l] == b[l]:',
  '        l += 1',
  '    return j + l * 0.1 * (1.0 - j)',
  '',
  'def levenshtein(s1, s2):',
  '    """Levenshtein edit distance."""',
  '    s1 = normalize_name(s1)',
  '    s2 = normalize_name(s2)',
  '    if len(s1) < len(s2):',
  '        return levenshtein(s2, s1)',
  '    if len(s2) == 0:',
  '        return len(s1)',
  '    previous_row = range(len(s2) + 1)',
  '    for i, c1 in enumerate(s1):',
  '        current_row = [i + 1]',
  '        for j, c2 in enumerate(s2):',
  '            insertions = previous_row[j + 1] + 1',
  '            deletions = current_row[j] + 1',
  '            substitutions = previous_row[j] + (c1 != c2)',
  '            current_row.append(min(insertions, deletions, substitutions))',
  '        previous_row = current_row',
  '    return previous_row[-1]',
  '',
  'def soundex(s):',
  '    """Soundex phonetic encoding."""',
  '    s = normalize_name(s)',
  '    if not s:',
  '        return "0000"',
  '    s = "".join(c for c in s if c.isalpha())',
  '    if not s:',
  '        return "0000"',
  '    soundex_map = {',
  '        "B": "1", "F": "1", "P": "1", "V": "1",',
  '        "C": "2", "G": "2", "J": "2", "K": "2", "Q": "2", "S": "2", "X": "2", "Z": "2",',
  '        "D": "3", "T": "3",',
  '        "L": "4",',
  '        "M": "5", "N": "5",',
  '        "R": "6"',
  '    }',
  '    result = s[0]',
  '    prev_code = soundex_map.get(s[0], "0")',
  '    for char in s[1:]:',
  '        code = soundex_map.get(char, "0")',
  '        if code != "0" and code != prev_code:',
  '            result += code',
  '        prev_code = code',
  '        if len(result) == 4:',
  '            break',
  '    return (result + "000")[:4]',
  '',
  '# ----------------------------',
  '# ENHANCED SIMILARITY (multi-algorithm + associations)',
  '# ----------------------------',
  'def enhanced_similarity(str1, str2, field_type, associations=None):',
  '    """',
  '    Multi-algorithm similarity scoring with learned associations.',
  '    field_type: "name", "first", "last", "address", "employer", "occupation"',
  '    """',
  '    str1 = normalize_name(str1)',
  '    str2 = normalize_name(str2)',
  '    ',
  '    # For name fields, check nickname database first',
  '    if field_type in ["name", "first"]:',
  '        # Extract first names for nickname check',
  '        first1 = str1.split()[0] if str1.split() else ""',
  '        first2 = str2.split()[0] if str2.split() else ""',
  '        if check_nickname(first1, first2):',
  '            return 0.95  # High score for nickname match',
  '    ',
  '    # Check learned associations',
  '    if associations and check_association(str1, str2, associations):',
  '        return 0.98',
  '    ',
  '    # Exact match',
  '    if str1 == str2:',
  '        return 1.0',
  '    ',
  '    # Empty strings',
  '    if not str1 and not str2:',
  '        return 1.0',
  '    if not str1 or not str2:',
  '        return 0.0',
  '    ',
  '    # 1. Jaro-Winkler',
  '    jw_score = jaro_winkler(str1, str2)',
  '    ',
  '    # 2. Levenshtein normalized',
  '    max_len = max(len(str1), len(str2))',
  '    lev_distance = levenshtein(str1, str2)',
  '    lev_score = 1.0 - (lev_distance / max_len) if max_len > 0 else 1.0',
  '    ',
  '    # 3. Token Jaccard',
  '    tokens1 = set(str1.split())',
  '    tokens2 = set(str2.split())',
  '    token_jaccard = len(tokens1 & tokens2) / len(tokens1 | tokens2) if (tokens1 or tokens2) else 0.0',
  '    ',
  '    # 4. Combine based on field type',
  '    if field_type in ["name", "first", "last"]:',
  '        phonetic_match = 1.0 if soundex(str1) == soundex(str2) else 0.0',
  '        base_score = jw_score * 0.35 + lev_score * 0.25 + token_jaccard * 0.2 + phonetic_match * 0.2',
  '        return max(base_score, phonetic_match * 0.8)',
  '    elif field_type == "address":',
  '        return jw_score * 0.4 + lev_score * 0.3 + token_jaccard * 0.3',
  '    else:',
  '        return jw_score * 0.5 + lev_score * 0.3 + token_jaccard * 0.2',
  '',
  'def check_association(term1, term2, associations, threshold=2):',
  '    """Check if two terms are associated based on learned co-occurrences."""',
  '    t1 = normalize_name(term1)',
  '    t2 = normalize_name(term2)',
  '    if t1 == t2:',
  '        return True',
  '    if t1 in associations and t2 in associations[t1]:',
  '        return associations[t1][t2] >= threshold',
  '    return False',
  '',
  '# Global association dictionaries (populated after loading training data)',
  'first_associations = {}',
  'last_associations = {}',
  'address_associations = {}',
  'city_associations = {}',
  'state_associations = {}',
  'zip_associations = {}',
  'employer_associations = {}',
  'occupation_associations = {}',
  '',
  '# Nickname database (populated from external source)',
  'nickname_db = {}',
  '',
  'def check_nickname(name1, name2):',
  '    """Check if two names are nicknames of each other."""',
  '    n1 = normalize_name(name1)',
  '    n2 = normalize_name(name2)',
  '    if not n1 or not n2:',
  '        return False',
  '    if n1 == n2:',
  '        return True',
  '    if n1 in nickname_db and n2 in nickname_db[n1]:',
  '        return True',
  '    if n2 in nickname_db and n1 in nickname_db[n2]:',
  '        return True',
  '    return False',
  '',
  'def load_nickname_db(base_url):',
  '    """Load nickname database from web app endpoint."""',
  '    global nickname_db',
  '    try:',
  '        nicknames_url = base_url.replace("model=1", "nicknames=1")',
  '        nicknames_json = http_get(nicknames_url).decode("utf-8")',
  '        nicknames_data = json.loads(nicknames_json)',
  '        nickname_db = nicknames_data.get("nicknames", {})',
  '        total_nicknames = sum(len(v) for v in nickname_db.values()) // 2',
  '        print(f"Loaded {total_nicknames} nickname pairs from database")',
  '    except Exception as e:',
  '        print(f"Warning: Could not load nicknames: {e}")',
  '        nickname_db = {}',
  '',
  '# ----------------------------',
  '# FEATURE CALCULATION (17 features)',
  '# ----------------------------',
  'def calculate_features(a, b, field_map_a=None, field_map_b=None):',
  '    """',
  '    Calculate 17 features for a pair of records.',
  '    field_map allows different column names for different sources.',
  '    """',
  '    # Default field mapping',
  '    default_map = {',
  '        "first": "DonorFirst", "last": "DonorLast", "addr": "Address1",',
  '        "city": "City", "state": "State", "zip": "Zip",',
  '        "employer": "Employer", "occupation": "Occupation"',
  '    }',
  '    map_a = field_map_a or default_map',
  '    map_b = field_map_b or field_map_a or default_map',
  '    ',
  '    # Extract and normalize fields',
  '    first_a = normalize_name(a.get(map_a["first"], ""))',
  '    first_b = normalize_name(b.get(map_b["first"], ""))',
  '    last_a = normalize_name(a.get(map_a["last"], ""))',
  '    last_b = normalize_name(b.get(map_b["last"], ""))',
  '    addr_a = normalize_address(a.get(map_a["addr"], ""))',
  '    addr_b = normalize_address(b.get(map_b["addr"], ""))',
  '    city_a = normalize_name(a.get(map_a["city"], ""))',
  '    city_b = normalize_name(b.get(map_b["city"], ""))',
  '    zip_a = normalize_zip(a.get(map_a["zip"], ""))',
  '    zip_b = normalize_zip(b.get(map_b["zip"], ""))',
  '    emp_a = normalize_name(a.get(map_a["employer"], ""))',
  '    emp_b = normalize_name(b.get(map_b["employer"], ""))',
  '    occ_a = normalize_name(a.get(map_a["occupation"], ""))',
  '    occ_b = normalize_name(b.get(map_b["occupation"], ""))',
  '    ',
  '    # Full name',
  '    full_a = (first_a + " " + last_a).strip()',
  '    full_b = (first_b + " " + last_b).strip()',
  '    ',
  '    # Base features [0-4]',
  '    f0 = enhanced_similarity(full_a, full_b, "name", first_associations)',
  '    f1 = enhanced_similarity(last_a, last_b, "last", last_associations)',
  '    f2 = enhanced_similarity(addr_a, addr_b, "address", address_associations)',
  '    f3 = enhanced_similarity(emp_a, emp_b, "employer", employer_associations)',
  '    f4 = enhanced_similarity(occ_a, occ_b, "occupation", occupation_associations)',
  '    ',
  '    # Geographic exact matches [5-6]',
  '    f5 = 1.0 if (zip_a and zip_b and zip_a == zip_b) else 0.0',
  '    f6 = 1.0 if (city_a and city_b and city_a == city_b) else 0.0',
  '    ',
  '    # Interaction features [7-16]',
  '    f7 = f0 * f1    # name * last',
  '    f8 = f0 * f2    # name * address',
  '    f9 = f0 * f3    # name * employer',
  '    f10 = f0 * f4   # name * occupation',
  '    f11 = f1 * f2   # last * address',
  '    f12 = f1 * f3   # last * employer',
  '    f13 = f1 * f4   # last * occupation',
  '    f14 = f2 * f3   # address * employer',
  '    f15 = f2 * f4   # address * occupation',
  '    f16 = f3 * f4   # employer * occupation',
  '    ',
  '    return [f0, f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15, f16]',
  '',
  '# ----------------------------',
  '# PREDICTION',
  '# ----------------------------',
  'def sigmoid(z):',
  '    """Sigmoid activation function."""',
  '    if z < -500:',
  '        return 0.0',
  '    if z > 500:',
  '        return 1.0',
  '    return 1.0 / (1.0 + math.exp(-z))',
  '',
  'def predict(a, b, weights, field_map_a=None, field_map_b=None):',
  '    """Calculate match probability for a pair."""',
  '    x = calculate_features(a, b, field_map_a, field_map_b)',
  '    z = weights[-1]  # bias',
  '    for i in range(len(x)):',
  '        z += weights[i] * x[i]',
  '    return sigmoid(z)',
  '',
  '# ----------------------------',
  '# FIELD-BY-FIELD MERGE',
  '# ----------------------------',
  'def merge_records(a, b, date_field="Date", source_field="source"):',
  '    """',
  '    Merge two matching records, keeping best data for each field.',
  '    Priority: Campaign Deputy > Most recent date > Non-empty',
  '    """',
  '    merged = {}',
  '    ',
  '    # Determine which record has priority for non-empty conflicts',
  '    a_is_cd = a.get(source_field) == "CD"',
  '    b_is_cd = b.get(source_field) == "CD"',
  '    ',
  '    # Parse dates (CD always wins, so date only matters for non-CD records)',
  '    a_date = a.get(date_field, "")',
  '    b_date = b.get(date_field, "")',
  '    ',
  '    # Determine priority record for conflicts',
  '    if a_is_cd:',
  '        priority = a',
  '        secondary = b',
  '    elif b_is_cd:',
  '        priority = b',
  '        secondary = a',
  '    elif a_date >= b_date:  # String comparison works for ISO dates',
  '        priority = a',
  '        secondary = b',
  '    else:',
  '        priority = b',
  '        secondary = a',
  '    ',
  '    # Merge each field',
  '    all_keys = set(a.keys()) | set(b.keys())',
  '    for key in all_keys:',
  '        val_a = str(a.get(key, "") or "").strip()',
  '        val_b = str(b.get(key, "") or "").strip()',
  '        ',
  '        if not val_a and not val_b:',
  '            merged[key] = ""',
  '        elif not val_a:',
  '            merged[key] = val_b',
  '        elif not val_b:',
  '            merged[key] = val_a',
  '        else:',
  '            # Both have data - use priority record',
  '            merged[key] = priority.get(key, secondary.get(key, ""))',
  '    ',
  '    # Preserve donation history if tracking',
  '    if "donations" in a or "donations" in b:',
  '        merged["donations"] = a.get("donations", []) + b.get("donations", [])',
  '    ',
  '    return merged',
  '',
  '# ----------------------------',
  '# DYNAMIC BLOCK PROCESSING',
  '# ----------------------------',
  'def process_block_with_merging(block, weights, threshold, field_map=None):',
  '    """',
  '    Process a block with dynamic passes and merge-back.',
  '    Returns list of deduplicated/merged records.',
  '    """',
  '    if len(block) <= 1:',
  '        return block',
  '    ',
  '    records = list(block)  # Copy',
  '    comparison_limit = 50  # Start with 50 comparisons per pass',
  '    ',
  '    while True:',
  '        total_possible = len(records) * (len(records) - 1) // 2',
  '        if total_possible == 0:',
  '            break',
  '        ',
  '        comparisons_done = 0',
  '        merged_any = False',
  '        ',
  '        # Compare pairs up to limit',
  '        i = 0',
  '        while i < len(records) and not merged_any:',
  '            j = i + 1',
  '            while j < len(records) and comparisons_done < comparison_limit:',
  '                comparisons_done += 1',
  '                score = predict(records[i], records[j], weights, field_map, field_map)',
  '                ',
  '                if score >= threshold:',
  '                    # Merge and put back',
  '                    merged = merge_records(records[i], records[j])',
  '                    records.pop(j)',
  '                    records.pop(i)',
  '                    records.append(merged)',
  '                    merged_any = True',
  '                    break',
  '                j += 1',
  '            i += 1',
  '        ',
  '        if not merged_any:',
  '            # No merges this pass',
  '            if comparison_limit >= total_possible:',
  '                break  # Done - checked everything',
  '            comparison_limit = min(comparison_limit * 2, total_possible)',
  '    ',
  '    return records',
  '',
  '# ----------------------------',
  '# BLOCKING',
  '# ----------------------------',
  'def create_blocking_key(record, field_map=None):',
  '    """Create blocking key from soundex of last name."""',
  '    default_map = {"last": "DonorLast"}',
  '    fm = field_map or default_map',
  '    last = normalize_name(record.get(fm["last"], ""))',
  '    return soundex(last)',
  '',
  '# ============================================',
  '# END UNIFIED MATCHING MODULE',
  '# ============================================',
  ''
].join('\n');

/** =========================
 * Step 1: Create Training Pairs
 * ========================= */
function donor_createTrainingPairs() {
  const rows = donor_loadAllRows_();
  const blocks = donor_buildBlocks_(rows);
  const pairs = donor_generatePairsFromBlocks_(blocks);
  const sampled = donor_sampleArray_(pairs, DONOR_CFG.sampleTrainingPairs);
  donor_writeTrainingSheet_(sampled);
  SpreadsheetApp.getActive().toast('Training sheet ready. Label 1 or 0, then run Train matcher.', 'Donor Matcher', 8);
}

/** =========================
 * Step 2: Train Matcher
 * ========================= */
function donor_trainMatcher() {
  const sh = donor_getOrCreateSheet_(DONOR_CFG.trainingSheet);
  const vals = sh.getDataRange().getValues();
  if (vals.length < 2) {
    SpreadsheetApp.getUi().alert('No training data found. Run Create training pairs.');
    return;
  }
  const h = vals[0].map(s => String(s || '').trim());
  function idx(name) {
    const i = h.indexOf(name);
    if (i === -1) throw new Error('Missing column: ' + name);
    return i;
  }
  const featureCols = [
    'feat_name','feat_last','feat_addr','feat_emp','feat_occ','feat_zip','feat_city',
    'feat_name_last','feat_name_addr','feat_name_emp','feat_name_occ',
    'feat_last_addr','feat_last_emp','feat_last_occ',
    'feat_addr_emp','feat_addr_occ','feat_emp_occ'
  ];
  const I = featureCols.map(idx);
  const iLabel = idx('Label');

  const X = [];
  const y = [];
  for (let r = 1; r < vals.length; r++) {
    const row = vals[r];
    const label = Number(row[iLabel]);
    if (label !== 0 && label !== 1) continue;
    const feat = [];
    for (let k = 0; k < I.length; k++) feat.push(Number(row[I[k]] || 0));
    X.push(feat);
    y.push(label);
  }
  if (!X.length) {
    SpreadsheetApp.getUi().alert('No labeled rows. Set Label to 1 or 0.');
    return;
  }
  const w = donor_fitLogReg_(X, y, DONOR_CFG.learningRate, DONOR_CFG.maxTrainIterations);
  PropertiesService.getDocumentProperties().setProperty('donor_model_weights', JSON.stringify(w));
  donor_buildAndStoreFirstNamePairTable_();
  SpreadsheetApp.getUi().alert('Training complete. Weights and first name table saved.');
}

/** =========================
 * Step 3: Assign Donor IDs
 * ========================= */
function donor_assignDonorIds() {
  const wRaw = PropertiesService.getDocumentProperties().getProperty('donor_model_weights');
  if (!wRaw) {
    SpreadsheetApp.getUi().alert('No trained model found. Run Train matcher first.');
    return;
  }
  const w = JSON.parse(wRaw);
  const rows = donor_loadAllRows_();
  const blocks = donor_buildBlocks_(rows);
  const pairs = donor_generatePairsFromBlocks_(blocks);
  const uf = donor_newUnionFind_(rows.length);
  let kept = 0;
  for (let i = 0; i < pairs.length; i++) {
    const f = pairs[i].features;
    const x = [
      f.nameSim, f.lastSim, f.addrSim, f.empSim, f.occSim, f.zipMatch, f.cityMatch,
      f.name_last, f.name_addr, f.name_emp, f.name_occ,
      f.last_addr, f.last_emp, f.last_occ,
      f.addr_emp, f.addr_occ, f.emp_occ
    ];
    const prob = donor_predictLogReg_(x, w);
    if (prob >= DONOR_CFG.predictThreshold) {
      donor_union_(uf, pairs[i].aIdx, pairs[i].bIdx);
      kept++;
    }
  }
  const comps = donor_components_(uf);
  const idMap = new Map();
  let nextId = 1;
  for (let c = 0; c < comps.length; c++) {
    const comp = comps[c];
    for (let j = 0; j < comp.length; j++) idMap.set(comp[j], nextId);
    nextId++;
  }
  donor_writeIdsToSheets_(rows, idMap);
  SpreadsheetApp.getActive().toast('Assigned DonorIDs. Matches kept: ' + kept, 'Donor Matcher', 8);
}

/** =========================
 * Step 4: Iterative labeling helper
 * ========================= */
function donor_addUncertainPairs() {
  const weightsRaw = PropertiesService.getDocumentProperties().getProperty('donor_model_weights');
  const rows = donor_loadAllRows_();
  const blocks = donor_buildBlocks_(rows);
  const pairs = donor_generatePairsFromBlocks_(blocks);

  if (!weightsRaw) {
    const seed = donor_sampleArray_(pairs, DONOR_CFG.initialSamplePairs);
    donor_appendPairsToTraining_(seed);
    SpreadsheetApp.getUi().alert('Added ' + seed.length + ' seed pairs. Label them, train, then run again.');
    return;
  }
  const w = JSON.parse(weightsRaw);
  const seen = donor_getSeenPairKeys_();
  const band = DONOR_CFG.uncertaintyBand;
  const lower = 0.5 - band;
  const upper = 0.5 + band;
  const candidates = [];
  for (let i = 0; i < pairs.length; i++) {
    const p = pairs[i];
    const key = donor_pairKey_(p.a, p.b);
    if (seen.has(key)) continue;
    const f = p.features;
    const x = [
      f.nameSim, f.lastSim, f.addrSim, f.empSim, f.occSim, f.zipMatch, f.cityMatch,
      f.name_last, f.name_addr, f.name_emp, f.name_occ,
      f.last_addr, f.last_emp, f.last_occ,
      f.addr_emp, f.addr_occ, f.emp_occ
    ];
    const prob = donor_predictLogReg_(x, w);
    if (prob >= lower && prob <= upper) {
      candidates.push({ pair: p, uncertainty: Math.abs(prob - 0.5) });
    }
  }
  candidates.sort((a, b) => a.uncertainty - b.uncertainty);
  const take = Math.min(DONOR_CFG.uncertainBatchSize, candidates.length);
  const toAppend = [];
  for (let j = 0; j < take; j++) toAppend.push(candidates[j].pair);
  if (!toAppend.length) {
    SpreadsheetApp.getUi().alert('No uncertain pairs in current band. Increase uncertaintyBand or add data.');
    return;
  }
  donor_appendPairsToTraining_(toAppend);
  SpreadsheetApp.getUi().alert('Added ' + toAppend.length + ' uncertain pairs. Label them and retrain.');
}

/** =========================
 * Data loading
 * ========================= */
function donor_loadAllRows_() {
  const kref = donor_readSheetAsObjects_(DONOR_CFG.krefSheet);
  const fec = donor_readSheetAsObjects_(DONOR_CFG.fecSheet);

  function mapRow(r, i, origin) {
    const first = String(r.DonorFirst || '').trim();
    const last = String(r.DonorLast || '').trim();
    const addr1 = String(r.Address1 || '').trim();
    const city = String(r.City || '').trim();
    const state = String(r.State || '').trim();
    const zip = donor_normalizeZip_(r.Zip);
    const employer = String(r.Employer || '').trim();
    const occupation = String(r.Occupation || '').trim();
    const fullName = (first + ' ' + last).trim();
    const house = donor_extractHouseNumber_(addr1);
    return { origin, rowIndex: i, first, last, addr1, city, state, zip, employer, occupation, fullName, house, globalIdx: -1 };
  }

  const all = [];
  for (let i = 0; i < kref.rows.length; i++) all.push(mapRow(kref.rows[i], i, 'K'));
  for (let j = 0; j < fec.rows.length; j++) all.push(mapRow(fec.rows[j], j, 'F'));
  for (let k = 0; k < all.length; k++) all[k].globalIdx = k;
  return all;
}

/** =========================
 * Blocking keys
 * ========================= */
function donor_blockingKeys_(r) {
  const keys = [];
  const last = String(r.last || '').toUpperCase();
  const lastSound = donor_soundex_(last);
  const first = String(r.first || '').toUpperCase();
  const firstInit = first ? first[0] : '';
  const zip = r.zip || '';
  const house = r.house || '';
  const city = String(r.city || '').toUpperCase().replace(/\s+/g, ' ').trim();
  const state = String(r.state || '').toUpperCase().trim();
  const employerTok = donor_employerToken_(r.employer);

  if (zip && lastSound) keys.push('ZL:' + zip + ':' + lastSound);
  if (house && lastSound) keys.push('HL:' + house + ':' + lastSound);
  if (lastSound) keys.push('L:' + lastSound);
  if (firstInit && lastSound) keys.push('FL:' + firstInit + ':' + lastSound);
  if (city && state && lastSound) keys.push('CSL:' + city + ':' + state + ':' + lastSound);
  if (employerTok && lastSound) keys.push('EL:' + employerTok + ':' + lastSound);
  return keys;
}

function donor_employerToken_(s) {
  let t = String(s || '').toUpperCase();
  if (!t) return '';
  t = t.replace(/[^A-Z0-9 ]+/g, ' ').replace(/\s+/g, ' ').trim();
  t = t.replace(/\b(THE|INC|LLC|LTD|CO|COMPANY|CORP|CORPORATION|UNIVERSITY|UNIV|SCHOOL|HOSPITAL|DEPT|DEPARTMENT|STATE|CITY|COUNTY)\b/g, '').trim();
  if (!t) return '';
  const parts = t.split(' ');
  const take = [];
  for (let i = 0; i < parts.length && take.length < 2; i++) {
    if (parts[i].length >= 3) take.push(parts[i]);
  }
  return take.join('_');
}

/** =========================
 * Pair generation with caps
 * ========================= */
function donor_generatePairsFromBlocks_(blocks) {
  const pairs = [];
  let total = 0;
  const seen = new Set();
  const it = blocks.entries();
  while (true) {
    const step = it.next();
    if (step.done) break;
    const blockRows = step.value[1];
    if (!blockRows || blockRows.length < 2) continue;
    const n = blockRows.length;
    if (n * n > DONOR_CFG.maxPairsPerBlock) continue;
    for (let i = 0; i < n; i++) {
      for (let j = i + 1; j < n; j++) {
        const a = blockRows[i];
        const b = blockRows[j];
        const ka = a.globalIdx < b.globalIdx ? a.globalIdx + '|' + b.globalIdx : b.globalIdx + '|' + a.globalIdx;
        if (seen.has(ka)) continue;
        seen.add(ka);
        const feat = donor_makeFeatures_(a, b);
        pairs.push({ a, b, aIdx: a.globalIdx, bIdx: b.globalIdx, features: feat });
        total++;
        if (total > DONOR_CFG.maxTotalPairs) return pairs;
      }
    }
  }
  return pairs;
}

/** =========================
 * Feature builder
 * ========================= */
function donor_makeFeatures_(a, b) {
  // Load learned associations (TODO: cache these)
  const associations = donor_getLearnedAssociations_();

  // Use enhanced similarity with multiple algorithms
  const nameSim = donor_enhancedSimilarity_(a.fullName, b.fullName, 'name', associations.first);
  const lastSim = donor_enhancedSimilarity_(a.last, b.last, 'last', associations.last);
  const addrSim = donor_enhancedSimilarity_(a.addr1, b.addr1, 'address', associations.address);
  const empSim = donor_enhancedSimilarity_(a.employer || '', b.employer || '', 'employer', associations.employer);
  const occSim = donor_enhancedSimilarity_(a.occupation || '', b.occupation || '', 'occupation', associations.occupation);

  // Geographic exact matches
  const zip_a = (a.zip || '').trim();
  const zip_b = (b.zip || '').trim();
  const zipMatch = (zip_a && zip_b && zip_a === zip_b) ? 1.0 : 0.0;

  const city_a = (a.city || '').trim().toUpperCase();
  const city_b = (b.city || '').trim().toUpperCase();
  const cityMatch = (city_a && city_b && city_a === city_b) ? 1.0 : 0.0;

  // Interaction features (products of base features)
  const f0 = nameSim;
  const f1 = lastSim;
  const f2 = addrSim;
  const f3 = empSim;
  const f4 = occSim;
  const f5 = zipMatch;
  const f6 = cityMatch;

  return {
    nameSim: donor_round4_(f0),
    lastSim: donor_round4_(f1),
    addrSim: donor_round4_(f2),
    empSim: donor_round4_(f3),
    occSim: donor_round4_(f4),
    zipMatch: donor_round4_(f5),
    cityMatch: donor_round4_(f6),
    name_last: donor_round4_(f0 * f1),
    name_addr: donor_round4_(f0 * f2),
    name_emp: donor_round4_(f0 * f3),
    name_occ: donor_round4_(f0 * f4),
    last_addr: donor_round4_(f1 * f2),
    last_emp: donor_round4_(f1 * f3),
    last_occ: donor_round4_(f1 * f4),
    addr_emp: donor_round4_(f2 * f3),
    addr_occ: donor_round4_(f2 * f4),
    emp_occ: donor_round4_(f3 * f4)
  };
}

function donor_getLearnedAssociations_() {
  // Extract learned associations from training data
  const sh = SpreadsheetApp.getActive().getSheetByName(DONOR_CFG.trainingSheet);
  const associations = {
    first: {},
    last: {},
    address: {},
    employer: {},
    occupation: {}
  };

  if (!sh || sh.getLastRow() < 2) return associations;

  const vals = sh.getDataRange().getValues();
  const h = vals[0].map(s => String(s || '').trim());

  const iLabel = h.indexOf('Label');
  const iFirstA = h.indexOf('first_a');
  const iFirstB = h.indexOf('first_b');
  const iLastA = h.indexOf('last_a');
  const iLastB = h.indexOf('last_b');
  const iAddrA = h.indexOf('addr_a');
  const iAddrB = h.indexOf('addr_b');
  const iEmpA = h.indexOf('emp_a');
  const iEmpB = h.indexOf('emp_b');
  const iOccA = h.indexOf('occ_a');
  const iOccB = h.indexOf('occ_b');

  if (iLabel === -1) return associations;

  for (let i = 1; i < vals.length; i++) {
    const row = vals[i];
    const label = Number(row[iLabel]);
    if (label !== 1) continue; // Only learn from positive matches

    function addAssoc(dict, idxA, idxB) {
      if (idxA === -1 || idxB === -1) return;
      const valA = String(row[idxA] || '').trim().toUpperCase();
      const valB = String(row[idxB] || '').trim().toUpperCase();
      if (!valA || !valB || valA === valB) return;

      if (!dict[valA]) dict[valA] = {};
      if (!dict[valB]) dict[valB] = {};
      dict[valA][valB] = 1.0;
      dict[valB][valA] = 1.0;
    }

    addAssoc(associations.first, iFirstA, iFirstB);
    addAssoc(associations.last, iLastA, iLastB);
    addAssoc(associations.address, iAddrA, iAddrB);
    addAssoc(associations.employer, iEmpA, iEmpB);
    addAssoc(associations.occupation, iOccA, iOccB);
  }

  return associations;
}

/** =========================
 * Training writers
 * ========================= */
function donor_writeTrainingSheet_(pairs) {
  const sh = donor_getOrCreateSheet_(DONOR_CFG.trainingSheet);
  sh.clear();
  sh.getRange(1, 1, 1, DONOR_TRAINING_HEADER.length).setValues([DONOR_TRAINING_HEADER]);
  const data = [];
  for (let i = 0; i < pairs.length; i++) {
    const p = pairs[i], f = p.features;
    data.push([
      '',
      p.a.origin, p.a.rowIndex + 2, p.a.first, p.a.last, p.a.addr1, p.a.city, p.a.state, p.a.zip, p.a.employer, p.a.occupation,
      p.b.origin, p.b.rowIndex + 2, p.b.first, p.b.last, p.b.addr1, p.b.city, p.b.state, p.b.zip, p.b.employer, p.b.occupation,
      f.nameSim, f.lastSim, f.addrSim, f.empSim, f.occSim, f.zipMatch, f.cityMatch,
      f.name_last, f.name_addr, f.name_emp, f.name_occ,
      f.last_addr, f.last_emp, f.last_occ,
      f.addr_emp, f.addr_occ, f.emp_occ
    ]);
  }
  if (data.length) sh.getRange(2, 1, data.length, DONOR_TRAINING_HEADER.length).setValues(data);
  sh.setFrozenRows(1);
}
function donor_appendPairsToTraining_(pairs) {
  const sh = donor_getOrCreateSheet_(DONOR_CFG.trainingSheet);
  let lastRow = sh.getLastRow();
  if (lastRow === 0) {
    sh.getRange(1, 1, 1, DONOR_TRAINING_HEADER.length).setValues([DONOR_TRAINING_HEADER]);
    sh.setFrozenRows(1);
    lastRow = 1;
  }
  const data = [];
  for (let i = 0; i < pairs.length; i++) {
    const p = pairs[i], f = p.features;
    data.push([
      '',
      p.a.origin, p.a.rowIndex + 2, p.a.first, p.a.last, p.a.addr1, p.a.city, p.a.state, p.a.zip, p.a.employer, p.a.occupation,
      p.b.origin, p.b.rowIndex + 2, p.b.first, p.b.last, p.b.addr1, p.b.city, p.b.state, p.b.zip, p.b.employer, p.b.occupation,
      f.nameSim, f.lastSim, f.addrSim, f.empSim, f.occSim, f.zipMatch, f.cityMatch,
      f.name_last, f.name_addr, f.name_emp, f.name_occ,
      f.last_addr, f.last_emp, f.last_occ,
      f.addr_emp, f.addr_occ, f.emp_occ
    ]);
  }
  if (data.length) sh.getRange(lastRow + 1, 1, data.length, DONOR_TRAINING_HEADER.length).setValues(data);
}

/** =========================
 * First name pair prior
 * ========================= */
function donor_buildAndStoreFirstNamePairTable_() {
  const sh = donor_getOrCreateSheet_(DONOR_CFG.trainingSheet);
  const vals = sh.getDataRange().getValues();
  if (vals.length < 2) {
    PropertiesService.getDocumentProperties().deleteProperty('donor_first_pair_table');
    return;
  }
  const h = vals[0].map(s => String(s || '').trim());
  const iLabel = h.indexOf('Label');
  const iFirstA = h.indexOf('first_a');
  const iFirstB = h.indexOf('first_b');
  if (iLabel === -1 || iFirstA === -1 || iFirstB === -1) return;

  const counts = {};
  function firstOf(name) {
    const s = String(name || '').trim().toUpperCase();
    if (!s) return '';
    return s.split(/\s+/)[0];
  }
  for (let i = 1; i < vals.length; i++) {
    const row = vals[i];
    const la = Number(row[iLabel]);
    if (la !== 0 && la !== 1) continue;
    const fa = firstOf(row[iFirstA]);
    const fb = firstOf(row[iFirstB]);
    if (!fa || !fb) continue;
    const key = fa < fb ? fa + '|' + fb : fb + '|' + fa;
    if (!counts[key]) counts[key] = {pos:0, neg:0};
    if (la === 1) counts[key].pos++; else counts[key].neg++;
  }
  const table = {};
  for (const k in counts) {
    const c = counts[k];
    const prob = (c.pos + 1) / (c.pos + c.neg + 2);
    table[k] = prob;
  }
  PropertiesService.getDocumentProperties().setProperty('donor_first_pair_table', JSON.stringify(table));
}
function donor_lookupFirstNamePairProb_(fa, fb) {
  const s = PropertiesService.getDocumentProperties().getProperty('donor_first_pair_table');
  if (!s) return 0.5;
  const table = JSON.parse(s);
  const A = String(fa || '').trim().toUpperCase().split(/\s+/)[0] || '';
  const B = String(fb || '').trim().toUpperCase().split(/\s+/)[0] || '';
  if (!A || !B) return 0.5;
  const key = A < B ? A + '|' + B : B + '|' + A;
  return table[key] != null ? Number(table[key]) : 0.5;
}

/** =========================
 * Write DonorIDs to sheets
 * ========================= */
function donor_writeIdsToSheets_(rows, idMap) {
  const ss = SpreadsheetApp.getActive();
  const groupedK = [];
  const groupedF = [];
  for (let i = 0; i < rows.length; i++) {
    const r = rows[i];
    if (r.origin === 'K') groupedK.push(r); else if (r.origin === 'F') groupedF.push(r);
  }
  function writeOne(sheetName, list) {
    const sh = ss.getSheetByName(sheetName);
    if (!sh) return;
    const header = sh.getRange(1, 1, 1, sh.getLastColumn()).getValues()[0];
    let idCol = header.indexOf(DONOR_CFG.idColumnName) + 1;
    if (idCol === 0) {
      idCol = header.length + 1;
      sh.getRange(1, idCol).setValue(DONOR_CFG.idColumnName);
    }
    const out = [];
    for (let i = 0; i < list.length; i++) out.push([idMap.get(list[i].globalIdx) || '']);
    if (out.length) sh.getRange(2, idCol, out.length, 1).setValues(out);
  }
  writeOne(DONOR_CFG.krefSheet, groupedK);
  writeOne(DONOR_CFG.fecSheet, groupedF);
}

/** =========================
 * Utilities
 * ========================= */
function donor_readSheetAsObjects_(name) {
  const sh = SpreadsheetApp.getActive().getSheetByName(name);
  if (!sh) return { header: [], rows: [] };
  const vals = sh.getDataRange().getValues();
  if (vals.length < 2) return { header: vals[0] || [], rows: [] };
  const header = vals[0].map(s => String(s || '').trim());
  const rows = [];
  for (let i = 1; i < vals.length; i++) {
    const row = vals[i];
    const obj = {};
    for (let j = 0; j < header.length; j++) obj[header[j]] = row[j];
    rows.push(obj);
  }
  return { header, rows };
}
function donor_getOrCreateSheet_(name) {
  const ss = SpreadsheetApp.getActive();
  return ss.getSheetByName(name) || ss.insertSheet(name);
}

/**
 * Ensures the Nicknames sheet exists and is populated.
 * Downloads from public source if sheet doesn't exist.
 * Returns the nickname pairs as array of [name1, name2] arrays.
 */
function donor_ensureNicknamesSheet_() {
  const ss = SpreadsheetApp.getActive();
  const NICKNAMES_SHEET = 'Nicknames';
  let sh = ss.getSheetByName(NICKNAMES_SHEET);

  // If sheet exists and has data, return it
  if (sh && sh.getLastRow() > 1) {
    const vals = sh.getDataRange().getValues();
    return vals.slice(1); // Skip header
  }

  // Create sheet if it doesn't exist
  if (!sh) {
    sh = ss.insertSheet(NICKNAMES_SHEET);
  }

  // Download nickname data from public source
  // Using a common nickname dataset format: name1,name2 pairs
  try {
    // Fetch from GitHub - carltonnorthern/nickname-and-diminutive-names-lookup
    const url = 'https://raw.githubusercontent.com/carltonnorthern/nickname-and-diminutive-names-lookup/master/names.csv';
    const response = UrlFetchApp.fetch(url, { muteHttpExceptions: true });

    if (response.getResponseCode() !== 200) {
      Logger.log('Failed to download nicknames: ' + response.getResponseCode());
      // Create empty sheet with header
      sh.getRange(1, 1, 1, 2).setValues([['Name', 'Nickname']]);
      return [];
    }

    const csvText = response.getContentText();
    const lines = csvText.split('\n');
    const pairs = [];

    // Parse the CSV - format is: name,relationship,nickname (skip header)
    let isFirstLine = true;
    for (const line of lines) {
      if (!line.trim()) continue;

      // Skip header row
      if (isFirstLine) {
        isFirstLine = false;
        continue;
      }

      const parts = line.split(',').map(s => s.trim().toUpperCase()).filter(s => s);
      if (parts.length < 3) continue;

      // Format: NAME, RELATIONSHIP_TYPE, NICKNAME
      // Only keep actual nickname pairs, skip relationship markers
      const name = parts[0];
      const nickname = parts[2];

      // Skip if either is empty or a relationship type
      if (!name || !nickname) continue;
      if (name === 'NAME1' || nickname === 'NAME2') continue; // Skip header variants
      if (nickname === 'HAS_NICKNAME' || name === 'HAS_NICKNAME') continue;

      // Add the pair
      if (name !== nickname) {
        pairs.push([name, nickname]);
      }
    }

    // Write to sheet
    sh.clear();
    sh.getRange(1, 1, 1, 2).setValues([['Name', 'Nickname']]);
    if (pairs.length > 0) {
      sh.getRange(2, 1, pairs.length, 2).setValues(pairs);
    }

    Logger.log('Loaded ' + pairs.length + ' nickname pairs');
    return pairs;

  } catch (e) {
    Logger.log('Error downloading nicknames: ' + e.message);
    // Create empty sheet with header
    sh.getRange(1, 1, 1, 2).setValues([['Name', 'Nickname']]);
    return [];
  }
}

function donor_normalizeZip_(s) {
  const digits = String(s || '').replace(/\D+/g, '');
  if (digits.length >= 5) return digits.slice(0, 5);
  if (digits.length > 0) return digits.padStart(5, '0');
  return '';
}
function donor_extractHouseNumber_(addr) {
  const m = String(addr || '').match(/\b(\d{1,6})\b/);
  return m ? m[1] : '';
}
function donor_round4_(x) {
  return Math.round(Number(x || 0) * 10000) / 10000;
}
function donor_soundex_(s) {
  s = String(s || '').toUpperCase().replace(/[^A-Z]/g, '');
  if (!s) return '';
  const f = s[0];
  const map = {B:1,F:1,P:1,V:1,C:2,G:2,J:2,K:2,Q:2,S:2,X:2,Z:2,D:3,T:3,L:4,M:5,N:5,R:6};
  let out = f;
  let prev = map[f] || 0;
  for (let i = 1; i < s.length && out.length < 4; i++) {
    const c = s[i];
    const code = map[c] || 0;
    if (code !== 0 && code !== prev) out += String(code);
    prev = code;
  }
  return (out + '000').slice(0, 4);
}
function donor_levenshtein_(s1, s2) {
  s1 = String(s1 || '').toUpperCase();
  s2 = String(s2 || '').toUpperCase();
  if (s1.length < s2.length) {
    const tmp = s1; s1 = s2; s2 = tmp;
  }
  if (s2.length === 0) return s1.length;

  let prevRow = [];
  for (let j = 0; j <= s2.length; j++) prevRow.push(j);

  for (let i = 0; i < s1.length; i++) {
    const currRow = [i + 1];
    for (let j = 0; j < s2.length; j++) {
      const insertions = prevRow[j + 1] + 1;
      const deletions = currRow[j] + 1;
      const substitutions = prevRow[j] + (s1[i] !== s2[j] ? 1 : 0);
      currRow.push(Math.min(insertions, deletions, substitutions));
    }
    prevRow = currRow;
  }
  return prevRow[prevRow.length - 1];
}
function donor_normalizeAddress_(addr) {
  let s = String(addr || '').toUpperCase().trim();
  const abbrevs = {
    'STREET':'ST', 'AVENUE':'AVE', 'ROAD':'RD', 'DRIVE':'DR', 'LANE':'LN',
    'COURT':'CT', 'PLACE':'PL', 'BOULEVARD':'BLVD', 'CIRCLE':'CIR',
    'PARKWAY':'PKWY', 'TERRACE':'TER', 'NORTH':'N', 'SOUTH':'S',
    'EAST':'E', 'WEST':'W', 'APARTMENT':'APT', 'SUITE':'STE', 'NUMBER':'NO'
  };
  // Expand abbreviations to full words for better matching
  Object.keys(abbrevs).forEach(full => {
    const abbr = abbrevs[full];
    const re = new RegExp('\\b' + abbr + '\\b', 'g');
    s = s.replace(re, full);
  });
  s = s.replace(/[^\w\s]/g, ' ').replace(/\s+/g, ' ').trim();
  return s;
}
function donor_enhancedSimilarity_(str1, str2, fieldType, associations) {
  str1 = String(str1 || '').trim().toUpperCase();
  str2 = String(str2 || '').trim().toUpperCase();

  // Check learned associations first
  if (associations && associations[str1] && associations[str1][str2]) {
    return associations[str1][str2];
  }
  if (associations && associations[str2] && associations[str2][str1]) {
    return associations[str2][str1];
  }

  if (str1 === str2) return 1.0;
  if (!str1 && !str2) return 1.0;
  if (!str1 || !str2) return 0.0;

  // Normalize addresses
  if (fieldType === 'address') {
    str1 = donor_normalizeAddress_(str1);
    str2 = donor_normalizeAddress_(str2);
    if (str1 === str2) return 1.0;
  }

  // Jaro-Winkler score
  const jwScore = donor_jaroWinkler_(str1, str2);

  // Levenshtein score
  const maxLen = Math.max(str1.length, str2.length);
  const levDist = donor_levenshtein_(str1, str2);
  const levScore = maxLen > 0 ? (1.0 - levDist / maxLen) : 1.0;

  // Token-based Jaccard similarity
  const tokens1 = str1.split(/\s+/).filter(t => t.length > 0);
  const tokens2 = str2.split(/\s+/).filter(t => t.length > 0);
  const set1 = new Set(tokens1);
  const set2 = new Set(tokens2);
  const intersection = new Set([...set1].filter(t => set2.has(t)));
  const union = new Set([...set1, ...set2]);
  const tokenJaccard = union.size > 0 ? intersection.size / union.size : 0.0;

  // Field-specific weights
  if (fieldType === 'name' || fieldType === 'first' || fieldType === 'last') {
    const phoneticMatch = donor_soundex_(str1) === donor_soundex_(str2) ? 1.0 : 0.0;
    const baseScore = jwScore * 0.35 + levScore * 0.25 + tokenJaccard * 0.2 + phoneticMatch * 0.2;
    return Math.max(baseScore, phoneticMatch * 0.8);
  } else if (fieldType === 'address') {
    return jwScore * 0.4 + levScore * 0.3 + tokenJaccard * 0.3;
  } else {
    return jwScore * 0.5 + levScore * 0.3 + tokenJaccard * 0.2;
  }
}
function donor_jaroWinkler_(s1, s2) {
  s1 = String(s1 || '').toUpperCase();
  s2 = String(s2 || '').toUpperCase();
  if (!s1 && !s2) return 1;
  if (!s1 || !s2) return 0;
  const mDist = Math.max(0, Math.floor(Math.max(s1.length, s2.length) / 2) - 1);
  const s1M = new Array(s1.length).fill(false);
  const s2M = new Array(s2.length).fill(false);
  let matches = 0;
  for (let i = 0; i < s1.length; i++) {
    const start = Math.max(0, i - mDist);
    const end = Math.min(i + mDist + 1, s2.length);
    for (let j = start; j < end; j++) {
      if (s2M[j] || s1[i] !== s2[j]) continue;
      s1M[i] = true;
      s2M[j] = true;
      matches++;
      break;
    }
  }
  if (!matches) return 0;
  let t = 0;
  let k = 0;
  for (let i = 0; i < s1.length; i++) {
    if (!s1M[i]) continue;
    while (!s2M[k]) k++;
    if (s1[i] !== s2[k]) t++;
    k++;
  }
  t = t / 2;
  const j = (matches / s1.length + matches / s2.length + (matches - t) / matches) / 3;
  let l = 0;
  while (l < 4 && s1[l] && s2[l] && s1[l] === s2[l]) l++;
  return j + l * 0.1 * (1 - j);
}

/** =========================
 * Logistic regression and clustering
 * ========================= */
function donor_fitLogReg_(X, y, lr, iters) {
  const n = X.length;
  const d = X[0].length;
  const w = new Array(d + 1).fill(0);
  function sigmoid(z) { return 1 / (1 + Math.exp(-z)); }
  for (let it = 0; it < iters; it++) {
    const grad = new Array(d + 1).fill(0);
    for (let i = 0; i < n; i++) {
      let z = w[d];
      const xi = X[i];
      for (let j = 0; j < d; j++) z += w[j] * xi[j];
      const p = sigmoid(z);
      const err = p - y[i];
      for (let j2 = 0; j2 < d; j2++) grad[j2] += err * xi[j2];
      grad[d] += err;
    }
    for (let g = 0; g < d + 1; g++) w[g] -= lr * grad[g] / n;
  }
  return w;
}
function donor_predictLogReg_(x, w) {
  const d = x.length;
  let z = w[d];
  for (let j = 0; j < d; j++) z += w[j] * x[j];
  return 1 / (1 + Math.exp(-z));
}
function donor_newUnionFind_(size) {
  const parent = [];
  const rank = [];
  for (let i = 0; i < size; i++) { parent[i] = i; rank[i] = 0; }
  return { parent, rank };
}
function donor_find_(uf, x) {
  if (uf.parent[x] !== x) uf.parent[x] = donor_find_(uf, uf.parent[x]);
  return uf.parent[x];
}
function donor_union_(uf, a, b) {
  let pa = donor_find_(uf, a);
  let pb = donor_find_(uf, b);
  if (pa === pb) return;
  if (uf.rank[pa] < uf.rank[pb]) { const tmp = pa; pa = pb; pb = tmp; }
  uf.parent[pb] = pa;
  if (uf.rank[pa] === uf.rank[pb]) uf.rank[pa]++;
}
function donor_components_(uf) {
  const groups = new Map();
  for (let i = 0; i < uf.parent.length; i++) {
    const p = donor_find_(uf, i);
    if (!groups.has(p)) groups.set(p, []);
    groups.get(p).push(i);
  }
  return Array.from(groups.values());
}

/** =========================
 * Sampling and keys
 * ========================= */
function donor_sampleArray_(arr, n) {
  n = Math.max(0, Math.floor(n || 0));
  const res = [];
  const L = Array.isArray(arr) ? arr.length : 0;
  if (n === 0 || L === 0) return res;
  for (let i = 0; i < L; i++) {
    if (i < n) {
      res.push(arr[i]);
    } else {
      const j = Math.floor(Math.random() * (i + 1));
      if (j < n) res[j] = arr[i];
    }
  }
  return res;
}
function donor_pairKey_(a, b) {
  const ka = a.origin + ':' + (a.rowIndex + 2);
  const kb = b.origin + ':' + (b.rowIndex + 2);
  return (ka < kb) ? (ka + '|' + kb) : (kb + '|' + ka);
}
function donor_getSeenPairKeys_() {
  const sh = donor_getOrCreateSheet_(DONOR_CFG.trainingSheet);
  const vals = sh.getDataRange().getValues();
  if (vals.length < 2) return new Set();
  const h = vals[0].map(s => String(s || '').trim());
  const iSheetA = h.indexOf('sheet_a');
  const iRowA = h.indexOf('row_a');
  const iSheetB = h.indexOf('sheet_b');
  const iRowB = h.indexOf('row_b');
  if (iSheetA === -1 || iRowA === -1 || iSheetB === -1 || iRowB === -1) return new Set();
  const set = new Set();
  for (let i = 1; i < vals.length; i++) {
    const row = vals[i];
    const ka = String(row[iSheetA] || '') + ':' + String(row[iRowA] || '');
    const kb = String(row[iSheetB] || '') + ':' + String(row[iRowB] || '');
    if (!ka || !kb) continue;
    const key = (ka < kb) ? (ka + '|' + kb) : (kb + '|' + ka);
    set.add(key);
  }
  return set;
}

/** =========================================================
 * Local runner adapter
 * ========================================================= */
const DL_CFG = {
  krefSheet: DONOR_CFG.krefSheet,
  fecSheet: DONOR_CFG.fecSheet,
  sampleTrainingPairs: DONOR_CFG.sampleTrainingPairs,
  uncertainBatchSize: DONOR_CFG.uncertainBatchSize,
  uncertaintyBand: DONOR_CFG.uncertaintyBand,
  maxPairsPerBlock: DONOR_CFG.maxPairsPerBlock,
  maxTotalPairs: DONOR_CFG.maxTotalPairs,
  tokenMinutes: 30
};

/** Prepare job and show Terminal command */
function dl_prepareLocalJobAndShowCommand_() {
  dl_resetProgress_();
  dl_setProgress_('Preparing job', 0, 1, 'Creating Drive bundle');

  const webAppUrl = dl_getExecUrlFromOptions_();
  if (!webAppUrl) {
    SpreadsheetApp.getUi().alert(
      'Put your Web App URL ending with /exec in Options!I2.\nExample:\nhttps://script.google.com/macros/s/AKfycb.../exec'
    );
    return;
  }

  const kref = dl_exportSheetAsCsv_(DL_CFG.krefSheet);
  const fec  = dl_exportSheetAsCsv_(DL_CFG.fecSheet);
  if (!kref.file || !fec.file) {
    SpreadsheetApp.getUi().alert(
      'Missing input sheets or no rows. Confirm sheets exist: ' + DL_CFG.krefSheet + ' and ' + DL_CFG.fecSheet
    );
    return;
  }

  const token = dl_makeToken_();
  const until = Date.now() + DL_CFG.tokenMinutes * 60 * 1000;

  // Read match threshold from Options!J2
  const ss = SpreadsheetApp.getActive();
  const optionsSheet = ss.getSheetByName('Options');
  const threshold = optionsSheet ? Number(optionsSheet.getRange('J2').getValue() || 0.7) : 0.7;

  const job = {
    jobId: 'job_' + Utilities.getUuid().replace(/-/g, '').slice(0, 12),
    cfg: {
      sampleTrainingPairs: DL_CFG.sampleTrainingPairs,
      uncertainBatchSize: DL_CFG.uncertainBatchSize,
      uncertaintyBand: DL_CFG.uncertaintyBand,
      maxPairsPerBlock: DL_CFG.maxPairsPerBlock,
      maxTotalPairs: DL_CFG.maxTotalPairs,
      predictThreshold: threshold
    },
    krefUrl: webAppUrl + '?csv=kref&token=' + encodeURIComponent(token),
    fecUrl:  webAppUrl + '?csv=fec&token=' + encodeURIComponent(token),
    modelUrl: webAppUrl + '?model=1',
    resultUrl: webAppUrl + '?result=1&token=' + encodeURIComponent(token)
  };

  const props = PropertiesService.getDocumentProperties();
  props.setProperty('dl_token', token);
  props.setProperty('dl_token_until', String(until));
  props.setProperty('dl_job_json', JSON.stringify(job));
  props.setProperty('dl_kref_fileId', kref.file.getId());
  props.setProperty('dl_fec_fileId',  fec.file.getId());
  props.setProperty('dl_staging_folderId', kref.folder.getId());

  const cmd =
    "curl -sSL '" + webAppUrl + "?runner=1' | " +
    "python3 - --bundle '" + webAppUrl + "?job=1&token=" + token + "' " +
    "--result '" + webAppUrl + "?result=1&token=" + token + "'";

  dl_setProgress_('Waiting', 0, 0, 'Copy the command into Terminal');

  const html = HtmlService.createHtmlOutput(
    '<div style="font-family:system-ui,Arial;padding:12px;max-width:720px">' +
      '<div style="margin:6px 0">Token expires in about ' + DL_CFG.tokenMinutes + ' minutes</div>' +
      '<div style="margin:6px 0">Copy this command into your Mac Terminal:</div>' +
      '<textarea id="cmd" style="width:100%;height:140px" readonly>' +
        dl_htmlEscape_(cmd) +
      '</textarea>' +
      '<div style="margin-top:8px">' +
        '<button onclick="navigator.clipboard.writeText(document.getElementById(\'cmd\').value)">Copy to clipboard</button>' +
      '</div>' +
      '<div style="margin-top:8px;color:#666;font-size:12px">Keep this sheet open. Progress will appear in the sidebar.</div>' +
    '</div>'
  ).setWidth(740).setHeight(280);
  SpreadsheetApp.getUi().showModalDialog(html, 'Run locally');
}

/** Incremental training: load existing data and add more pairs */
function dl_prepareIncrementalTrainingJob_() {
  dl_resetProgress_();
  dl_setProgress_('Preparing incremental training job', 0, 1, 'Creating Drive bundle');

  const webAppUrl = dl_getExecUrlFromOptions_();
  if (!webAppUrl) {
    SpreadsheetApp.getUi().alert(
      'Put your Web App URL ending with /exec in Options!I2.\nExample:\nhttps://script.google.com/macros/s/AKfycb.../exec'
    );
    return;
  }

  const kref = dl_exportSheetAsCsv_(DL_CFG.krefSheet);
  const fec  = dl_exportSheetAsCsv_(DL_CFG.fecSheet);
  if (!kref.file || !fec.file) {
    SpreadsheetApp.getUi().alert(
      'Missing input sheets or no rows. Confirm sheets exist: ' + DL_CFG.krefSheet + ' and ' + DL_CFG.fecSheet
    );
    return;
  }

  const token = dl_makeToken_();
  const until = Date.now() + DL_CFG.tokenMinutes * 60 * 1000;

  // Read match threshold from Options!J2
  const ss = SpreadsheetApp.getActive();
  const optionsSheet = ss.getSheetByName('Options');
  const threshold = optionsSheet ? Number(optionsSheet.getRange('J2').getValue() || 0.7) : 0.7;

  const job = {
    jobId: 'job_' + Utilities.getUuid().replace(/-/g, '').slice(0, 12),
    mode: 'incremental',  // NEW: Flag to indicate incremental training mode
    cfg: {
      sampleTrainingPairs: DL_CFG.sampleTrainingPairs,
      uncertainBatchSize: DL_CFG.uncertainBatchSize,
      uncertaintyBand: DL_CFG.uncertaintyBand,
      maxPairsPerBlock: DL_CFG.maxPairsPerBlock,
      maxTotalPairs: DL_CFG.maxTotalPairs,
      predictThreshold: threshold
    },
    krefUrl: webAppUrl + '?csv=kref&token=' + encodeURIComponent(token),
    fecUrl:  webAppUrl + '?csv=fec&token=' + encodeURIComponent(token),
    modelUrl: webAppUrl + '?model=1',
    resultUrl: webAppUrl + '?result=1&token=' + encodeURIComponent(token)
  };

  const props = PropertiesService.getDocumentProperties();
  props.setProperty('dl_token', token);
  props.setProperty('dl_token_until', String(until));
  props.setProperty('dl_job_json', JSON.stringify(job));
  props.setProperty('dl_kref_fileId', kref.file.getId());
  props.setProperty('dl_fec_fileId',  fec.file.getId());
  props.setProperty('dl_staging_folderId', kref.folder.getId());

  const cmd =
    "curl -sSL '" + webAppUrl + "?runner=1' | " +
    "python3 - --bundle '" + webAppUrl + "?job=1&token=" + token + "' " +
    "--result '" + webAppUrl + "?result=1&token=" + token + "'";

  dl_setProgress_('Waiting', 0, 0, 'Copy the command into Terminal');

  const html = HtmlService.createHtmlOutput(
    '<div style="font-family:system-ui,Arial;padding:12px;max-width:720px">' +
      '<div style="margin:6px 0"><strong>Incremental Training Mode</strong></div>' +
      '<div style="margin:6px 0;color:#666">This will load your existing training data and let you add more labeled pairs.</div>' +
      '<div style="margin:6px 0">Token expires in about ' + DL_CFG.tokenMinutes + ' minutes</div>' +
      '<div style="margin:6px 0">Copy this command into your Mac Terminal:</div>' +
      '<textarea id="cmd" style="width:100%;height:140px" readonly>' +
        dl_htmlEscape_(cmd) +
      '</textarea>' +
      '<div style="margin-top:8px">' +
        '<button onclick="navigator.clipboard.writeText(document.getElementById(\'cmd\').value)">Copy to clipboard</button>' +
      '</div>' +
      '<div style="margin-top:8px;color:#666;font-size:12px">Keep this sheet open. Progress will appear in the sidebar.</div>' +
    '</div>'
  ).setWidth(740).setHeight(320);
  SpreadsheetApp.getUi().showModalDialog(html, 'Continue Training Locally');
}

/** Progress sidebar */
function dl_showProgressSidebar() {
  const html = HtmlService.createHtmlOutput(
    '<div style="font-family:system-ui,Arial;padding:12px;max-width:360px">' +
      '<h3 style="margin:0 0 8px 0;font-weight:600">Local run progress</h3>' +
      '<div id="p">No updates yet.</div>' +
      '<script>' +
      '  function fmt(p){ if(!p) return "No updates yet."; ' +
      '    return "Phase: " + (p.phase||"") + "<br>Done: " + (p.done||0) + " of " + (p.total||0) + "<br>Note: " + (p.note||"") + "<br><small>"+new Date(p.ts).toLocaleString()+"</small>"; }' +
      '  function poll(){ google.script.run.withSuccessHandler(function(s){ ' +
      '      try{var p=s?JSON.parse(s):null; document.getElementById("p").innerHTML = fmt(p);}catch(e){document.getElementById("p").textContent="Parse error";}' +
      '    }).dl_getProgressPayload_(); }' +
      '  setInterval(poll, 1500); poll();' +
      '</script>' +
    '</div>'
  ).setTitle('Local run progress');
  SpreadsheetApp.getUi().showSidebar(html);
}
function dl_getProgressPayload_() {
  return PropertiesService.getDocumentProperties().getProperty('dl_progress') || '';
}

/** Web app endpoints */

function doGet(e) {
  const p = e.parameter || {};

  // Python runner payload
  if (p.runner == '1') {
    const py = [
      '#!/usr/bin/env python3',
      'import sys, json, csv, io, urllib.request, argparse, time, math, random, re',
      'from collections import defaultdict',
      '',
      '# ----------------------------',
      '# HTTP helpers with retry logic',
      'def http_get(url):',
      '    max_retries = 4',
      '    delays = [2, 4, 8, 16]  # exponential backoff in seconds',
      '    ',
      '    # Show which URL we\'re fetching (truncate for readability)',
      '    url_display = url if len(url) <= 100 else url[:97] + "..."',
      '    print(f"\\nFetching: {url_display}")',
      '    start_time = time.time()',
      '    ',
      '    for attempt in range(max_retries + 1):',
      '        try:',
      '            with urllib.request.urlopen(url, timeout=300) as r:',
      '                data = r.read()',
      '                elapsed = time.time() - start_time',
      '                size_mb = len(data) / (1024 * 1024)',
      '                print(f" Received {size_mb:.2f} MB in {elapsed:.1f}s")',
      '                return data',
      '        except urllib.error.HTTPError as e:',
      '            elapsed = time.time() - start_time',
      '            print(f" HTTP {e.code} error after {elapsed:.1f}s")',
      '            if e.code == 500:',
      '                print(f"   Server error - likely timeout or memory limit on server side")',
      '                print(f"   This often happens with very large datasets (100k+ rows)")',
      '            if e.code == 429 and attempt < max_retries:',
      '                delay = delays[attempt]',
      '                print(f"   Rate limit hit. Retrying in {delay}s... (attempt {attempt + 1}/{max_retries})")',
      '                time.sleep(delay)',
      '            else:',
      '                raise',
      '        except Exception as e:',
      '            elapsed = time.time() - start_time',
      '            print(f" Error after {elapsed:.1f}s: {type(e).__name__}: {e}")',
      '            raise',
      '    ',
      '    raise Exception("Max retries exceeded")',
      '',
      'def http_post(url, obj):',
      '    max_retries = 4',
      '    delays = [2, 4, 8, 16]  # exponential backoff in seconds',
      '    ',
      '    data = json.dumps(obj).encode("utf-8")',
      '    ',
      '    for attempt in range(max_retries + 1):',
      '        try:',
      '            req = urllib.request.Request(url, data=data, headers={"Content-Type":"application/json"})',
      '            with urllib.request.urlopen(req) as r:',
      '                return r.read().decode("utf-8")',
      '        except urllib.error.HTTPError as e:',
      '            if e.code == 429 and attempt < max_retries:',
      '                delay = delays[attempt]',
      '                print(f"HTTP 429 rate limit hit. Retrying in {delay}s... (attempt {attempt + 1}/{max_retries})")',
      '                time.sleep(delay)',
      '            else:',
      '                raise',
      '    ',
      '    raise Exception("Max retries exceeded")',
      '',
      'def parse_csv(text):',
      '    f = io.StringIO(text)',
      '    rdr = csv.reader(f)',
      '    rows = list(rdr)',
      '    if not rows: return [], []',
      '    header = rows[0]',
      '    return header, rows[1:]',
      '',
      UNIFIED_MATCHING_MODULE,
      '',
      '# ----------------------------',
      '# Training-specific functions',
      '# ----------------------------',
      'def sgd_update(w, x, y, lr=0.1):',
      '    """Stochastic gradient descent update for logistic regression."""',
      '    p = predict({"_x": x}, {"_x": x}, w)  # Dummy call to get sigmoid',
      '    # Actually need direct calculation here',
      '    z = w[-1]',
      '    for i in range(len(x)):',
      '        z += w[i] * x[i]',
      '    p = sigmoid(z)',
      '    err = p - y',
      '    for i in range(len(x)):',
      '        w[i] -= lr * err * x[i]',
      '    w[-1] -= lr * err',
      '    return w',
      '',
      '# ----------------------------',
      '# Learn semantic associations from training data',
      'def learn_associations(training_rows):',
      '    """',
      '    Analyze matched pairs to learn which terms co-occur across ALL fields.',
      '    Returns dictionaries for first, last, address, city, state, zip, employer, occupation associations.',
      '    """',
      '    first_cooccur = defaultdict(lambda: defaultdict(int))',
      '    last_cooccur = defaultdict(lambda: defaultdict(int))',
      '    address_cooccur = defaultdict(lambda: defaultdict(int))',
      '    city_cooccur = defaultdict(lambda: defaultdict(int))',
      '    state_cooccur = defaultdict(lambda: defaultdict(int))',
      '    zip_cooccur = defaultdict(lambda: defaultdict(int))',
      '    employer_cooccur = defaultdict(lambda: defaultdict(int))',
      '    occupation_cooccur = defaultdict(lambda: defaultdict(int))',
      '    ',
      '    for row in training_rows:',
      '        if row.get("label") != 1:  # Only learn from matches',
      '            continue',
      '        ',
      '        # Extract and normalize terms',
      '        first_a = normalize_name(row["a"].get("first", ""))',
      '        first_b = normalize_name(row["b"].get("first", ""))',
      '        last_a = normalize_name(row["a"].get("last", ""))',
      '        last_b = normalize_name(row["b"].get("last", ""))',
      '        addr_a = normalize_address(row["a"].get("addr", ""))',
      '        addr_b = normalize_address(row["b"].get("addr", ""))',
      '        city_a = normalize_name(row["a"].get("city", ""))',
      '        city_b = normalize_name(row["b"].get("city", ""))',
      '        state_a = normalize_state(row["a"].get("state", ""))',
      '        state_b = normalize_state(row["b"].get("state", ""))',
      '        zip_a = normalize_zip(row["a"].get("zip", ""))',
      '        zip_b = normalize_zip(row["b"].get("zip", ""))',
      '        emp_a = normalize_name(row["a"].get("employer", ""))',
      '        emp_b = normalize_name(row["b"].get("employer", ""))',
      '        occ_a = normalize_name(row["a"].get("occupation", ""))',
      '        occ_b = normalize_name(row["b"].get("occupation", ""))',
      '        ',
      '        # Track co-occurrences (bidirectional)',
      '        if first_a and first_b and first_a != first_b:',
      '            first_cooccur[first_a][first_b] += 1',
      '            first_cooccur[first_b][first_a] += 1',
      '        ',
      '        if last_a and last_b and last_a != last_b:',
      '            last_cooccur[last_a][last_b] += 1',
      '            last_cooccur[last_b][last_a] += 1',
      '        ',
      '        if addr_a and addr_b and addr_a != addr_b:',
      '            address_cooccur[addr_a][addr_b] += 1',
      '            address_cooccur[addr_b][addr_a] += 1',
      '        ',
      '        if city_a and city_b and city_a != city_b:',
      '            city_cooccur[city_a][city_b] += 1',
      '            city_cooccur[city_b][city_a] += 1',
      '        ',
      '        if state_a and state_b and state_a != state_b:',
      '            state_cooccur[state_a][state_b] += 1',
      '            state_cooccur[state_b][state_a] += 1',
      '        ',
      '        if zip_a and zip_b and zip_a != zip_b:',
      '            zip_cooccur[zip_a][zip_b] += 1',
      '            zip_cooccur[zip_b][zip_a] += 1',
      '        ',
      '        if emp_a and emp_b and emp_a != emp_b:',
      '            employer_cooccur[emp_a][emp_b] += 1',
      '            employer_cooccur[emp_b][emp_a] += 1',
      '        ',
      '        if occ_a and occ_b and occ_a != occ_b:',
      '            occupation_cooccur[occ_a][occ_b] += 1',
      '            occupation_cooccur[occ_b][occ_a] += 1',
      '    ',
      '    return (dict(first_cooccur), dict(last_cooccur), dict(address_cooccur), ',
      '            dict(city_cooccur), dict(state_cooccur), dict(zip_cooccur),',
      '            dict(employer_cooccur), dict(occupation_cooccur))',
      '',
      '# Wrapper for backward compatibility',
      'def features(a, b):',
      '    """Calculate features using unified module."""',
      '    return calculate_features(a, b)',
      '',
      'def features_dict(a, b):',
      '    """Return features as a dict for storage in training data."""',
      '    f = features(a, b)',
      '    return {',
      '        "nameSim": f[0], "lastSim": f[1], "addrSim": f[2], "empSim": f[3], "occSim": f[4],',
      '        "zipMatch": f[5], "cityMatch": f[6],',
      '        "name_last": f[7], "name_addr": f[8], "name_emp": f[9], "name_occ": f[10],',
      '        "last_addr": f[11], "last_emp": f[12], "last_occ": f[13],',
      '        "addr_emp": f[14], "addr_occ": f[15], "emp_occ": f[16]',
      '    }',
      '',
      '# ----------------------------',
      '# TTY input that works even when stdin is piped',
      'def read_tty(prompt=""):',
      '    try:',
      '        if sys.stdin and hasattr(sys.stdin, "isatty") and sys.stdin.isatty():',
      '            return input(prompt)',
      '        with open("/dev/tty") as tty:',
      '            if prompt:',
      '                print(prompt, end="", flush=True)',
      '            return tty.readline().rstrip("\\n")',
      '    except Exception:',
      '        return ""',
      '',
      'def show_pair_and_get_label(i, total, a, b, x, p):',
      '    print(f"[{i}/{total}]")',
      '    def fmt(o):',
      '        nm = (o.get("DonorFirst","")+" "+o.get("DonorLast","")).strip()',
      '        ad = ", ".join([s for s in [o.get("Address1",""), o.get("City",""), o.get("State","")] if s])',
      '        em = " | ".join([s for s in [o.get("Employer",""), o.get("Occupation","")] if s])',
      '        dt = o.get("ReceiptDate", "").strip() or "No date"',
      '        return nm+"\\n"+ad+"\\n"+em+"\\n"+"Donated: "+dt',
      '    print("A:"); print(fmt(a))',
      '    print("B:"); print(fmt(b))',
      '    # Show confidence as percentage',
      '    confidence_pct = p * 100',
      '    print(f"Model confidence: {confidence_pct:.1f}% match")',
      '    while True:',
      '        s = read_tty("Label this pair (1=match, 0=not match, 2=skip): ").strip()',
      '        if s in ("0","1","2"):',
      '            return int(s)',
      '        print("Please enter 1, 0, or 2.")',
      '',
      '# ----------------------------',
      '# Stable keys and candidate generators',
      'def stable_pair_key(a_obj, b_obj):',
      '    a = (str(a_obj.get("_origin","C")), int(a_obj.get("_rownum", 0)))',
      '    b = (str(b_obj.get("_origin","C")), int(b_obj.get("_rownum", 0)))',
      '    return (a, b) if a <= b else (b, a)',
      '',
      'def cold_start_candidates(window_rows, want, seen_pairs, donor_usage_count, max_usage=3):',
      '    # Address-forward scoring. Skip exact name+address dupes and already-seen pairs.',
      '    cands = []; L = len(window_rows)',
      '    if L < 2: return []',
      '    limit = min(L, 2500)',
      '    for i in range(limit):',
      '        a = window_rows[i]',
      '        for j in range(i+1, limit):',
      '            b = window_rows[j]',
      '            k = stable_pair_key(a, b)',
      '            if k in seen_pairs:',
      '                continue',
      '            # Skip overused donors',
      '            ra = (a.get("_origin","C"), a.get("_rownum",0))',
      '            rb = (b.get("_origin","C"), b.get("_rownum",0))',
      '            if donor_usage_count.get(ra, 0) >= max_usage or donor_usage_count.get(rb, 0) >= max_usage:',
      '                continue',
      '            fullA = (a.get("DonorFirst","")+" "+a.get("DonorLast","")).strip().upper()',
      '            fullB = (b.get("DonorFirst","")+" "+b.get("DonorLast","")).strip().upper()',
      '            addrA = (a.get("Address1","") or "").strip().upper()',
      '            addrB = (b.get("Address1","") or "").strip().upper()',
      '            if fullA == fullB and addrA and addrA == addrB:',
      '                continue',
      '            x = features(a,b)',
      '            score = 0.60*x[2] + 0.25*x[0] + 0.15*x[3]',
      '            if score >= 0.75 or x[2] >= 0.80:',
      '                cands.append((score, a, b, x))',
      '            if len(cands) >= want*80:',
      '                break',
      '        if len(cands) >= want*80:',
      '            break',
      '    cands.sort(key=lambda t: -t[0])',
      '    chosen = []; used = set()',
      '    for _, a, b, x in cands:',
      '        # one-use per row in a batch',
      '        ra = (a.get("_origin","C"), a.get("_rownum",0))',
      '        rb = (b.get("_origin","C"), b.get("_rownum",0))',
      '        if ra in used or rb in used:',
      '            continue',
      '        chosen.append((a, b, x))',
      '        used.add(ra); used.add(rb)',
      '        if len(chosen) >= want:',
      '            break',
      '    return chosen',
      '',
      'def uncertain_candidates(window_rows, want, seen_pairs, w, donor_usage_count, max_usage=3):',
      '    # Pairs closest to 0.5 under current model, skipping seen ones.',
      '    picks = []; L = len(window_rows)',
      '    if L < 2: return []',
      '    limit = min(L, 5000)',
      '    # More aggressive sampling to find uncertain pairs',
      '    if limit <= 2000:',
      '        step = 1  # Dense sampling for smaller windows',
      '    else:',
      '        step = max(1, limit // 1500)  # More dense than before',
      '    sampled = 0',
      '    # Increase search effort',
      '    max_samples = want * 1000',
      '    for i in range(0, limit, step):',
      '        a = window_rows[i]',
      '        for j in range(i+1, limit, step):',
      '            b = window_rows[j]',
      '            k = stable_pair_key(a, b)',
      '            if k in seen_pairs:',
      '                continue',
      '            # Skip overused donors',
      '            ra = (a.get("_origin","C"), a.get("_rownum",0))',
      '            rb = (b.get("_origin","C"), b.get("_rownum",0))',
      '            if donor_usage_count.get(ra, 0) >= max_usage or donor_usage_count.get(rb, 0) >= max_usage:',
      '                continue',
      '            x = features(a, b)',
      '            z = w[-1]',
      '            for t in range(0,5):',
      '                z += w[t]*x[t]',
      '            p = 1.0/(1.0+math.exp(-z))',
      '            u = abs(p - 0.5)',
      '            # Accept slightly wider uncertainty band (0.35 to 0.65)',
      '            if 0.35 <= p <= 0.65:',
      '                picks.append((u, p, a, b, x, p))',
      '            sampled += 1',
      '            if sampled >= max_samples:',
      '                break',
      '        if sampled >= max_samples:',
      '            break',
      '    # sort by lowest uncertainty, then prefer a bit higher p as tie-break',
      '    picks.sort(key=lambda t: (t[0], -t[1]))',
      '    chosen = []; used_rows = set()',
      '    for _, __, a, b, x, p in picks:',
      '        ra = (a.get("_origin","C"), a.get("_rownum",0))',
      '        rb = (b.get("_origin","C"), b.get("_rownum",0))',
      '        if ra in used_rows or rb in used_rows:',
      '            continue',
      '        chosen.append((a, b, x, p))',
      '        used_rows.add(ra); used_rows.add(rb)',
      '        if len(chosen) >= want:',
      '            break',
      '    return chosen',
      '',
      '# ----------------------------',
      '# CLI args and data',
      'ap = argparse.ArgumentParser()',
      'ap.add_argument("--bundle", required=True)',
      'ap.add_argument("--result", required=True)',
      'args = ap.parse_args()',
      '',
      'job = json.loads(http_get(args.bundle).decode("utf-8"))',
      '# Extract threshold from job config (default to 0.7 if not specified)',
      'predict_threshold = job.get("cfg", {}).get("predictThreshold", 0.7)',
      'print(f"Using prediction threshold: {predict_threshold}")',
      '',
      '# Load nickname database',
      'load_nickname_db(job["modelUrl"])',
      '',
      'hdrK, rowsK = parse_csv(http_get(job["krefUrl"]).decode("utf-8"))',
      'hdrF, rowsF = parse_csv(http_get(job["fecUrl"]).decode("utf-8"))',
      '',
      '# Complete ZIP prefix to State lookup (all 50 states + DC)',
      'ZIP_TO_STATE = {',
      '    # Massachusetts',
      '    "010": "MA", "011": "MA", "012": "MA", "013": "MA", "014": "MA", "015": "MA", "016": "MA", "017": "MA", "018": "MA", "019": "MA",',
      '    "020": "MA", "021": "MA", "022": "MA", "023": "MA", "024": "MA", "025": "MA", "026": "MA", "027": "MA",',
      '    # Rhode Island',
      '    "028": "RI", "029": "RI",',
      '    # New Hampshire',
      '    "030": "NH", "031": "NH", "032": "NH", "033": "NH", "034": "NH", "035": "NH", "036": "NH", "037": "NH", "038": "NH",',
      '    # Maine',
      '    "039": "ME", "040": "ME", "041": "ME", "042": "ME", "043": "ME", "044": "ME", "045": "ME", "046": "ME", "047": "ME", "048": "ME", "049": "ME",',
      '    # Vermont',
      '    "050": "VT", "051": "VT", "052": "VT", "053": "VT", "054": "VT", "055": "VT", "056": "VT", "057": "VT", "058": "VT", "059": "VT",',
      '    # Connecticut',
      '    "060": "CT", "061": "CT", "062": "CT", "063": "CT", "064": "CT", "065": "CT", "066": "CT", "067": "CT", "068": "CT", "069": "CT",',
      '    # New Jersey',
      '    "070": "NJ", "071": "NJ", "072": "NJ", "073": "NJ", "074": "NJ", "075": "NJ", "076": "NJ", "077": "NJ", "078": "NJ", "079": "NJ",',
      '    "080": "NJ", "081": "NJ", "082": "NJ", "083": "NJ", "084": "NJ", "085": "NJ", "086": "NJ", "087": "NJ", "088": "NJ", "089": "NJ",',
      '    # New York',
      '    "100": "NY", "101": "NY", "102": "NY", "103": "NY", "104": "NY", "105": "NY", "106": "NY", "107": "NY", "108": "NY", "109": "NY",',
      '    "110": "NY", "111": "NY", "112": "NY", "113": "NY", "114": "NY", "115": "NY", "116": "NY", "117": "NY", "118": "NY", "119": "NY",',
      '    "120": "NY", "121": "NY", "122": "NY", "123": "NY", "124": "NY", "125": "NY", "126": "NY", "127": "NY", "128": "NY", "129": "NY",',
      '    "130": "NY", "131": "NY", "132": "NY", "133": "NY", "134": "NY", "135": "NY", "136": "NY", "137": "NY", "138": "NY", "139": "NY",',
      '    "140": "NY", "141": "NY", "142": "NY", "143": "NY", "144": "NY", "145": "NY", "146": "NY", "147": "NY", "148": "NY", "149": "NY",',
      '    # Pennsylvania',
      '    "150": "PA", "151": "PA", "152": "PA", "153": "PA", "154": "PA", "155": "PA", "156": "PA", "157": "PA", "158": "PA", "159": "PA",',
      '    "160": "PA", "161": "PA", "162": "PA", "163": "PA", "164": "PA", "165": "PA", "166": "PA", "167": "PA", "168": "PA", "169": "PA",',
      '    "170": "PA", "171": "PA", "172": "PA", "173": "PA", "174": "PA", "175": "PA", "176": "PA", "177": "PA", "178": "PA", "179": "PA",',
      '    "180": "PA", "181": "PA", "182": "PA", "183": "PA", "184": "PA", "185": "PA", "186": "PA", "187": "PA", "188": "PA", "189": "PA",',
      '    "190": "PA", "191": "PA", "192": "PA", "193": "PA", "194": "PA", "195": "PA", "196": "PA",',
      '    # Delaware',
      '    "197": "DE", "198": "DE", "199": "DE",',
      '    # DC',
      '    "200": "DC", "201": "DC", "202": "DC", "203": "DC", "204": "DC", "205": "DC",',
      '    # Maryland',
      '    "206": "MD", "207": "MD", "208": "MD", "209": "MD", "210": "MD", "211": "MD", "212": "MD", "213": "MD", "214": "MD", "215": "MD",',
      '    "216": "MD", "217": "MD", "218": "MD", "219": "MD",',
      '    # Virginia',
      '    "220": "VA", "221": "VA", "222": "VA", "223": "VA", "224": "VA", "225": "VA", "226": "VA", "227": "VA", "228": "VA", "229": "VA",',
      '    "230": "VA", "231": "VA", "232": "VA", "233": "VA", "234": "VA", "235": "VA", "236": "VA", "237": "VA", "238": "VA", "239": "VA",',
      '    "240": "VA", "241": "VA", "242": "VA", "243": "VA", "244": "VA", "245": "VA", "246": "VA",',
      '    # West Virginia',
      '    "247": "WV", "248": "WV", "249": "WV", "250": "WV", "251": "WV", "252": "WV", "253": "WV", "254": "WV", "255": "WV", "256": "WV",',
      '    "257": "WV", "258": "WV", "259": "WV", "260": "WV", "261": "WV", "262": "WV", "263": "WV", "264": "WV", "265": "WV", "266": "WV",',
      '    "267": "WV", "268": "WV",',
      '    # North Carolina',
      '    "270": "NC", "271": "NC", "272": "NC", "273": "NC", "274": "NC", "275": "NC", "276": "NC", "277": "NC", "278": "NC", "279": "NC",',
      '    "280": "NC", "281": "NC", "282": "NC", "283": "NC", "284": "NC", "285": "NC", "286": "NC", "287": "NC", "288": "NC", "289": "NC",',
      '    # South Carolina',
      '    "290": "SC", "291": "SC", "292": "SC", "293": "SC", "294": "SC", "295": "SC", "296": "SC", "297": "SC", "298": "SC", "299": "SC",',
      '    # Florida',
      '    "300": "FL", "301": "FL", "302": "FL", "303": "FL", "304": "FL", "305": "FL", "306": "FL", "307": "FL", "308": "FL", "309": "FL",',
      '    "310": "FL", "311": "FL", "312": "FL", "313": "FL", "314": "FL", "315": "FL", "316": "FL", "317": "FL", "318": "FL", "319": "FL",',
      '    "320": "FL", "321": "FL", "322": "FL", "323": "FL", "324": "FL", "325": "FL", "326": "FL", "327": "FL", "328": "FL", "329": "FL",',
      '    "330": "FL", "331": "FL", "332": "FL", "333": "FL", "334": "FL", "335": "FL", "336": "FL", "337": "FL", "338": "FL", "339": "FL",',
      '    "341": "FL", "342": "FL", "343": "FL", "344": "FL", "345": "FL", "346": "FL", "347": "FL", "349": "FL",',
      '    # Alabama',
      '    "350": "AL", "351": "AL", "352": "AL", "353": "AL", "354": "AL", "355": "AL", "356": "AL", "357": "AL", "358": "AL", "359": "AL",',
      '    "360": "AL", "361": "AL", "362": "AL", "363": "AL", "364": "AL", "365": "AL", "366": "AL", "367": "AL", "368": "AL", "369": "AL",',
      '    # Tennessee',
      '    "370": "TN", "371": "TN", "372": "TN", "373": "TN", "374": "TN", "375": "TN", "376": "TN", "377": "TN", "378": "TN", "379": "TN",',
      '    "380": "TN", "381": "TN", "382": "TN", "383": "TN", "384": "TN", "385": "TN",',
      '    # Mississippi',
      '    "386": "MS", "387": "MS", "388": "MS", "389": "MS", "390": "MS", "391": "MS", "392": "MS", "393": "MS", "394": "MS", "395": "MS",',
      '    "396": "MS", "397": "MS",',
      '    # Georgia',
      '    "398": "GA", "399": "GA",',
      '    # Kentucky (IMPORTANT STATE - was completely missing!)',
      '    "400": "KY", "401": "KY", "402": "KY", "403": "KY", "404": "KY", "405": "KY", "406": "KY", "407": "KY", "408": "KY", "409": "KY",',
      '    "410": "KY", "411": "KY", "412": "KY", "413": "KY", "414": "KY", "415": "KY", "416": "KY", "417": "KY", "418": "KY",',
      '    "420": "KY", "421": "KY", "422": "KY", "423": "KY", "424": "KY", "425": "KY", "426": "KY", "427": "KY",',
      '    # Ohio',
      '    "430": "OH", "431": "OH", "432": "OH", "433": "OH", "434": "OH", "435": "OH", "436": "OH", "437": "OH", "438": "OH", "439": "OH",',
      '    "440": "OH", "441": "OH", "442": "OH", "443": "OH", "444": "OH", "445": "OH", "446": "OH", "447": "OH", "448": "OH", "449": "OH",',
      '    "450": "OH", "451": "OH", "452": "OH", "453": "OH", "454": "OH", "455": "OH", "456": "OH", "457": "OH", "458": "OH",',
      '    # Indiana',
      '    "460": "IN", "461": "IN", "462": "IN", "463": "IN", "464": "IN", "465": "IN", "466": "IN", "467": "IN", "468": "IN", "469": "IN",',
      '    "470": "IN", "471": "IN", "472": "IN", "473": "IN", "474": "IN", "475": "IN", "476": "IN", "477": "IN", "478": "IN", "479": "IN",',
      '    # Michigan',
      '    "480": "MI", "481": "MI", "482": "MI", "483": "MI", "484": "MI", "485": "MI", "486": "MI", "487": "MI", "488": "MI", "489": "MI",',
      '    "490": "MI", "491": "MI", "492": "MI", "493": "MI", "494": "MI", "495": "MI", "496": "MI", "497": "MI", "498": "MI", "499": "MI",',
      '    # Iowa',
      '    "500": "IA", "501": "IA", "502": "IA", "503": "IA", "504": "IA", "505": "IA", "506": "IA", "507": "IA", "508": "IA", "509": "IA",',
      '    "510": "IA", "511": "IA", "512": "IA", "513": "IA", "514": "IA", "515": "IA", "516": "IA", "520": "IA", "521": "IA", "522": "IA",',
      '    "523": "IA", "524": "IA", "525": "IA", "526": "IA", "527": "IA", "528": "IA",',
      '    # Wisconsin',
      '    "530": "WI", "531": "WI", "532": "WI", "534": "WI", "535": "WI", "537": "WI", "538": "WI", "539": "WI",',
      '    "540": "WI", "541": "WI", "542": "WI", "543": "WI", "544": "WI", "545": "WI", "546": "WI", "547": "WI", "548": "WI", "549": "WI",',
      '    # Minnesota',
      '    "550": "MN", "551": "MN", "553": "MN", "554": "MN", "555": "MN", "556": "MN", "557": "MN", "558": "MN", "559": "MN",',
      '    "560": "MN", "561": "MN", "562": "MN", "563": "MN", "564": "MN", "565": "MN", "566": "MN", "567": "MN",',
      '    # South Dakota',
      '    "570": "SD", "571": "SD", "572": "SD", "573": "SD", "574": "SD", "575": "SD", "576": "SD", "577": "SD",',
      '    # North Dakota',
      '    "580": "ND", "581": "ND", "582": "ND", "583": "ND", "584": "ND", "585": "ND", "586": "ND", "587": "ND", "588": "ND",',
      '    # Montana',
      '    "590": "MT", "591": "MT", "592": "MT", "593": "MT", "594": "MT", "595": "MT", "596": "MT", "597": "MT", "598": "MT", "599": "MT",',
      '    # Illinois',
      '    "600": "IL", "601": "IL", "602": "IL", "603": "IL", "604": "IL", "605": "IL", "606": "IL", "607": "IL", "608": "IL", "609": "IL",',
      '    "610": "IL", "611": "IL", "612": "IL", "613": "IL", "614": "IL", "615": "IL", "616": "IL", "617": "IL", "618": "IL", "619": "IL",',
      '    "620": "IL", "622": "IL", "623": "IL", "624": "IL", "625": "IL", "626": "IL", "627": "IL", "628": "IL", "629": "IL",',
      '    # Missouri',
      '    "630": "MO", "631": "MO", "633": "MO", "634": "MO", "635": "MO", "636": "MO", "637": "MO", "638": "MO", "639": "MO",',
      '    "640": "MO", "641": "MO", "644": "MO", "645": "MO", "646": "MO", "647": "MO", "648": "MO", "649": "MO",',
      '    "650": "MO", "651": "MO", "652": "MO", "653": "MO", "654": "MO", "655": "MO", "656": "MO", "657": "MO", "658": "MO",',
      '    # Kansas',
      '    "660": "KS", "661": "KS", "662": "KS", "664": "KS", "665": "KS", "666": "KS", "667": "KS", "668": "KS", "669": "KS",',
      '    "670": "KS", "671": "KS", "672": "KS", "673": "KS", "674": "KS", "675": "KS", "676": "KS", "677": "KS", "678": "KS", "679": "KS",',
      '    # Nebraska',
      '    "680": "NE", "681": "NE", "683": "NE", "684": "NE", "685": "NE", "686": "NE", "687": "NE", "688": "NE", "689": "NE",',
      '    "690": "NE", "691": "NE", "692": "NE", "693": "NE",',
      '    # Louisiana',
      '    "700": "LA", "701": "LA", "703": "LA", "704": "LA", "705": "LA", "706": "LA", "707": "LA", "708": "LA",',
      '    "710": "LA", "711": "LA", "712": "LA", "713": "LA", "714": "LA",',
      '    # Arkansas',
      '    "716": "AR", "717": "AR", "718": "AR", "719": "AR", "720": "AR", "721": "AR", "722": "AR", "723": "AR", "724": "AR", "725": "AR",',
      '    "726": "AR", "727": "AR", "728": "AR", "729": "AR",',
      '    # Oklahoma',
      '    "730": "OK", "731": "OK", "734": "OK", "735": "OK", "736": "OK", "737": "OK", "738": "OK", "739": "OK",',
      '    "740": "OK", "741": "OK", "743": "OK", "744": "OK", "745": "OK", "746": "OK", "747": "OK", "748": "OK", "749": "OK",',
      '    # Texas',
      '    "750": "TX", "751": "TX", "752": "TX", "753": "TX", "754": "TX", "755": "TX", "756": "TX", "757": "TX", "758": "TX", "759": "TX",',
      '    "760": "TX", "761": "TX", "762": "TX", "763": "TX", "764": "TX", "765": "TX", "766": "TX", "767": "TX", "768": "TX", "769": "TX",',
      '    "770": "TX", "771": "TX", "772": "TX", "773": "TX", "774": "TX", "775": "TX", "776": "TX", "777": "TX", "778": "TX", "779": "TX",',
      '    "780": "TX", "781": "TX", "782": "TX", "783": "TX", "784": "TX", "785": "TX", "786": "TX", "787": "TX", "788": "TX", "789": "TX",',
      '    "790": "TX", "791": "TX", "792": "TX", "793": "TX", "794": "TX", "795": "TX", "796": "TX", "797": "TX", "798": "TX", "799": "TX",',
      '    # Colorado',
      '    "800": "CO", "801": "CO", "802": "CO", "803": "CO", "804": "CO", "805": "CO", "806": "CO", "807": "CO", "808": "CO", "809": "CO",',
      '    "810": "CO", "811": "CO", "812": "CO", "813": "CO", "814": "CO", "815": "CO", "816": "CO",',
      '    # Wyoming',
      '    "820": "WY", "821": "WY", "822": "WY", "823": "WY", "824": "WY", "825": "WY", "826": "WY", "827": "WY", "828": "WY", "829": "WY",',
      '    "830": "WY", "831": "WY",',
      '    # Idaho',
      '    "832": "ID", "833": "ID", "834": "ID", "835": "ID", "836": "ID", "837": "ID", "838": "ID",',
      '    # Utah',
      '    "840": "UT", "841": "UT", "842": "UT", "843": "UT", "844": "UT", "845": "UT", "846": "UT", "847": "UT",',
      '    # Arizona',
      '    "850": "AZ", "851": "AZ", "852": "AZ", "853": "AZ", "855": "AZ", "856": "AZ", "857": "AZ", "859": "AZ",',
      '    "860": "AZ", "863": "AZ", "864": "AZ", "865": "AZ",',
      '    # New Mexico',
      '    "870": "NM", "871": "NM", "872": "NM", "873": "NM", "874": "NM", "875": "NM", "877": "NM", "878": "NM", "879": "NM",',
      '    "880": "NM", "881": "NM", "882": "NM", "883": "NM", "884": "NM",',
      '    # Nevada',
      '    "889": "NV", "890": "NV", "891": "NV", "893": "NV", "894": "NV", "895": "NV", "897": "NV", "898": "NV",',
      '    # California',
      '    "900": "CA", "901": "CA", "902": "CA", "903": "CA", "904": "CA", "905": "CA", "906": "CA", "907": "CA", "908": "CA",',
      '    "910": "CA", "911": "CA", "912": "CA", "913": "CA", "914": "CA", "915": "CA", "916": "CA", "917": "CA", "918": "CA",',
      '    "919": "CA", "920": "CA", "921": "CA", "922": "CA", "923": "CA", "924": "CA", "925": "CA", "926": "CA", "927": "CA",',
      '    "928": "CA", "930": "CA", "931": "CA", "932": "CA", "933": "CA", "934": "CA", "935": "CA", "936": "CA", "937": "CA",',
      '    "938": "CA", "939": "CA", "940": "CA", "941": "CA", "942": "CA", "943": "CA", "944": "CA", "945": "CA", "946": "CA",',
      '    "947": "CA", "948": "CA", "949": "CA", "950": "CA", "951": "CA", "952": "CA", "953": "CA", "954": "CA", "955": "CA",',
      '    "956": "CA", "957": "CA", "958": "CA", "959": "CA", "960": "CA", "961": "CA",',
      '    # Hawaii',
      '    "967": "HI", "968": "HI",',
      '    # Oregon',
      '    "970": "OR", "971": "OR", "972": "OR", "973": "OR", "974": "OR", "975": "OR", "976": "OR", "977": "OR", "978": "OR", "979": "OR",',
      '    # Washington',
      '    "980": "WA", "981": "WA", "982": "WA", "983": "WA", "984": "WA", "985": "WA", "986": "WA", "988": "WA", "989": "WA",',
      '    "990": "WA", "991": "WA", "992": "WA", "993": "WA", "994": "WA",',
      '    # Alaska',
      '    "995": "AK", "996": "AK", "997": "AK", "998": "AK", "999": "AK"',
      '}',
      '',
      'def lookup_state_from_zip(zip_code):',
      '    if not zip_code: return ""',
      '    zip_str = str(zip_code).strip()[:3]',
      '    return ZIP_TO_STATE.get(zip_str, "")',
      '',
      '# Valid US state abbreviations (50 states + DC + territories)',
      'VALID_US_STATES = {',
      '    "AL", "AK", "AZ", "AR", "CA", "CO", "CT", "DE", "FL", "GA",',
      '    "HI", "ID", "IL", "IN", "IA", "KS", "KY", "LA", "ME", "MD",',
      '    "MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ",',
      '    "NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA", "RI", "SC",',
      '    "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY",',
      '    "DC", "PR", "VI", "GU", "AS", "MP"',
      '}',
      '',
      'def is_broken_value(val):',
      '    if not val: return True',
      '    s = str(val).strip().upper()',
      '    # Check if empty, NA, or other broken values',
      '    if s in ["", "NA", "N/A", "UNKNOWN", "NULL"]: return True',
      '    # Also check if it\'s not a valid US state',
      '    return s not in VALID_US_STATES',
      '',
      'def row_as_obj(header, row):',
      '    m = dict(zip(header, row))',
      '    state = (m.get("State","") or "").strip()',
      '    zip_code = (m.get("Zip","") or "").strip()',
      '    # Fill in missing state from ZIP code',
      '    if is_broken_value(state) and zip_code:',
      '        state = lookup_state_from_zip(zip_code)',
      '    # Preserve ALL columns for output, normalize matching fields',
      '    return {',
      '        # Matching fields (normalized)',
      '        "DonorFirst": (m.get("DonorFirst","") or "").strip(),',
      '        "DonorLast":  (m.get("DonorLast","") or "").strip(),',
      '        "Address1":   (m.get("Address1","") or "").strip(),',
      '        "City":       (m.get("City","") or "").strip(),',
      '        "State":      state,',
      '        "Zip":        zip_code,',
      '        "Employer":   (m.get("Employer","") or "").strip(),',
      '        "Occupation": (m.get("Occupation","") or "").strip(),',
      '        # Output-only fields (preserved from CSV)',
      '        "Recipient":      (m.get("Recipient","") or "").strip(),',
      '        "Address2":       (m.get("Address2","") or "").strip(),',
      '        "Amount":         (m.get("Amount","") or "").strip(),',
      '        "WeightedAmount": (m.get("WeightedAmount","") or "").strip(),',
      '        "ReceiptDate":    (m.get("ReceiptDate","") or "").strip(),',
      '        "DonationID":     (m.get("DonationID","") or "").strip()',
      '    }',
      '',
      'K_rows = [row_as_obj(hdrK, r) for r in rowsK]',
      'F_rows = [row_as_obj(hdrF, r) for r in rowsF]',
      'COMBINED = []',
      'global_idx = 0',
      'for i, r in enumerate(K_rows, start=2):',
      '    o = dict(r); o["_origin"] = "K"; o["_rownum"] = i; o["_globalIdx"] = global_idx; COMBINED.append(o); global_idx += 1',
      'for j, r in enumerate(F_rows, start=2):',
      '    o = dict(r); o["_origin"] = "F"; o["_rownum"] = j; o["_globalIdx"] = global_idx; COMBINED.append(o); global_idx += 1',
      'print(f"Loaded {len(COMBINED)} donors total")',
      '',
      '# Session storage',
      'training_rows = []',
      'seen_pairs = set()',
      'w = [0.0]*18  # 17 features (5 similarity + 2 geographic + 10 interactions) + bias',
      'selected_donors = []  # Progressive random sample',
      'remaining_pool = list(COMBINED)  # Pool of not-yet-selected donors',
      'random.shuffle(remaining_pool)  # Shuffle once at start',
      'donor_usage_count = {}  # Track how many times each donor appears in training',
      'feature_names = ["name", "last", "addr", "employer", "occupation", "zip", "city", "name*last", "name*addr", "name*employer", "name*occupation", "last*addr", "last*employer", "last*occupation", "addr*employer", "addr*occupation", "employer*occupation"]',
      'loaded_training_count = 0  # Track how many training rows were loaded (vs newly added)',
      '',
      '# ----------------------------',
      '# Check for existing training data',
      'print("\\nChecking for existing training data...")',
      'existing_model = http_get(job["modelUrl"]).decode("utf-8")',
      'existing_data = json.loads(existing_model)',
      'is_continuing = False  # Track if we loaded existing data',
      'job_mode = job.get("mode", "full")  # Get job mode (full or incremental)',
      'print(f"DEBUG: Job mode detected as: {job_mode}")',
      'print(f"DEBUG: Job keys: {list(job.keys())}")',
      '',
      'if existing_data.get("weights"):',
      '    weights_count = len(existing_data.get("weights", []))',
      '    print(f"Found existing model with {weights_count} weights")',
      '    ',
      '    # Incremental mode: automatically load existing data',
      '    if job_mode == "incremental":',
      '        print("INCREMENTAL MODE: Loading existing training data...")',
      '        training_url = job["modelUrl"].replace("model=1", "training=1")',
      '        training_json = http_get(training_url).decode("utf-8")',
      '        training_data = json.loads(training_json)',
      '        ',
      '        if training_data.get("training_rows"):',
      '            raw_training_rows = training_data["training_rows"]',
      '            ',
      '            # Deduplicate training rows - keep only unique pairs',
      '            deduped_rows = []',
      '            seen_pair_keys = set()',
      '            duplicates_removed = 0',
      '            for row in raw_training_rows:',
      '                a_key = (row["a"]["origin"], row["a"]["row"])',
      '                b_key = (row["b"]["origin"], row["b"]["row"])',
      '                pair_key = (a_key, b_key) if a_key <= b_key else (b_key, a_key)',
      '                if pair_key not in seen_pair_keys:',
      '                    seen_pair_keys.add(pair_key)',
      '                    deduped_rows.append(row)',
      '                else:',
      '                    duplicates_removed += 1',
      '            ',
      '            training_rows = deduped_rows',
      '            if duplicates_removed > 0:',
      '                print(f"  Removed {duplicates_removed} duplicate pairs from training data")',
      '            ',
      '            old_w = existing_data["weights"]',
      '            ',
      '            # Handle model version compatibility',
      '            if len(old_w) == 16:',
      '                print("  Detected old model format (15 features + bias)")',
      '                print("  Upgrading to new model format (17 features + bias)")',
      '                print("  Adding 2 new geographic features (zip, city)")',
      '                # Expand weights: old had 15 features + bias, new has 17 features + bias',
      '                # Insert zeros at positions 5 and 6 (zip and city features)',
      '                w = old_w[:5] + [0.0, 0.0] + old_w[5:]',
      '                print(f"  Expanded weights from {len(old_w)} to {len(w)} elements")',
      '            elif len(old_w) == 18:',
      '                print("  Model already in new format (17 features + bias)")',
      '                w = old_w',
      '            else:',
      '                print(f"  WARNING: Unexpected weight count: {len(old_w)}")',
      '                print(f"  Starting fresh with {18} zero weights")',
      '                w = [0.0] * 18',
      '            ',
      '            # NOTE: is_continuing stays False so we run training loop',
      '            loaded_training_count = len(training_rows)',
      '            ',
      '            # Build seen_pairs from loaded data',
      '            for row in training_rows:',
      '                a_key = (row["a"]["origin"], row["a"]["row"])',
      '                b_key = (row["b"]["origin"], row["b"]["row"])',
      '                pair_key = (a_key, b_key) if a_key <= b_key else (b_key, a_key)',
      '                seen_pairs.add(pair_key)',
      '                ',
      '                # Track donor usage',
      '                donor_usage_count[a_key] = donor_usage_count.get(a_key, 0) + 1',
      '                donor_usage_count[b_key] = donor_usage_count.get(b_key, 0) + 1',
      '            ',
      '            print(f"Loaded {len(training_rows)} existing labeled pairs")',
      '            print(f"Model has {len(w)} weights")',
      '            ',
      '            # Learn semantic associations from existing training data',
      '            print("Learning semantic associations from training data...")',
      '            (first_associations, last_associations, address_associations,',
      '             city_associations, state_associations, zip_associations,',
      '             employer_associations, occupation_associations) = learn_associations(training_rows)',
      '            first_pairs = sum(len(v) for v in first_associations.values()) // 2',
      '            last_pairs = sum(len(v) for v in last_associations.values()) // 2',
      '            addr_pairs = sum(len(v) for v in address_associations.values()) // 2',
      '            city_pairs = sum(len(v) for v in city_associations.values()) // 2',
      '            state_pairs = sum(len(v) for v in state_associations.values()) // 2',
      '            zip_pairs = sum(len(v) for v in zip_associations.values()) // 2',
      '            emp_pairs = sum(len(v) for v in employer_associations.values()) // 2',
      '            occ_pairs = sum(len(v) for v in occupation_associations.values()) // 2',
      '            print(f"  Learned {first_pairs} first name associations (nicknames)")',
      '            print(f"  Learned {last_pairs} last name associations")',
      '            print(f"  Learned {addr_pairs} address associations")',
      '            print(f"  Learned {city_pairs} city associations")',
      '            print(f"  Learned {state_pairs} state associations")',
      '            print(f"  Learned {zip_pairs} zip associations")',
      '            print(f"  Learned {emp_pairs} employer associations")',
      '            print(f"  Learned {occ_pairs} occupation associations")',
      '            if first_pairs > 0 or addr_pairs > 0 or emp_pairs > 0 or occ_pairs > 0:',
      '                print("  (e.g., Bob  Robert, N/A  RETIRED)")',
      '            print("You can now add more training pairs to improve the model.\\n")',
      '        else:',
      '            print("No training data found, starting fresh")',
      '    # Full mode: ask user to load or start new',
      '    else:',
      '        while True:',
      '            choice = read_tty("(L)oad existing training data and continue, or start (N)ew? (l/n): ").strip().lower()',
      '            if choice == "l":',
      '                print("Loading existing training data...")',
      '                # Fetch training data',
      '                training_url = job["modelUrl"].replace("model=1", "training=1")',
      '                training_json = http_get(training_url).decode("utf-8")',
      '                training_data = json.loads(training_json)',
      '                ',
      '                if training_data.get("training_rows"):',
      '                    raw_training_rows = training_data["training_rows"]',
      '                    ',
      '                    # Deduplicate training rows - keep only unique pairs',
      '                    deduped_rows = []',
      '                    seen_pair_keys = set()',
      '                    duplicates_removed = 0',
      '                    for row in raw_training_rows:',
      '                        a_key = (row["a"]["origin"], row["a"]["row"])',
      '                        b_key = (row["b"]["origin"], row["b"]["row"])',
      '                        pair_key = (a_key, b_key) if a_key <= b_key else (b_key, a_key)',
      '                        if pair_key not in seen_pair_keys:',
      '                            seen_pair_keys.add(pair_key)',
      '                            deduped_rows.append(row)',
      '                        else:',
      '                            duplicates_removed += 1',
      '                    ',
      '                    training_rows = deduped_rows',
      '                    if duplicates_removed > 0:',
      '                        print(f"  Removed {duplicates_removed} duplicate pairs from training data")',
      '                    ',
      '                    old_w = existing_data["weights"]',
      '                    ',
      '                    # Handle model version compatibility',
      '                    if len(old_w) == 16:',
      '                        print("  Detected old model format (15 features + bias)")',
      '                        print("  Upgrading to new model format (17 features + bias)")',
      '                        print("  Adding 2 new geographic features (zip, city)")',
      '                        # Expand weights: old had 15 features + bias, new has 17 features + bias',
      '                        # Insert zeros at positions 5 and 6 (zip and city features)',
      '                        w = old_w[:5] + [0.0, 0.0] + old_w[5:]',
      '                        print(f"  Expanded weights from {len(old_w)} to {len(w)} elements")',
      '                    elif len(old_w) == 18:',
      '                        print("  Model already in new format (17 features + bias)")',
      '                        w = old_w',
      '                    else:',
      '                        print(f"  WARNING: Unexpected weight count: {len(old_w)}")',
      '                        print(f"  Starting fresh with {18} zero weights")',
      '                        w = [0.0] * 18',
      '                    ',
      '                    is_continuing = True  # Mark as continuing (skip training loop)',
      '                    loaded_training_count = len(training_rows)  # Track how many we loaded',
      '                    ',
      '                    # Build seen_pairs from loaded data',
      '                    for row in training_rows:',
      '                        a_key = (row["a"]["origin"], row["a"]["row"])',
      '                        b_key = (row["b"]["origin"], row["b"]["row"])',
      '                        pair_key = (a_key, b_key) if a_key <= b_key else (b_key, a_key)',
      '                        seen_pairs.add(pair_key)',
      '                        ',
      '                        # Track donor usage',
      '                        donor_usage_count[a_key] = donor_usage_count.get(a_key, 0) + 1',
      '                        donor_usage_count[b_key] = donor_usage_count.get(b_key, 0) + 1',
      '                    ',
      '                    print(f"Loaded {len(training_rows)} existing labeled pairs")',
      '                    print(f"Model has {len(w)} weights")',
      '                    ',
      '                    # Learn semantic associations from existing training data',
      '                    print("Learning semantic associations from training data...")',
      '                    (first_associations, last_associations, address_associations,',
      '                     city_associations, state_associations, zip_associations,',
      '                     employer_associations, occupation_associations) = learn_associations(training_rows)',
      '                    first_pairs = sum(len(v) for v in first_associations.values()) // 2',
      '                    last_pairs = sum(len(v) for v in last_associations.values()) // 2',
      '                    addr_pairs = sum(len(v) for v in address_associations.values()) // 2',
      '                    city_pairs = sum(len(v) for v in city_associations.values()) // 2',
      '                    state_pairs = sum(len(v) for v in state_associations.values()) // 2',
      '                    zip_pairs = sum(len(v) for v in zip_associations.values()) // 2',
      '                    emp_pairs = sum(len(v) for v in employer_associations.values()) // 2',
      '                    occ_pairs = sum(len(v) for v in occupation_associations.values()) // 2',
      '                    print(f"  Learned {first_pairs} first name associations (nicknames)")',
      '                    print(f"  Learned {last_pairs} last name associations")',
      '                    print(f"  Learned {addr_pairs} address associations")',
      '                    print(f"  Learned {city_pairs} city associations")',
      '                    print(f"  Learned {state_pairs} state associations")',
      '                    print(f"  Learned {zip_pairs} zip associations")',
      '                    print(f"  Learned {emp_pairs} employer associations")',
      '                    print(f"  Learned {occ_pairs} occupation associations")',
      '                else:',
      '                    print("No training data found, starting fresh")',
      '                break',
      '            elif choice == "n":',
      '                print("Starting new training session...")',
      '                # Clear any existing weights so we start fresh',
      '                print("Clearing any previously saved model weights...")',
      '                clear_url = job["modelUrl"].replace("model=1", "clear=1")',
      '                try:',
      '                    http_get(clear_url)',
      '                    print("Old model cleared.")',
      '                except:',
      '                    print("(No old model to clear)")',
      '                break',
      '            else:',
      '                print(\'Please enter "l" to load or "n" for new.\')',
      'else:',
      '    print("No existing model found. Starting fresh.")',
      '    if job_mode == "incremental":',
      '        print("WARNING: Incremental mode requested but no existing training data found.")',
      '        print("This will behave the same as full mode.\\n")',
      '',
      '# ----------------------------',
      '# Retraining function',
      'def retrain_model(training_rows, w, epochs=50, lr=0.1):',
      '    """Do multiple passes through training data to improve model."""',
      '    if not training_rows:',
      '        return w',
      '    for epoch in range(epochs):',
      '        # Shuffle training data each epoch',
      '        shuffled = list(training_rows)',
      '        random.shuffle(shuffled)',
      '        for row in shuffled:',
      '            # Extract all 17 features from stored training row',
      '            f = row.get("features", {})',
      '            x = [f.get("nameSim",0), f.get("lastSim",0), f.get("addrSim",0), f.get("empSim",0), f.get("occSim",0),',
      '                 f.get("zipMatch",0), f.get("cityMatch",0),',
      '                 f.get("name_last",0), f.get("name_addr",0), f.get("name_emp",0), f.get("name_occ",0),',
      '                 f.get("last_addr",0), f.get("last_emp",0), f.get("last_occ",0),',
      '                 f.get("addr_emp",0), f.get("addr_occ",0), f.get("emp_occ",0)]',
      '            y = row.get("label", 0)',
      '            w[:] = sgd_update(w, x, y, lr)',
      '    return w',
      '',
      '# ----------------------------',
      '# Weight analysis and blocking strategy generation',
      'def analyze_weights(w, feature_names):',
      '    """Analyze learned weights to identify top features/interactions."""',
      '    # Get absolute weights (excluding bias)',
      '    weights_with_names = [(abs(w[i]), w[i], feature_names[i], i) for i in range(len(feature_names))]',
      '    weights_with_names.sort(key=lambda x: -x[0])  # Sort by absolute value descending',
      '    return weights_with_names',
      '',
      'def generate_blocking_key(donor, feature_name):',
      '    """Generate a blocking key for a donor based on feature name."""',
      '    # Extract relevant fields based on feature',
      '    if "name" in feature_name and "last" not in feature_name:',
      '        # Use full name soundex',
      '        full_name = (donor.get("DonorFirst","") + " " + donor.get("DonorLast","")).strip().upper()',
      '        if not full_name: return None',
      '        # Simple soundex-like: first letter + next 2 consonants',
      '        clean = re.sub(r"[^A-Z]", "", full_name)',
      '        return clean[:4] if len(clean) >= 4 else clean',
      '    ',
      '    if "last" in feature_name:',
      '        # Use last name soundex',
      '        last = donor.get("DonorLast","").strip().upper()',
      '        if not last: return None',
      '        clean = re.sub(r"[^A-Z]", "", last)',
      '        return clean[:3] if len(clean) >= 3 else clean',
      '    ',
      '    if "addr" in feature_name:',
      '        # Use zip code or city+state',
      '        zip_code = donor.get("Zip","").strip()',
      '        if zip_code and len(zip_code) >= 5:',
      '            return zip_code[:5]',
      '        city = donor.get("City","").strip().upper()',
      '        state = donor.get("State","").strip().upper()',
      '        if city and state:',
      '            return city + ":" + state',
      '        return None',
      '    ',
      '    if "employer" in feature_name:',
      '        # Use employer token',
      '        emp = donor.get("Employer","").strip().upper()',
      '        if not emp or emp == "N/A": return None',
      '        clean = re.sub(r"[^A-Z0-9]", " ", emp)',
      '        tokens = [t for t in clean.split() if len(t) >= 3 and t not in ["THE", "INC", "LLC", "LTD"]]',
      '        return tokens[0] if tokens else None',
      '    ',
      '    if "occupation" in feature_name:',
      '        # Use occupation token',
      '        occ = donor.get("Occupation","").strip().upper()',
      '        if not occ or occ == "N/A": return None',
      '        clean = re.sub(r"[^A-Z]", " ", occ)',
      '        tokens = [t for t in clean.split() if len(t) >= 4]',
      '        return tokens[0] if tokens else None',
      '    ',
      '    return None',
      '',
      'def generate_interaction_blocking_key(donor, feature_name):',
      '    """Generate blocking key for interaction features (e.g., last*employer)."""',
      '    parts = feature_name.split("*")',
      '    if len(parts) != 2:',
      '        return generate_blocking_key(donor, feature_name)',
      '    ',
      '    key1 = generate_blocking_key(donor, parts[0])',
      '    key2 = generate_blocking_key(donor, parts[1])',
      '    ',
      '    if key1 and key2:',
      '        return f"{key1}|{key2}"',
      '    return None',
      '',
      'def adaptive_candidates(window_rows, want, seen_pairs, w, feature_names, donor_usage_count, threshold, max_usage=3):',
      '    """Find candidate pairs using learned blocking strategies."""',
      '    # Calculate uncertainty band centered on the threshold',
      '    # Band width shrinks as threshold approaches 1.0',
      '    # At 0.5: 0.15 (30% range)',
      '    # At 0.7: 0.15 (30% range)',
      '    # At 0.9: 0.09 (18% range)',
      '    # At 0.95: 0.045 (9% range)',
      '    base_band = 0.15',
      '    # Scale band proportionally: shrinks as threshold approaches 1.0',
      '    band_width = base_band * (1.0 - threshold) / 0.5 if threshold > 0.5 else base_band',
      '    lower_bound = max(0.0, threshold - band_width)',
      '    upper_bound = min(1.0, threshold + band_width)',
      '    ',
      '    print(f"  Uncertainty band: {lower_bound:.2f} to {upper_bound:.2f} (centered on threshold {threshold:.2f})")',
      '    ',
      '    # Analyze weights to get top features',
      '    weight_analysis = analyze_weights(w, feature_names)',
      '    ',
      '    print(f"  Top learned features: {[\', \'.join([f\'{name}={weight:.2f}\' for _, weight, name, _ in weight_analysis[:5]])]}")',
      '    ',
      '    # Use top 3-5 features for blocking',
      '    top_features = weight_analysis[:5]',
      '    ',
      '    # Build blocks using top features',
      '    all_blocks = {}',
      '    for abs_w, w_val, feat_name, feat_idx in top_features:',
      '        if abs_w < 0.1:  # Skip very weak features',
      '            continue',
      '        ',
      '        blocks = {}',
      '        for donor in window_rows:',
      '            if "*" in feat_name:',
      '                key = generate_interaction_blocking_key(donor, feat_name)',
      '            else:',
      '                key = generate_blocking_key(donor, feat_name)',
      '            ',
      '            if key:',
      '                if key not in blocks:',
      '                    blocks[key] = []',
      '                blocks[key].append(donor)',
      '        ',
      '        # Merge blocks from this feature',
      '        for key, donors in blocks.items():',
      '            if len(donors) >= 2:  # Only keep blocks with 2+ donors',
      '                block_id = f"{feat_name}:{key}"',
      '                all_blocks[block_id] = donors',
      '    ',
      '    print(f"  Generated {len(all_blocks)} blocks from top features")',
      '    ',
      '    # Collect candidate pairs from all blocks',
      '    candidates = []',
      '    seen_candidate_pairs = set()',
      '    ',
      '    for block_id, block_donors in all_blocks.items():',
      '        if len(block_donors) > 100:  # Skip very large blocks',
      '            continue',
      '        ',
      '        for i in range(len(block_donors)):',
      '            for j in range(i+1, len(block_donors)):',
      '                a = block_donors[i]',
      '                b = block_donors[j]',
      '                ',
      '                pair_key = stable_pair_key(a, b)',
      '                if pair_key in seen_pairs or pair_key in seen_candidate_pairs:',
      '                    continue',
      '                ',
      '                # Skip overused donors',
      '                ra = (a.get("_origin","C"), a.get("_rownum",0))',
      '                rb = (b.get("_origin","C"), b.get("_rownum",0))',
      '                if donor_usage_count.get(ra, 0) >= max_usage or donor_usage_count.get(rb, 0) >= max_usage:',
      '                    continue',
      '                ',
      '                seen_candidate_pairs.add(pair_key)',
      '                ',
      '                # Compute features and predict',
      '                x = features(a, b)',
      '                z = w[-1]  # bias',
      '                for t in range(len(x)):',
      '                    z += w[t] * x[t]',
      '                p = 1.0 / (1.0 + math.exp(-z))',
      '                u = abs(p - threshold)  # Uncertainty is distance from threshold',
      '                ',
      '                # Accept pairs in uncertainty band',
      '                if lower_bound <= p <= upper_bound:',
      '                    candidates.append((u, p, a, b, x, p))',
      '    ',
      '    print(f"  Found {len(candidates)} uncertain pairs in blocks")',
      '    ',
      '    # Sort by uncertainty and select diverse pairs',
      '    candidates.sort(key=lambda t: (t[0], -t[1]))',
      '    ',
      '    chosen = []',
      '    used_rows = set()',
      '    for _, __, a, b, x, p in candidates:',
      '        ra = (a.get("_origin","C"), a.get("_rownum",0))',
      '        rb = (b.get("_origin","C"), b.get("_rownum",0))',
      '        if ra in used_rows or rb in used_rows:',
      '            continue',
      '        chosen.append((a, b, x, p))',
      '        used_rows.add(ra)',
      '        used_rows.add(rb)',
      '        if len(chosen) >= want:',
      '            break',
      '    ',
      '    return chosen',
      '',
      '# ----------------------------',
      '# Rounds',
      'def run_round(round_num):',
      '    # Add 1000 more random donors to selected pool',
      '    batch_size = 1000',
      '    to_add = min(batch_size, len(remaining_pool))',
      '    if to_add == 0:',
      '        print("No more donors to add.")',
      '        return False',
      '    for _ in range(to_add):',
      '        selected_donors.append(remaining_pool.pop())',
      '    print(f"Round {round_num}: Now considering {len(selected_donors)} total donors (added {to_add} new)")',
      '    ',
      '    if len(selected_donors) < 2:',
      '        print("Not enough donors to compare.")',
      '        return False',
      '    ',
      '    # Generate candidate pairs from current selected set',
      '    # Skip cold start if we loaded existing training data',
      '    if round_num == 1 and loaded_training_count == 0:',
      '        ask = 15',
      '        batch = cold_start_candidates(selected_donors, ask, seen_pairs, donor_usage_count)',
      '    else:',
      '        ask = 5',
      '        print("Using adaptive blocking based on learned weights...")',
      '        batch = adaptive_candidates(selected_donors, ask, seen_pairs, w, feature_names, donor_usage_count, predict_threshold)',
      '    ',
      '    if not batch:',
      '        print("No new candidate pairs found this round.")',
      '        return True',
      '    ',
      '    labeled_count = 0',
      '    for i, item in enumerate(batch, start=1):',
      '        # cold_start_candidates returns 3 values, adaptive_candidates returns 4',
      '        if round_num == 1 and loaded_training_count == 0:',
      '            a, b, x = item; p = 0.5',
      '        else:',
      '            a, b, x, p = item',
      '        y = show_pair_and_get_label(i, len(batch), a, b, x, p)',
      '        ',
      '        if y == 2:  # Skip',
      '            print("  Skipped.")',
      '            continue',
      '        ',
      '        # Only process if labeled (0 or 1)',
      '        training_rows.append({',
      '          "a":{"origin":a.get("_origin","C"), "row":a.get("_rownum",0), "first":a.get("DonorFirst",""), "last":a.get("DonorLast",""), "addr":a.get("Address1",""), "city":a.get("City",""), "state":a.get("State",""), "zip":a.get("Zip",""), "employer":a.get("Employer",""), "occupation":a.get("Occupation","")},',
      '          "b":{"origin":b.get("_origin","C"), "row":b.get("_rownum",0), "first":b.get("DonorFirst",""), "last":b.get("DonorLast",""), "addr":b.get("Address1",""), "city":b.get("City",""), "state":b.get("State",""), "zip":b.get("Zip",""), "employer":b.get("Employer",""), "occupation":b.get("Occupation","")},',
      '          "features": features_dict(a, b),',
      '          "label": int(y)',
      '        })',
      '        w[:] = sgd_update(w, x, int(y), lr=0.1)',
      '        seen_pairs.add(stable_pair_key(a, b))',
      '        # Track donor usage',
      '        ra = (a.get("_origin","C"), a.get("_rownum",0))',
      '        rb = (b.get("_origin","C"), b.get("_rownum",0))',
      '        donor_usage_count[ra] = donor_usage_count.get(ra, 0) + 1',
      '        donor_usage_count[rb] = donor_usage_count.get(rb, 0) + 1',
      '        labeled_count += 1',
      '    ',
      '    print(f"Round {round_num} complete. Labeled {len(batch)} pairs. Total labels: {len(training_rows)}")',
      '    ',
      '    # Retrain model on all data after each round',
      '    if len(training_rows) > 0:',
      '        print(f"Retraining model on {len(training_rows)} labeled pairs...")',
      '        w[:] = retrain_model(training_rows, w, epochs=50, lr=0.1)',
      '        print("Model retrained.")',
      '        ',
      '        # Update associations with new training data',
      '        (first_associations, last_associations, address_associations,',
      '         city_associations, state_associations, zip_associations,',
      '         employer_associations, occupation_associations) = learn_associations(training_rows)',
      '    ',
      '    print(f"Donors remaining in pool: {len(remaining_pool)}")',
      '    return True',
      '',
      'if not is_continuing:',
      '    # Start fresh training',
      '    round_num = 1',
      '    while True:',
      '        if not run_round(round_num):',
      '            break',
      '        if len(remaining_pool) == 0:',
      '            print("All donors have been included in training!")',
      '            break',
      '        # Require valid y/n response',
      '        while True:',
      '            ans = read_tty("Continue to next round? (y/n): ").strip().lower()',
      '            if ans == "y":',
      '                break',
      '            elif ans == "n":',
      '                print("Training stopped by user.")',
      '                break',
      '            else:',
      '                print(\'Please enter "y" to continue or "n" to stop.\')',
      '        if ans == "n":',
      '            break',
      '        round_num += 1',
      '    ',
      '    # After training loop: Check if incremental mode - if so, save and exit WITHOUT matching',
      '    if job_mode == "incremental":',
      '        print("\\n" + "="*60)',
      '        print("INCREMENTAL TRAINING COMPLETE")',
      '        print(f"Total labeled pairs: {len(training_rows)}")',
      '        print(f"New pairs added this session: {len(training_rows) - loaded_training_count}")',
      '        print("="*60)',
      '        ',
      '        # Save updated model and training data',
      '        print("\\nSaving updated training data and model...")',
      '        payload = {',
      '            "training_rows": training_rows,',
      '            "model_weights": w',
      '        }',
      '        response = http_post(job["resultUrl"], payload)',
      '        print(" Training data saved successfully!")',
      '        print(" Model weights updated")',
      '        print("\\nTo run final matching with your updated training, use:")',
      '        print("  \'Donor Tools  Run locally: prepare job and show Terminal command\'")',
      '        sys.exit(0)  # Exit without running final matching',
      '',
      'else:',
      '    print("Using existing trained model - proceeding directly to final matching...")',
      '',
      '# ----------------------------',
      '# Final Matching Phase',
      'print("\\n" + "="*60)',
      'print("TRAINING COMPLETE")',
      'print(f"Total labeled pairs: {len(training_rows)}")',
      'print("="*60)',
      'print("\\nStarting final matching on all donors...")',
      '',
      'threshold = job.get("cfg", {}).get("predictThreshold", 0.7)',
      'print(f"Using match threshold: {threshold}")',
      '',
      '# Analyze weights to determine blocking strategy',
      'weight_analysis = analyze_weights(w, feature_names)',
      'print(f"Top features for blocking: {[\', \'.join([f\'{name}={weight:.2f}\' for _, weight, name, _ in weight_analysis[:3]])]}")',
      '',
      '# Multi-phase matching strategy',
      'print("\\n" + "="*60)',
      'print("DONOR DEDUPLICATION STRATEGY")',
      'print("="*60)',
      'print("Phase 0: Exact matching (normalized capitalization)")',
      'print("Phase 1: Limited matching (blocks 50 donors)")',
      'print("Phase 2: Thorough matching (blocks 200 donors)")',
      'print("Phase 3: Exhaustive matching (unlimited block size)")',
      'print("="*60)',
      '',
      '# Use Union-Find for clustering',
      'parent = list(range(len(COMBINED)))',
      'rank = [0] * len(COMBINED)',
      'donor_confidence = {}  # Track confidence per donor',
      '',
      'def uf_find(x):',
      '    if parent[x] != x:',
      '        parent[x] = uf_find(parent[x])',
      '    return parent[x]',
      '',
      'def uf_union(a, b):',
      '    pa = uf_find(a)',
      '    pb = uf_find(b)',
      '    if pa == pb:',
      '        return',
      '    if rank[pa] < rank[pb]:',
      '        pa, pb = pb, pa',
      '    parent[pb] = pa',
      '    if rank[pa] == rank[pb]:',
      '        rank[pa] += 1',
      '',
      'matches_found = 0',
      'pairs_checked = 0',
      'seen_final_pairs = set()',
      '',
      '# ----------------------------',
      '# PHASE 0: Exact matches (normalized)',
      'print("\\n" + "="*60)',
      'print("PHASE 0: Exact Matching")',
      'print("="*60)',
      'exact_blocks = {}',
      'for donor in COMBINED:',
      '    # Normalize: uppercase, strip, remove extra whitespace',
      '    name = (donor.get("DonorFirst","") + " " + donor.get("DonorLast","")).strip().upper()',
      '    name = " ".join(name.split())  # Remove extra whitespace',
      '    # Use enhanced address normalization (handles abbreviations)',
      '    addr = normalize_address(donor.get("Address1",""))',
      '    key = f"{name}|{addr}"',
      '    if key and name and addr:  # Skip empty keys',
      '        if key not in exact_blocks:',
      '            exact_blocks[key] = []',
      '        exact_blocks[key].append(donor)',
      '',
      'phase0_matches = 0',
      'for key, donors in exact_blocks.items():',
      '    if len(donors) >= 2:',
      '        for i in range(len(donors)):',
      '            for j in range(i+1, len(donors)):',
      '                a_idx = donors[i].get("_globalIdx", -1)',
      '                b_idx = donors[j].get("_globalIdx", -1)',
      '                if a_idx != -1 and b_idx != -1 and uf_find(a_idx) != uf_find(b_idx):',
      '                    uf_union(a_idx, b_idx)',
      '                    donor_confidence[a_idx] = 1.0',
      '                    donor_confidence[b_idx] = 1.0',
      '                    phase0_matches += 1',
      '',
      'print(f"  Found {phase0_matches} exact matches")',
      'matches_found += phase0_matches',
      '',
      '# ----------------------------',
      '# PHASE 1: Limited matching',
      'print("\\n" + "="*60)',
      'print("PHASE 1: Limited Matching")',
      'print("="*60)',
      'top_features_phase1 = weight_analysis[:3]  # Use top 3 features',
      'phase1_blocks = {}',
      '',
      'for abs_w, w_val, feat_name, feat_idx in top_features_phase1:',
      '    if abs_w < 0.1:',
      '        continue',
      '    blocks = {}',
      '    for donor in COMBINED:',
      '        if "*" in feat_name:',
      '            key = generate_interaction_blocking_key(donor, feat_name)',
      '        else:',
      '            key = generate_blocking_key(donor, feat_name)',
      '        if key:',
      '            if key not in blocks:',
      '                blocks[key] = []',
      '            blocks[key].append(donor)',
      '    for key, donors in blocks.items():',
      '        if len(donors) >= 2 and len(donors) <= 50:  # Tight limit',
      '            block_id = f"P2_{feat_name}:{key}"',
      '            phase2_blocks[block_id] = donors',
      '',
      'print(f"  Generated {len(phase2_blocks)} blocks")',
      'phase2_matches = 0',
      'phase2_checked = 0',
      '',
      'for block_id, block_donors in phase2_blocks.items():',
      '    for i in range(len(block_donors)):',
      '        for j in range(i+1, len(block_donors)):',
      '            a = block_donors[i]',
      '            b = block_donors[j]',
      '            a_idx = a.get("_globalIdx", -1)',
      '            b_idx = b.get("_globalIdx", -1)',
      '            if a_idx == -1 or b_idx == -1:',
      '                continue',
      '            # Skip if already matched in Phase 1',
      '            if uf_find(a_idx) == uf_find(b_idx):',
      '                continue',
      '            pair_key = (min(a_idx, b_idx), max(a_idx, b_idx))',
      '            if pair_key in seen_final_pairs:',
      '                continue',
      '            seen_final_pairs.add(pair_key)',
      '            x = features(a, b)',
      '            z = w[-1]',
      '            for t in range(len(x)):',
      '                z += w[t] * x[t]',
      '            p = 1.0 / (1.0 + math.exp(-z))',
      '            phase2_checked += 1',
      '            if p >= threshold:',
      '                uf_union(a_idx, b_idx)',
      '                donor_confidence[a_idx] = max(donor_confidence.get(a_idx, 0), p)',
      '                donor_confidence[b_idx] = max(donor_confidence.get(b_idx, 0), p)',
      '                phase2_matches += 1',
      '',
      'print(f"  Checked {phase2_checked} pairs, found {phase2_matches} matches")',
      'matches_found += phase2_matches',
      'pairs_checked += phase2_checked',
      '',
      '# ----------------------------',
      '# PHASE 3: Thorough matching',
      'print("\\n" + "="*60)',
      'print("PHASE 3: Thorough Matching")',
      'print("="*60)',
      'top_features_phase3 = weight_analysis[:5]  # Use top 5 features',
      'phase3_blocks = {}',
      '',
      'for abs_w, w_val, feat_name, feat_idx in top_features_phase3:',
      '    if abs_w < 0.1:',
      '        continue',
      '    blocks = {}',
      '    for donor in COMBINED:',
      '        if "*" in feat_name:',
      '            key = generate_interaction_blocking_key(donor, feat_name)',
      '        else:',
      '            key = generate_blocking_key(donor, feat_name)',
      '        if key:',
      '            if key not in blocks:',
      '                blocks[key] = []',
      '            blocks[key].append(donor)',
      '    for key, donors in blocks.items():',
      '        if len(donors) >= 2 and len(donors) <= 200:  # Broad limit',
      '            block_id = f"P3_{feat_name}:{key}"',
      '            phase3_blocks[block_id] = donors',
      '',
      'print(f"  Generated {len(phase3_blocks)} blocks")',
      'phase3_matches = 0',
      'phase3_checked = 0',
      '',
      'for block_id, block_donors in phase3_blocks.items():',
      '    for i in range(len(block_donors)):',
      '        for j in range(i+1, len(block_donors)):',
      '            a = block_donors[i]',
      '            b = block_donors[j]',
      '            a_idx = a.get("_globalIdx", -1)',
      '            b_idx = b.get("_globalIdx", -1)',
      '            if a_idx == -1 or b_idx == -1:',
      '                continue',
      '            # Skip if already matched in Phase 1 or 2',
      '            if uf_find(a_idx) == uf_find(b_idx):',
      '                continue',
      '            pair_key = (min(a_idx, b_idx), max(a_idx, b_idx))',
      '            if pair_key in seen_final_pairs:',
      '                continue',
      '            seen_final_pairs.add(pair_key)',
      '            x = features(a, b)',
      '            z = w[-1]',
      '            for t in range(len(x)):',
      '                z += w[t] * x[t]',
      '            p = 1.0 / (1.0 + math.exp(-z))',
      '            phase3_checked += 1',
      '            if p >= threshold:',
      '                uf_union(a_idx, b_idx)',
      '                donor_confidence[a_idx] = max(donor_confidence.get(a_idx, 0), p)',
      '                donor_confidence[b_idx] = max(donor_confidence.get(b_idx, 0), p)',
      '                phase3_matches += 1',
      '',
      'print(f"  Checked {phase3_checked} pairs, found {phase3_matches} matches")',
      'matches_found += phase3_matches',
      'pairs_checked += phase3_checked',
      '',
      '# Summary',
      'print("\\n" + "="*60)',
      'print("MATCHING COMPLETE")',
      'print("="*60)',
      'print(f"Phase 1 (Exact):  {phase1_matches} matches")',
      'print(f"Phase 2 (Quick):  {phase2_matches} matches ({phase2_checked} pairs checked)")',
      'print(f"Phase 3 (Broad):  {phase3_matches} matches ({phase3_checked} pairs checked)")',
      'print(f"TOTAL: {matches_found} matches ({pairs_checked} pairs checked)")',
      'print("="*60)',
      '',
      '# Assign DonorIDs',
      'components = {}',
      'for i in range(len(COMBINED)):',
      '    root = uf_find(i)',
      '    if root not in components:',
      '        components[root] = []',
      '    components[root].append(i)',
      '',
      'donor_assignments = {}',
      'next_id = 1',
      '',
      'for root, members in components.items():',
      '    for member_idx in members:',
      '        donor = COMBINED[member_idx]',
      '        origin = donor.get("_origin", "C")',
      '        rownum = donor.get("_rownum", 0)',
      '        key = f"{origin}:{rownum}"',
      '        ',
      '        # Solo donors get confidence 1.0',
      '        conf = donor_confidence.get(member_idx, 1.0) if len(members) > 1 else 1.0',
      '        ',
      '        donor_assignments[key] = {',
      '            "donorId": next_id,',
      '            "confidence": round(conf, 4)',
      '        }',
      '    next_id += 1',
      '',
      'print(f"\\nAssigned {next_id - 1} unique DonorIDs to {len(COMBINED)} donors")',
      'unique_matched = sum(1 for members in components.values() if len(members) > 1)',
      'print(f"Unique donors with matches: {unique_matched}")',
      'print(f"Solo donors (no matches): {len(components) - unique_matched}")',
      '',
      '# Build complete merge output rows locally',
      'print(f"\\nBuilding merge output rows...")',
      'merge_rows = []',
      '',
      '# Process each donor in COMBINED and build output row',
      'for donor in COMBINED:',
      '    origin = donor.get("_origin", "C")',
      '    rownum = donor.get("_rownum", 0)',
      '    key = f"{origin}:{rownum}"',
      '    ',
      '    # Get assignment for this donor',
      '    assignment = donor_assignments.get(key, {"donorId": "", "confidence": ""})',
      '    ',
      '    # Format date if present',
      '    receipt_date = donor.get("ReceiptDate", "")',
      '    if isinstance(receipt_date, str):',
      '        receipt_date_str = receipt_date',
      '    else:',
      '        receipt_date_str = str(receipt_date) if receipt_date else ""',
      '    ',
      '    # Build prefixed DonationID (K:12345 or F:67890)',
      '    donation_id = donor.get("DonationID", "")',
      '    if not donation_id:',
      '        donation_id = str(rownum)',
      '    prefixed_donation_id = f"{origin}:{donation_id}"',
      '    ',
      '    # Build complete row',
      '    row = [',
      '        donor.get("Recipient", origin),',
      '        donor.get("DonorFirst", ""),',
      '        donor.get("DonorLast", ""),',
      '        donor.get("Address1", ""),',
      '        donor.get("Address2", ""),',
      '        donor.get("City", ""),',
      '        donor.get("State", ""),',
      '        donor.get("Zip", ""),',
      '        donor.get("Amount", ""),',
      '        donor.get("WeightedAmount", ""),',
      '        receipt_date_str,',
      '        donor.get("Occupation", ""),',
      '        donor.get("Employer", ""),',
      '        prefixed_donation_id,',
      '        assignment["donorId"],',
      '        assignment["confidence"]',
      '    ]',
      '    merge_rows.append(row)',
      '',
      'print(f"Built {len(merge_rows)} output rows")',
      '',
      '# Upload results to Apps Script',
      'new_training_rows = training_rows[loaded_training_count:]',
      'print("\\n" + "="*60)',
      'print("UPLOADING RESULTS TO GOOGLE SHEETS")',
      'print("="*60)',
      'print(f"  - Training rows: {len(new_training_rows)}")',
      'print(f"  - Merge output rows: {len(merge_rows)}")',
      'print(f"  - Model weights: {len(w)}")',
      '',
      '# Send in batches of 5000 rows (safe under 10 MB payload limit)',
      'batch_size = 5000',
      'num_batches = math.ceil(len(merge_rows) / batch_size) if merge_rows else 1',
      '',
      'print(f"  Sending in {num_batches} batch(es) of {batch_size} rows each...")',
      '',
      'for batch_idx in range(num_batches):',
      '    start = batch_idx * batch_size',
      '    end = min(start + batch_size, len(merge_rows))',
      '    batch_rows = merge_rows[start:end]',
      '    ',
      '    is_final = (batch_idx == num_batches - 1)',
      '    ',
      '    payload = {',
      '        "progress": {',
      '            "phase": "Complete" if is_final else "Uploading",',
      '            "done": end,',
      '            "total": len(merge_rows)',
      '        },',
      '        "batch_index": batch_idx,',
      '        "merge_rows": batch_rows',
      '    }',
      '    ',
      '    # Include training rows and weights only in first batch',
      '    if batch_idx == 0:',
      '        payload["training_rows"] = new_training_rows',
      '        payload["model_weights"] = w',
      '    ',
      '    try:',
      '        response_raw = http_post(job["resultUrl"], payload)',
      '        response = json.loads(response_raw)',
      '        ',
      '        if not response.get("ok"):',
      '            print(f"\\n Upload failed: {response.get(\'error\', \'unknown\')}")',
      '            break',
      '        ',
      '        # Show progress',
      '        if not is_final:',
      '            pct = 100 * end / len(merge_rows)',
      '            print(f"  Progress: {end}/{len(merge_rows)} rows ({pct:.1f}%)")',
      '        else:',
      '            print(f"\\n Upload complete: {len(merge_rows)} rows sent")',
      '            if "merge_output" in response:',
      '                merge = response["merge_output"]',
      '                if merge.get("success"):',
      '                    print(f"\\n Merge output complete")',
      '                    print(f"  - Total rows: {merge.get(\'rows\', \'unknown\')}")',
      '                else:',
      '                    print(f"\\n Merge failed: {merge.get(\'error\', \'unknown\')}")',
      '        ',
      '        # Small delay between batches',
      '        if not is_final:',
      '            time.sleep(0.5)',
      '            ',
      '    except json.JSONDecodeError:',
      '        print(f"\\n Invalid response from server")',
      '        print(f"First 500 chars: {response_raw[:500]}")',
      '        break',
      '    except Exception as e:',
      '        print(f"\\n Upload error: {e}")',
      '        break'
    ].join('\n');

    return ContentService.createTextOutput(py).setMimeType(ContentService.MimeType.TEXT);
  }

  // Job bundle for the runner
  if (p.job == '1') {
    const chk = dl_validateToken_(p.token);
    if (!chk.ok) return ContentService.createTextOutput(chk.msg).setMimeType(ContentService.MimeType.TEXT);
    const job = PropertiesService.getDocumentProperties().getProperty('dl_job_json') || '{}';
    return ContentService.createTextOutput(job).setMimeType(ContentService.MimeType.JSON);
  }

  // Diagnostic job bundle endpoint
  if (p.diag_job == '1') {
    const chk = dl_validateToken_(p.token);
    if (!chk.ok) return ContentService.createTextOutput(chk.msg).setMimeType(ContentService.MimeType.TEXT);
    const job = PropertiesService.getDocumentProperties().getProperty('diag_job_json') || '{}';
    return ContentService.createTextOutput(job).setMimeType(ContentService.MimeType.JSON);
  }

  // CSV endpoints for runner input
  if (p.csv == 'kref' || p.csv == 'fec') {
    const chk = dl_validateToken_(p.token);
    if (!chk.ok) return ContentService.createTextOutput(chk.msg).setMimeType(ContentService.MimeType.TEXT);
    const props = PropertiesService.getDocumentProperties();
    const fileId = p.csv === 'kref' ? props.getProperty('dl_kref_fileId') : props.getProperty('dl_fec_fileId');
    if (!fileId) return ContentService.createTextOutput('No CSV file id').setMimeType(ContentService.MimeType.TEXT);
    const file = DriveApp.getFileById(fileId);
    const out = ContentService.createTextOutput(file.getBlob().getDataAsString());
    out.setMimeType(ContentService.MimeType.CSV);
    return out;
  }

  // Model endpoint for runner to fetch current weights/prior if any
  if (p.model == '1') {
    const props = PropertiesService.getDocumentProperties();
    const w = props.getProperty('donor_model_weights');
    const fp = props.getProperty('donor_first_pair_table');
    const body = {
      weights: w ? JSON.parse(w) : null,
      first_pair_table: fp ? JSON.parse(fp) : {}
    };
    return ContentService.createTextOutput(JSON.stringify(body)).setMimeType(ContentService.MimeType.JSON);
  }

  // Diagnostic script endpoint
  if (p.script == '1') {
    return diagnostic_getScript_();
  }

  // Clear model endpoint - delete saved weights
  if (p.clear == '1') {
    const props = PropertiesService.getDocumentProperties();
    props.deleteProperty('donor_model_weights');
    props.deleteProperty('donor_first_pair_table');
    return ContentService.createTextOutput(JSON.stringify({ ok: true, message: 'Model cleared' })).setMimeType(ContentService.MimeType.JSON);
  }

  // Training data endpoint - fetch existing training rows
  if (p.training == '1') {
    const ss = SpreadsheetApp.getActive();
    const sh = ss.getSheetByName(DONOR_CFG.trainingSheet);

    if (!sh || sh.getLastRow() < 2) {
      return ContentService.createTextOutput(JSON.stringify({ training_rows: [] })).setMimeType(ContentService.MimeType.JSON);
    }

    const vals = sh.getDataRange().getValues();
    const header = vals[0];
    const trainingRows = [];

    for (let i = 1; i < vals.length; i++) {
      const row = vals[i];
      const label = Number(row[0]);
      if (label !== 0 && label !== 1) continue; // Skip unlabeled

      // Parse the row into the format Python expects
      trainingRows.push({
        a: {
          origin: String(row[1] || ''),
          row: Number(row[2] || 0),
          first: String(row[3] || ''),
          last: String(row[4] || ''),
          addr: String(row[5] || ''),
          city: String(row[6] || ''),
          state: String(row[7] || ''),
          zip: String(row[8] || ''),
          employer: String(row[9] || ''),
          occupation: String(row[10] || '')
        },
        b: {
          origin: String(row[11] || ''),
          row: Number(row[12] || 0),
          first: String(row[13] || ''),
          last: String(row[14] || ''),
          addr: String(row[15] || ''),
          city: String(row[16] || ''),
          state: String(row[17] || ''),
          zip: String(row[18] || ''),
          employer: String(row[19] || ''),
          occupation: String(row[20] || '')
        },
        features: {
          nameSim: Number(row[21] || 0),
          lastSim: Number(row[22] || 0),
          addrSim: Number(row[24] || 0),
          empSim: Number(row[31] || 0),
          occSim: Number(row[32] || 0),
          // Compute interactions from base features
          name_last: Number(row[21] || 0) * Number(row[22] || 0),
          name_addr: Number(row[21] || 0) * Number(row[24] || 0),
          name_emp: Number(row[21] || 0) * Number(row[31] || 0),
          name_occ: Number(row[21] || 0) * Number(row[32] || 0),
          last_addr: Number(row[22] || 0) * Number(row[24] || 0),
          last_emp: Number(row[22] || 0) * Number(row[31] || 0),
          last_occ: Number(row[22] || 0) * Number(row[32] || 0),
          addr_emp: Number(row[24] || 0) * Number(row[31] || 0),
          addr_occ: Number(row[24] || 0) * Number(row[32] || 0),
          emp_occ: Number(row[31] || 0) * Number(row[32] || 0)
        },
        label: label
      });
    }

    return ContentService.createTextOutput(JSON.stringify({ training_rows: trainingRows })).setMimeType(ContentService.MimeType.JSON);
  }

  // Nicknames endpoint - serve nickname pairs for enhanced matching
  if (p.nicknames == '1') {
    const pairs = donor_ensureNicknamesSheet_();
    // Convert to object format for easy lookup in Python
    const nicknameMap = {};
    for (const pair of pairs) {
      const name1 = String(pair[0] || '').toUpperCase();
      const name2 = String(pair[1] || '').toUpperCase();
      if (!name1 || !name2) continue;

      // Add bidirectional mappings
      if (!nicknameMap[name1]) nicknameMap[name1] = [];
      if (!nicknameMap[name1].includes(name2)) nicknameMap[name1].push(name2);

      if (!nicknameMap[name2]) nicknameMap[name2] = [];
      if (!nicknameMap[name2].includes(name1)) nicknameMap[name2].push(name1);
    }
    return ContentService.createTextOutput(JSON.stringify({ nicknames: nicknameMap })).setMimeType(ContentService.MimeType.JSON);
  }

  // Campaign Deputy CSV endpoints
  if (p.cd_csv == 'merge' || p.cd_csv == 'cd') {
    return cd_serveCsv_(p.cd_csv, p.token);
  }

  // Campaign Deputy matcher Python script
  if (p.cd_matcher == '1') {
    return cd_getMatcherScript_();
  }

  // Diagnostic analysis Python script
  if (p.diagnostic == '1') {
    return diagnostic_getScript_();
  }

  return ContentService.createTextOutput('Invalid query').setMimeType(ContentService.MimeType.TEXT);
}


function doPost(e) {
  const p = e.parameter || {};

  // Handle Campaign Deputy matching results separately
  if (p.cd_result == '1') {
    const chk = cd_validateToken_(p.token);
    if (!chk.ok) {
      return ContentService.createTextOutput(JSON.stringify({ ok: false, error: chk.msg })).setMimeType(ContentService.MimeType.JSON);
    }

    let body = {};
    try {
      body = e.postData && e.postData.contents ? JSON.parse(e.postData.contents) : {};
    } catch (err) {
      return ContentService.createTextOutput(JSON.stringify({ ok: false, error: 'Invalid JSON' })).setMimeType(ContentService.MimeType.JSON);
    }

    const result = cd_receiveResults_(body);
    return ContentService.createTextOutput(JSON.stringify(result)).setMimeType(ContentService.MimeType.JSON);
  }

  // Regular donor matching
  const chk = dl_validateToken_(p.token);
  if (!chk.ok) {
    return ContentService.createTextOutput(JSON.stringify({ ok: false, error: chk.msg })).setMimeType(ContentService.MimeType.JSON);
  }

  let body = {};
  try {
    body = e.postData && e.postData.contents ? JSON.parse(e.postData.contents) : {};
  } catch (err) {
    return ContentService.createTextOutput(JSON.stringify({ ok: false, error: 'Invalid JSON' })).setMimeType(ContentService.MimeType.JSON);
  }

  const responseData = { ok: true };

  if (body.progress) {
    dl_setProgress_(String(body.progress.phase || ''), Number(body.progress.done || 0), Number(body.progress.total || 0), String(body.progress.note || ''));
  }

  if (Array.isArray(body.training_rows) && body.training_rows.length) {
    donor_applyResult_trainingRows_(body.training_rows);
    responseData.training_saved = body.training_rows.length;
  }

  if (Array.isArray(body.model_weights) && body.model_weights.length) {
    donor_applyResult_modelWeights_(body.model_weights);
    responseData.weights_saved = body.model_weights.length;
  }

  if (body.id_map && typeof body.id_map === 'object') {
    donor_applyResult_assignedIds_(body.id_map);
  }

  // Handle new merge_rows format (Python builds complete rows)
  if (Array.isArray(body.merge_rows) && body.merge_rows.length) {
    const batchSize = body.merge_rows.length;
    const batchIndex = typeof body.batch_index === 'number' ? body.batch_index : -1;
    Logger.log('Received batch ' + batchIndex + ' with ' + batchSize + ' merge rows');

    const ss = SpreadsheetApp.getActive();
    const mergeSheet = donor_getOrCreateSheet_('Merge output');

    // Define output header
    const outputHeader = [
      'Recipient', 'DonorFirst', 'DonorLast', 'Address1', 'Address2',
      'City', 'State', 'Zip', 'Amount', 'WeightedAmount', 'ReceiptDate',
      'Occupation', 'Employer', 'DonationID', 'DonorID', 'Confidence'
    ];

    // Clear sheet on first batch of new run
    const isFirstBatch = batchIndex === 0;

    if (isFirstBatch) {
      // Clear all data rows (row 2 and below), preserve header if it exists
      const lastRow = mergeSheet.getLastRow();
      if (lastRow > 1) {
        mergeSheet.deleteRows(2, lastRow - 1);
        Logger.log('Cleared data rows (2-' + lastRow + ') from Merge output sheet');
      }

      // Ensure header exists and is formatted
      const hasHeader = mergeSheet.getLastRow() >= 1 && mergeSheet.getRange(1, 1).getValue() !== '';
      if (!hasHeader) {
        mergeSheet.getRange(1, 1, 1, outputHeader.length).setValues([outputHeader]);
        Logger.log('Created header row');
      }

      // Format header
      mergeSheet.getRange(1, 1, 1, outputHeader.length)
        .setFontWeight('bold')
        .setBackground('#4285f4')
        .setFontColor('#ffffff');
      mergeSheet.setFrozenRows(1);
      Logger.log('Prepared Merge output sheet for new run');
    }

    // Append rows to Merge output
    const startRow = mergeSheet.getLastRow() + 1;
    mergeSheet.getRange(startRow, 1, body.merge_rows.length, outputHeader.length)
      .setValues(body.merge_rows);

    const totalRows = mergeSheet.getLastRow() - 1; // Exclude header
    Logger.log('Wrote ' + batchSize + ' rows. Total in sheet: ' + totalRows);

    // Unhide all rows and columns to ensure data is visible
    if (mergeSheet.getMaxRows() > 0) {
      mergeSheet.showRows(1, mergeSheet.getMaxRows());
    }
    if (mergeSheet.getMaxColumns() > 0) {
      mergeSheet.showColumns(1, mergeSheet.getMaxColumns());
    }

    responseData.merge_output = {
      success: true,
      rows_in_batch: batchSize,
      total_rows: totalRows,
      batch_index: batchIndex,
      message: 'Batch written to Merge output'
    };
  }

  return ContentService.createTextOutput(JSON.stringify(responseData)).setMimeType(ContentService.MimeType.JSON);
}

/** Apply helpers for POST payloads */
function donor_applyResult_trainingRows_(rows) {
  const sh = donor_getOrCreateSheet_(DONOR_CFG.trainingSheet);

  // Clear the sheet and rewrite everything to remove duplicates
  sh.clear();
  sh.getRange(1, 1, 1, DONOR_TRAINING_HEADER.length).setValues([DONOR_TRAINING_HEADER]);
  sh.setFrozenRows(1);
  sh.getRange(1, 1, 1, DONOR_TRAINING_HEADER.length)
    .setFontWeight('bold')
    .setBackground('#4285f4')
    .setFontColor('#ffffff');

  const data = [];
  for (let i = 0; i < rows.length; i++) {
    const r = rows[i];
    const a = r.a || {};
    const b = r.b || {};
    const f = r.features || {};
    const label = (r.label === 0 || r.label === 1) ? r.label : '';
    data.push([
      label,
      String(a.origin || ''), Number(a.row || 2), String(a.first || ''), String(a.last || ''), String(a.addr || ''), String(a.city || ''), String(a.state || ''), String(a.zip || ''), String(a.employer || ''), String(a.occupation || ''),
      String(b.origin || ''), Number(b.row || 2), String(b.first || ''), String(b.last || ''), String(b.addr || ''), String(b.city || ''), String(b.state || ''), String(b.zip || ''), String(b.employer || ''), String(b.occupation || ''),
      Number(f.nameSim||0), Number(f.lastSim||0), Number(f.addrSim||0), Number(f.empSim||0), Number(f.occSim||0),
      Number(f.zipMatch||0), Number(f.cityMatch||0),
      Number(f.name_last||0), Number(f.name_addr||0), Number(f.name_emp||0), Number(f.name_occ||0),
      Number(f.last_addr||0), Number(f.last_emp||0), Number(f.last_occ||0),
      Number(f.addr_emp||0), Number(f.addr_occ||0), Number(f.emp_occ||0)
    ]);
  }
  if (data.length) sh.getRange(2, 1, data.length, DONOR_TRAINING_HEADER.length).setValues(data);
}

function donor_applyResult_modelWeights_(weights) {
  PropertiesService.getDocumentProperties()
    .setProperty('donor_model_weights', JSON.stringify(weights));
}

function donor_applyResult_assignedIds_(idMapObj) {
  const ss = SpreadsheetApp.getActive();
  function writeTo(sheetName, originTag) {
    const sh = ss.getSheetByName(sheetName);
    if (!sh) return;
    const header = sh.getRange(1, 1, 1, sh.getLastColumn()).getValues()[0];
    let idCol = header.indexOf(DONOR_CFG.idColumnName) + 1;
    if (idCol === 0) {
      idCol = header.length + 1;
      sh.getRange(1, idCol).setValue(DONOR_CFG.idColumnName);
    }
    const last = sh.getLastRow();
    const out = new Array(Math.max(0, last - 1)).fill(['']);
    Object.keys(idMapObj).forEach(k => {
      const parts = String(k).split(':');
      if (parts.length !== 2) return;
      if (parts[0] !== originTag) return;
      const row = Number(parts[1] || 0);
      const id = idMapObj[k];
      if (row >= 2 && row <= last) out[row - 2] = [id];
    });
    if (out.length) sh.getRange(2, idCol, out.length, 1).setValues(out);
  }
  writeTo(DONOR_CFG.krefSheet, 'K');
  writeTo(DONOR_CFG.fecSheet, 'F');
}

function donor_applyResult_donorAssignments_(assignments) {
  // Don't store in PropertiesService - it's too large!
  // Instead, pass directly to build function

  Logger.log('Building donor ID list with ' + Object.keys(assignments).length + ' assignments');
  const result = donor_buildDonorIdList(assignments);

  if (result.success) {
    Logger.log('Successfully built donor ID list: ' + result.message);
  } else {
    Logger.log('ERROR building donor ID list: ' + result.error);
  }

  return result;
}

/**
 * Format a date value to simple M/D/YYYY string
 * Handles Date objects, strings, and other values
 */
function donor_formatDate_(val) {
  if (!val) return '';

  // If it's already a string and not a Date object toString, return as-is
  if (typeof val === 'string') return val;

  // If it's a Date object, format it simply
  if (val instanceof Date && !isNaN(val.getTime())) {
    const month = val.getMonth() + 1;  // 0-indexed
    const day = val.getDate();
    const year = val.getFullYear();
    return `${month}/${day}/${year}`;
  }

  // Try to parse as date if it's a number or other type
  try {
    const d = new Date(val);
    if (!isNaN(d.getTime())) {
      const month = d.getMonth() + 1;
      const day = d.getDate();
      const year = d.getFullYear();
      return `${month}/${day}/${year}`;
    }
  } catch (e) {
    // Ignore parse errors
  }

  return String(val);
}

function donor_buildDonorIdList(assignments) {
  const ss = SpreadsheetApp.getActive();

  try {
    // Use assignments parameter instead of loading from PropertiesService
    if (!assignments) {
      // Fallback: try to load from PropertiesService (for manual menu calls)
      const assignmentsRaw = PropertiesService.getDocumentProperties()
        .getProperty('donor_final_assignments');

      if (!assignmentsRaw) {
        throw new Error('No donor assignments found. Please run the training process first.');
      }

      assignments = JSON.parse(assignmentsRaw);
    }

    // Load KREF and FEC sheets with ALL columns
    const krefSheet = ss.getSheetByName(DONOR_CFG.krefSheet);
    const fecSheet = ss.getSheetByName(DONOR_CFG.fecSheet);

    if (!krefSheet || !fecSheet) {
      throw new Error('Missing KREF_Exports or FEC_Exports sheets.');
    }

    // Read all data
    const krefData = krefSheet.getDataRange().getValues();
    const fecData = fecSheet.getDataRange().getValues();

    if (krefData.length < 2 && fecData.length < 2) {
      throw new Error('No data found in KREF or FEC sheets.');
    }

    // Define output header
    const outputHeader = [
      'Recipient', 'DonorFirst', 'DonorLast', 'Address1', 'Address2',
      'City', 'State', 'Zip', 'Amount', 'WeightedAmount', 'ReceiptDate',
      'Occupation', 'Employer', 'DonationID', 'DonorID', 'Confidence'
    ];

    const outputRows = [outputHeader];

    // Helper to find column index
    function findCol(header, name) {
      const idx = header.indexOf(name);
      if (idx === -1) throw new Error('Column not found: ' + name);
      return idx;
    }

    // Process KREF
    if (krefData.length > 1) {
      const krefHeader = krefData[0];
      try {
        const colRecipient = krefHeader.indexOf('Recipient'); // Optional
        const colFirst = findCol(krefHeader, 'DonorFirst');
        const colLast = findCol(krefHeader, 'DonorLast');
        const colAddr1 = findCol(krefHeader, 'Address1');
        const colAddr2 = krefHeader.indexOf('Address2'); // Optional
        const colCity = findCol(krefHeader, 'City');
        const colState = findCol(krefHeader, 'State');
        const colZip = findCol(krefHeader, 'Zip');
        const colAmount = findCol(krefHeader, 'Amount');
        const colWeightedAmount = findCol(krefHeader, 'WeightedAmount');
        const colReceiptDate = findCol(krefHeader, 'ReceiptDate');
        const colOccupation = findCol(krefHeader, 'Occupation');
        const colEmployer = findCol(krefHeader, 'Employer');

        for (let i = 1; i < krefData.length; i++) {
          const row = krefData[i];
          const key = 'K:' + (i + 1);
          const assignment = assignments[key] || { donorId: '', confidence: '' };

          outputRows.push([
            colRecipient >= 0 ? (row[colRecipient] || 'K') : 'K',
            row[colFirst] || '',
            row[colLast] || '',
            row[colAddr1] || '',
            colAddr2 >= 0 ? (row[colAddr2] || '') : '',
            row[colCity] || '',
            row[colState] || '',
            row[colZip] || '',
            row[colAmount] || '',
            row[colWeightedAmount] || '',
            donor_formatDate_(row[colReceiptDate]),
            row[colOccupation] || '',
            row[colEmployer] || '',
            key,
            assignment.donorId,
            assignment.confidence
          ]);
        }
      } catch (err) {
        throw new Error('Error processing KREF: ' + err.message + '. Make sure Amount and WeightedAmount columns exist.');
      }
    }

    // Process FEC
    if (fecData.length > 1) {
      const fecHeader = fecData[0];
      try {
        const colRecipient = fecHeader.indexOf('Recipient'); // Optional
        const colFirst = findCol(fecHeader, 'DonorFirst');
        const colLast = findCol(fecHeader, 'DonorLast');
        const colAddr1 = findCol(fecHeader, 'Address1');
        const colAddr2 = fecHeader.indexOf('Address2'); // Optional
        const colCity = findCol(fecHeader, 'City');
        const colState = findCol(fecHeader, 'State');
        const colZip = findCol(fecHeader, 'Zip');
        const colAmount = findCol(fecHeader, 'Amount');
        const colWeightedAmount = findCol(fecHeader, 'WeightedAmount');
        const colReceiptDate = findCol(fecHeader, 'ReceiptDate');
        const colOccupation = findCol(fecHeader, 'Occupation');
        const colEmployer = findCol(fecHeader, 'Employer');

        for (let i = 1; i < fecData.length; i++) {
          const row = fecData[i];
          const key = 'F:' + (i + 1);
          const assignment = assignments[key] || { donorId: '', confidence: '' };

          outputRows.push([
            colRecipient >= 0 ? (row[colRecipient] || 'F') : 'F',
            row[colFirst] || '',
            row[colLast] || '',
            row[colAddr1] || '',
            colAddr2 >= 0 ? (row[colAddr2] || '') : '',
            row[colCity] || '',
            row[colState] || '',
            row[colZip] || '',
            row[colAmount] || '',
            row[colWeightedAmount] || '',
            donor_formatDate_(row[colReceiptDate]),
            row[colOccupation] || '',
            row[colEmployer] || '',
            key,
            assignment.donorId,
            assignment.confidence
          ]);
        }
      } catch (err) {
        throw new Error('Error processing FEC: ' + err.message + '. Make sure Amount and WeightedAmount columns exist.');
      }
    }

    // Write to Merge output sheet in batches to avoid limits
    const mergeSheet = donor_getOrCreateSheet_('Merge output');
    mergeSheet.clear();

    Logger.log('Writing ' + outputRows.length + ' rows (including header) to Merge output sheet in batches');
    Logger.log('Total assignments: ' + Object.keys(assignments).length);

    const BATCH_SIZE = 1000;
    const totalRows = outputRows.length;
    const numBatches = Math.ceil(totalRows / BATCH_SIZE);

    for (let batchNum = 0; batchNum < numBatches; batchNum++) {
      const startIdx = batchNum * BATCH_SIZE;
      const endIdx = Math.min(startIdx + BATCH_SIZE, totalRows);
      const batchRows = outputRows.slice(startIdx, endIdx);

      const startRow = startIdx + 1; // Sheets are 1-indexed
      mergeSheet.getRange(startRow, 1, batchRows.length, outputHeader.length).setValues(batchRows);

      Logger.log('Wrote batch ' + (batchNum + 1) + '/' + numBatches + ' (' + batchRows.length + ' rows, rows ' + startRow + '-' + (startRow + batchRows.length - 1) + ')');

      // Small delay between batches to avoid rate limiting
      if (batchNum < numBatches - 1) {
        Utilities.sleep(100); // 100ms pause
      }
    }

    mergeSheet.setFrozenRows(1);

    // Force ReceiptDate column (column 11) to be formatted as plain text
    // This prevents Google Sheets from auto-converting date strings back to Date objects
    const receiptDateCol = 11; // 'ReceiptDate' is the 11th column
    mergeSheet.getRange(1, receiptDateCol, totalRows, 1).setNumberFormat('@');

    Logger.log('Merge output sheet populated successfully with ' + (totalRows - 1) + ' data rows');
    SpreadsheetApp.getActive().toast(' All done! Merge output: ' + (totalRows - 1) + ' rows with DonorID and Confidence', 'Success', 5);

    return {
      success: true,
      rowCount: outputRows.length - 1,
      assignmentCount: Object.keys(assignments).length,
      message: 'Merge output sheet populated with ' + (outputRows.length - 1) + ' rows'
    };
  } catch (err) {
    Logger.log('ERROR in donor_buildDonorIdList: ' + err.message);
    Logger.log('Stack: ' + err.stack);
    return {
      success: false,
      error: err.message,
      stack: err.stack
    };
  }
}

/** Token and helpers */
function dl_validateToken_(token) {
  const props = PropertiesService.getDocumentProperties();
  const t = props.getProperty('dl_token');
  const until = Number(props.getProperty('dl_token_until') || '0');
  if (!token || !t || token !== t) return {ok:false, msg:'Invalid or missing token'};
  if (Date.now() > until) return {ok:false, msg:'Token expired'};
  return {ok:true};
}
function dl_getExecUrlFromOptions_() {
  const ss = SpreadsheetApp.getActive();
  const sh = ss.getSheetByName('Options');
  if (!sh) return null;
  const val = String(sh.getRange('I2').getValue() || '').trim();
  if (!val) return null;
  return dl_normalizeWebAppUrl_(val);
}
function dl_normalizeWebAppUrl_(rawUrl) {
  if (!rawUrl) return '';
  return String(rawUrl).replace(/\/a\/[^/]+\/macros\//, '/macros/');
}
function dl_htmlEscape_(s) {
  return String(s).replace(/&/g, '&amp;').replace(/</g, '&lt;');
}
function dl_setProgress_(phase, done, total, note) {
  const props = PropertiesService.getDocumentProperties();
  const payload = { phase, done, total, note, ts: Date.now() };
  props.setProperty('dl_progress', JSON.stringify(payload));
}
function dl_resetProgress_() {
  PropertiesService.getDocumentProperties().deleteProperty('dl_progress');
}
function dl_makeToken_() {
  return Utilities.getUuid().replace(/-/g, '').slice(0, 40);
}
function dl_exportSheetAsCsv_(sheetName) {
  const ss = SpreadsheetApp.getActive();
  const sh = ss.getSheetByName(sheetName);
  if (!sh) return { file: null, folder: null };
  const lastRow = sh.getLastRow();
  const lastCol = sh.getLastColumn();
  if (lastRow < 1 || lastCol < 1) return { file: null, folder: null };

  // Use getValues() instead of getDisplayValues() to get raw values
  const rawValues = sh.getRange(1, 1, lastRow, lastCol).getValues();

  // Convert any Date objects to simple M/D/YYYY strings
  const values = rawValues.map(row => row.map(cell => {
    if (cell instanceof Date && !isNaN(cell.getTime())) {
      const month = cell.getMonth() + 1;
      const day = cell.getDate();
      const year = cell.getFullYear();
      return `${month}/${day}/${year}`;
    }
    return cell;
  }));

  const csv = dl_sheetToCsv_(values);
  const folder = dl_getOrCreateStagingFolder_();
  const blob = Utilities.newBlob(csv, 'text/csv', sheetName + '.csv');
  const file = folder.createFile(blob);
  return { file, folder };
}
function dl_sheetToCsv_(rows2d) {
  function esc(v) {
    const s = String(v == null ? '' : v);
    if (/[",\r\n]/.test(s)) return '"' + s.replace(/"/g, '""') + '"';
    return s;
  }
  return rows2d.map(row => (row || []).map(esc).join(',')).join('\r\n');
}
function dl_getOrCreateStagingFolder_() {
  const props = PropertiesService.getDocumentProperties();
  const existingId = props.getProperty('dl_staging_folderId');
  if (existingId) {
    try { return DriveApp.getFolderById(existingId); } catch (e) {}
  }
  const parent = DriveApp.getFileById(SpreadsheetApp.getActive().getId()).getParents().hasNext()
    ? DriveApp.getFileById(SpreadsheetApp.getActive().getId()).getParents().next()
    : DriveApp.getRootFolder();
  const folder = parent.createFolder('dl_staging_' + Utilities.getUuid().slice(0, 8));
  props.setProperty('dl_staging_folderId', folder.getId());
  return folder;
}

/** Optional quick logs */
function dl_logExecUrlFromOptions_() {
  const url = dl_getExecUrlFromOptions_();
  console.log('Options!I2 /exec URL:', url || '(missing or invalid)');
}
function logCurrentWebAppUrl() {
  const rawUrl = ScriptApp.getService().getUrl();
  const webAppUrl = rawUrl ? rawUrl.replace(/\/a\/[^/]+\/macros\//, '/macros/') : null;
  console.log('Service URL:', webAppUrl || '(no web app deployment found)');
}

/** Debug helpers preserved */
function donor_createTrainingPairs_debug() {
  const ui = SpreadsheetApp.getUi();
  const t0 = Date.now();
  SpreadsheetApp.getActive().toast('Debug: loading rows', 'Donor Matcher', 5);
  const rows = donor_loadAllRows__debug_();
  const t1 = Date.now();
  if (!rows.length) {
    ui.alert('No input rows found. Check sheet names and headers.');
    return;
  }
  SpreadsheetApp.getActive().toast('Debug: building blocks', 'Donor Matcher', 5);
  const dbg1 = donor_buildBlocks__debug_(rows);
  const t2 = Date.now();
  SpreadsheetApp.getActive().toast('Debug: generating pairs', 'Donor Matcher', 5);
  const pairs = donor_generatePairsFromBlocks__debug_(dbg1.blocks, dbg1.bigBlocksSkipped);
  const t3 = Date.now();
  if (!pairs.length) {
    ui.alert('Zero pairs after blocking or all blocks skipped by size.');
    return;
  }
  SpreadsheetApp.getActive().toast('Debug: sampling and writing', 'Donor Matcher', 5);
  const sampled = donor_sampleArray_(pairs, DONOR_CFG.sampleTrainingPairs);
  donor_writeTrainingSheet_(sampled);
  const t4 = Date.now();
  const msg = 'Debug summary'
    + '\n\nRows total: ' + rows.length
    + '\nBlocks total: ' + dbg1.blockCount
    + '\nBlocks with <2 rows: ' + dbg1.smallBlocks
    + '\nBlocks skipped for size: ' + dbg1.bigBlocksSkipped
    + '\nPairs produced: ' + pairs.length
    + '\nPairs sampled to Training: ' + sampled.length
    + '\n\nTiming (ms)'
    + '\nLoad rows: ' + (t1 - t0)
    + '\nBuild blocks: ' + (t2 - t1)
    + '\nGenerate pairs: ' + (t3 - t2)
    + '\nWrite sheet: ' + (t4 - t3)
    + '\nTotal: ' + (t4 - t0);
  Logger.log(msg);
  SpreadsheetApp.getUi().alert(msg);
  SpreadsheetApp.getActive().toast('Training sheet ready with ' + sampled.length + ' pairs', 'Donor Matcher', 8);
}
function donor_loadAllRows__debug_() {
  const kref = donor_readSheetAsObjects_(DONOR_CFG.krefSheet);
  const fec = donor_readSheetAsObjects_(DONOR_CFG.fecSheet);
  function headerHas(h, name) { return h.indexOf(name) !== -1; }
  const need = ['DonorFirst','DonorLast','Address1','City','State','Zip','Employer','Occupation'];
  const missingK = need.filter(n => !headerHas(kref.header, n));
  const missingF = need.filter(n => !headerHas(fec.header, n));
  if (missingK.length && missingF.length) {
    Logger.log('Both sheets missing expected headers. K missing: ' + missingK.join(', ') + ' | F missing: ' + missingF.join(', '));
  } else {
    if (missingK.length) Logger.log('K sheet missing: ' + missingK.join(', '));
    if (missingF.length) Logger.log('F sheet missing: ' + missingF.join(', '));
  }
  function mapRow(r, i, origin) {
    const first = String(r.DonorFirst || '').trim();
    const last = String(r.DonorLast || '').trim();
    const addr1 = String(r.Address1 || '').trim();
    const city = String(r.City || '').trim();
    const state = String(r.State || '').trim();
    const zip = donor_normalizeZip_(r.Zip);
    const employer = String(r.Employer || '').trim();
    const occupation = String(r.Occupation || '').trim();
    const fullName = (first + ' ' + last).trim();
    const house = donor_extractHouseNumber_(addr1);
    return { origin, rowIndex: i, first, last, addr1, city, state, zip, employer, occupation, fullName, house, globalIdx: -1 };
  }
  const all = [];
  for (let i = 0; i < kref.rows.length; i++) all.push(mapRow(kref.rows[i], i, 'K'));
  for (let j = 0; j < fec.rows.length; j++) all.push(mapRow(fec.rows[j], j, 'F'));
  for (let k = 0; k < all.length; k++) all[k].globalIdx = k;
  Logger.log('Loaded rows. K: ' + kref.rows.length + ' F: ' + fec.rows.length + ' total: ' + all.length);
  return all;
}
function donor_buildBlocks__debug_(rows) {
  const blocks = new Map();
  let blockCount = 0;
  let smallBlocks = 0;
  for (let i = 0; i < rows.length; i++) {
    const r = rows[i];
    const keys = donor_blockingKeys_(r);
    for (let j = 0; j < keys.length; j++) {
      const key = keys[j];
      if (!blocks.has(key)) { blocks.set(key, []); blockCount++; }
      blocks.get(key).push(r);
    }
  }
  const it = blocks.values();
  while (true) {
    const step = it.next();
    if (step.done) break;
    const arr = step.value;
    if (!arr || arr.length < 2) smallBlocks++;
  }
  Logger.log('Blocks built. Total: ' + blockCount + ' small: ' + smallBlocks);
  return { blocks, blockCount, smallBlocks, bigBlocksSkipped: 0 };
}
function donor_generatePairsFromBlocks__debug_(blocks, bigBlocksSkippedInitial) {
  const pairs = [];
  let total = 0;
  const seen = new Set();
  let bigBlocksSkipped = bigBlocksSkippedInitial || 0;
  const it = blocks.entries();
  while (true) {
    const step = it.next();
    if (step.done) break;
    const blockRows = step.value[1];
    if (!blockRows || blockRows.length < 2) continue;
    const n = blockRows.length;
    if (n * n > DONOR_CFG.maxPairsPerBlock) { bigBlocksSkipped++; continue; }
    for (let i = 0; i < n; i++) {
      for (let j = i + 1; j < n; j++) {
        const a = blockRows[i];
        const b = blockRows[j];
        const ka = a.globalIdx < b.globalIdx ? a.globalIdx + '|' + b.globalIdx : b.globalIdx + '|' + a.globalIdx;
        if (seen.has(ka)) continue;
        seen.add(ka);
        const feat = donor_makeFeatures_(a, b);
        pairs.push({ a, b, aIdx: a.globalIdx, bIdx: b.globalIdx, features: feat });
        total++;
        if (total > DONOR_CFG.maxTotalPairs) {
          Logger.log('Hit maxTotalPairs cap at ' + DONOR_CFG.maxTotalPairs + '.');
          return pairs;
        }
      }
    }
  }
  Logger.log('Pairs generated: ' + pairs.length + ' | bigBlocksSkipped: ' + bigBlocksSkipped);
  return pairs;
}

// ========================================
// Campaign Deputy Upload Functions
// ========================================

/**
 * Opens a dialog for drag-and-drop CSV upload
 */
function cd_uploadDialog() {
  const html = HtmlService.createHtmlOutput(cd_getUploadHtml_())
    .setWidth(500)
    .setHeight(300)
    .setTitle('Upload Campaign Deputy Export');
  SpreadsheetApp.getUi().showModalDialog(html, 'Campaign Deputy Upload');
}

/**
 * Returns the HTML for the drag-and-drop interface
 */
function cd_getUploadHtml_() {
  return `
<!DOCTYPE html>
<html>
<head>
  <base target="_top">
  <style>
    body {
      font-family: Arial, sans-serif;
      padding: 20px;
      margin: 0;
    }
    #dropzone {
      border: 3px dashed #4285f4;
      border-radius: 8px;
      padding: 40px;
      text-align: center;
      background-color: #f8f9fa;
      transition: all 0.3s;
      cursor: pointer;
    }
    #dropzone.dragover {
      background-color: #e8f0fe;
      border-color: #1967d2;
    }
    #dropzone:hover {
      background-color: #e8f0fe;
    }
    .icon {
      font-size: 48px;
      color: #4285f4;
      margin-bottom: 10px;
    }
    .message {
      font-size: 16px;
      color: #5f6368;
      margin-bottom: 8px;
    }
    .hint {
      font-size: 12px;
      color: #80868b;
    }
    #status {
      margin-top: 20px;
      padding: 10px;
      border-radius: 4px;
      display: none;
    }
    #status.success {
      background-color: #d4edda;
      color: #155724;
      border: 1px solid #c3e6cb;
      display: block;
    }
    #status.error {
      background-color: #f8d7da;
      color: #721c24;
      border: 1px solid #f5c6cb;
      display: block;
    }
    #status.processing {
      background-color: #fff3cd;
      color: #856404;
      border: 1px solid #ffeaa7;
      display: block;
    }
  </style>
</head>
<body>
  <div id="dropzone">
    <div class="icon"></div>
    <div class="message">Drag and drop your Campaign Deputy CSV file here</div>
    <div class="hint">or click to select a file</div>
  </div>
  <input type="file" id="fileInput" accept=".csv" style="display: none;">
  <div id="status"></div>

  <script>
    const dropzone = document.getElementById('dropzone');
    const fileInput = document.getElementById('fileInput');
    const status = document.getElementById('status');

    // Click to select file
    dropzone.addEventListener('click', () => {
      fileInput.click();
    });

    // File selected via input
    fileInput.addEventListener('change', (e) => {
      if (e.target.files.length > 0) {
        handleFile(e.target.files[0]);
      }
    });

    // Prevent default drag behaviors
    ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {
      dropzone.addEventListener(eventName, preventDefaults, false);
      document.body.addEventListener(eventName, preventDefaults, false);
    });

    function preventDefaults(e) {
      e.preventDefault();
      e.stopPropagation();
    }

    // Highlight drop zone when dragging over it
    ['dragenter', 'dragover'].forEach(eventName => {
      dropzone.addEventListener(eventName, () => {
        dropzone.classList.add('dragover');
      }, false);
    });

    ['dragleave', 'drop'].forEach(eventName => {
      dropzone.addEventListener(eventName, () => {
        dropzone.classList.remove('dragover');
      }, false);
    });

    // Handle dropped files
    dropzone.addEventListener('drop', (e) => {
      const files = e.dataTransfer.files;
      if (files.length > 0) {
        handleFile(files[0]);
      }
    }, false);

    function handleFile(file) {
      // Validate file type
      if (!file.name.endsWith('.csv')) {
        showStatus('error', 'Please upload a CSV file only.');
        return;
      }

      showStatus('processing', 'Reading file: ' + file.name + '...');

      // Read file contents
      const reader = new FileReader();
      reader.onload = (e) => {
        const csvContent = e.target.result;
        showStatus('processing', 'Uploading to Google Sheets...');
        
        // Send to server
        google.script.run
          .withSuccessHandler(onSuccess)
          .withFailureHandler(onError)
          .cd_processUpload(csvContent, file.name);
      };
      reader.onerror = () => {
        showStatus('error', 'Failed to read file. Please try again.');
      };
      reader.readAsText(file);
    }

    function onSuccess(result) {
      showStatus('success', result.message + ' (' + result.rowCount + ' rows imported)');
      setTimeout(() => {
        google.script.host.close();
      }, 2000);
    }

    function onError(error) {
      showStatus('error', 'Error: ' + error.message);
    }

    function showStatus(type, message) {
      status.className = type;
      status.textContent = message;
    }
  </script>
</body>
</html>
  `;
}

/**
 * Lookup state from ZIP code prefix
 */
function cd_lookupStateFromZip_(zipCode) {
  if (!zipCode) return '';

  const ZIP_TO_STATE = {
    '900': 'CA', '901': 'CA', '902': 'CA', '903': 'CA', '904': 'CA', '905': 'CA', '906': 'CA', '907': 'CA', '908': 'CA',
    '910': 'CA', '911': 'CA', '912': 'CA', '913': 'CA', '914': 'CA', '915': 'CA', '916': 'CA', '917': 'CA', '918': 'CA',
    '919': 'CA', '920': 'CA', '921': 'CA', '922': 'CA', '923': 'CA', '924': 'CA', '925': 'CA', '926': 'CA', '927': 'CA',
    '928': 'CA', '930': 'CA', '931': 'CA', '932': 'CA', '933': 'CA', '934': 'CA', '935': 'CA', '936': 'CA', '937': 'CA',
    '938': 'CA', '939': 'CA', '940': 'CA', '941': 'CA', '942': 'CA', '943': 'CA', '944': 'CA', '945': 'CA', '946': 'CA',
    '947': 'CA', '948': 'CA', '949': 'CA', '950': 'CA', '951': 'CA', '952': 'CA', '953': 'CA', '954': 'CA', '955': 'CA',
    '956': 'CA', '957': 'CA', '958': 'CA', '959': 'CA', '960': 'CA', '961': 'CA',
    '100': 'NY', '101': 'NY', '102': 'NY', '103': 'NY', '104': 'NY', '105': 'NY', '106': 'NY', '107': 'NY', '108': 'NY',
    '109': 'NY', '110': 'NY', '111': 'NY', '112': 'NY', '113': 'NY', '114': 'NY', '115': 'NY', '116': 'NY', '117': 'NY',
    '118': 'NY', '119': 'NY', '120': 'NY', '121': 'NY', '122': 'NY', '123': 'NY', '124': 'NY', '125': 'NY', '126': 'NY',
    '127': 'NY', '128': 'NY', '129': 'NY', '130': 'NY', '131': 'NY', '132': 'NY', '133': 'NY', '134': 'NY', '135': 'NY',
    '136': 'NY', '137': 'NY', '138': 'NY', '139': 'NY', '140': 'NY', '141': 'NY', '142': 'NY', '143': 'NY', '144': 'NY',
    '145': 'NY', '146': 'NY', '147': 'NY', '148': 'NY', '149': 'NY',
    '600': 'IL', '601': 'IL', '602': 'IL', '603': 'IL', '604': 'IL', '605': 'IL', '606': 'IL', '607': 'IL', '608': 'IL',
    '609': 'IL', '610': 'IL', '611': 'IL', '612': 'IL', '613': 'IL', '614': 'IL', '615': 'IL', '616': 'IL', '617': 'IL',
    '618': 'IL', '619': 'IL', '620': 'IL', '622': 'IL', '623': 'IL', '624': 'IL', '625': 'IL', '626': 'IL', '627': 'IL',
    '628': 'IL', '629': 'IL',
    '700': 'LA', '701': 'LA', '702': 'LA', '703': 'LA', '704': 'LA', '705': 'LA', '706': 'LA', '707': 'LA', '708': 'LA',
    '710': 'LA', '711': 'LA', '712': 'LA', '713': 'LA', '714': 'LA',
    '750': 'TX', '751': 'TX', '752': 'TX', '753': 'TX', '754': 'TX', '755': 'TX', '756': 'TX', '757': 'TX', '758': 'TX',
    '759': 'TX', '760': 'TX', '761': 'TX', '762': 'TX', '763': 'TX', '764': 'TX', '765': 'TX', '766': 'TX', '767': 'TX',
    '768': 'TX', '769': 'TX', '770': 'TX', '771': 'TX', '772': 'TX', '773': 'TX', '774': 'TX', '775': 'TX', '776': 'TX',
    '777': 'TX', '778': 'TX', '779': 'TX', '780': 'TX', '781': 'TX', '782': 'TX', '783': 'TX', '784': 'TX', '785': 'TX',
    '786': 'TX', '787': 'TX', '788': 'TX', '789': 'TX', '790': 'TX', '791': 'TX', '792': 'TX', '793': 'TX', '794': 'TX',
    '795': 'TX', '796': 'TX', '797': 'TX', '798': 'TX', '799': 'TX',
    '300': 'FL', '301': 'FL', '302': 'FL', '303': 'FL', '304': 'FL', '305': 'FL', '306': 'FL', '307': 'FL', '308': 'FL',
    '309': 'FL', '310': 'FL', '311': 'FL', '312': 'FL', '313': 'FL', '314': 'FL', '315': 'FL', '316': 'FL', '317': 'FL',
    '318': 'FL', '319': 'FL', '320': 'FL', '321': 'FL', '322': 'FL', '323': 'FL', '324': 'FL', '325': 'FL', '326': 'FL',
    '327': 'FL', '328': 'FL', '329': 'FL', '330': 'FL', '331': 'FL', '332': 'FL', '333': 'FL', '334': 'FL', '335': 'FL',
    '336': 'FL', '337': 'FL', '338': 'FL', '339': 'FL',
    '150': 'PA', '151': 'PA', '152': 'PA', '153': 'PA', '154': 'PA', '155': 'PA', '156': 'PA', '157': 'PA', '158': 'PA',
    '159': 'PA', '160': 'PA', '161': 'PA', '162': 'PA', '163': 'PA', '164': 'PA', '165': 'PA', '166': 'PA', '167': 'PA',
    '168': 'PA', '169': 'PA', '170': 'PA', '171': 'PA', '172': 'PA', '173': 'PA', '174': 'PA', '175': 'PA', '176': 'PA',
    '177': 'PA', '178': 'PA', '179': 'PA', '180': 'PA', '181': 'PA', '182': 'PA', '183': 'PA', '184': 'PA', '185': 'PA',
    '186': 'PA', '187': 'PA', '188': 'PA', '189': 'PA', '190': 'PA', '191': 'PA', '192': 'PA', '193': 'PA', '194': 'PA',
    '195': 'PA', '196': 'PA'
  };

  const zipStr = String(zipCode).trim().substring(0, 3);
  return ZIP_TO_STATE[zipStr] || '';
}

/**
 * Valid US state abbreviations (50 states + DC + territories)
 */
const CD_VALID_US_STATES = new Set([
  'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA',
  'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD',
  'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ',
  'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC',
  'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY',
  'DC', 'PR', 'VI', 'GU', 'AS', 'MP'
]);

/**
 * Check if value is broken/missing or not a valid US state
 */
function cd_isBrokenValue_(val) {
  if (!val) return true;
  const s = String(val).trim().toUpperCase();
  // Check if empty, NA, or other broken values
  if (s === '' || s === 'NA' || s === 'N/A' || s === 'UNKNOWN' || s === 'NULL') return true;
  // Also check if it's not a valid US state
  return !CD_VALID_US_STATES.has(s);
}

/**
 * Processes the uploaded CSV and writes to CD_donors sheet
 * @param {string} csvContent - The CSV file contents
 * @param {string} fileName - The original filename
 * @returns {Object} Result with success status and message
 */
function cd_processUpload(csvContent, fileName) {
  try {
    const ss = SpreadsheetApp.getActive();

    // Get or create CD_donors sheet
    let sheet = ss.getSheetByName('CD_donors');
    if (!sheet) {
      sheet = ss.insertSheet('CD_donors');
    } else {
      // Clear existing content
      sheet.clear();
    }

    // Parse CSV
    const rows = Utilities.parseCsv(csvContent);

    if (rows.length === 0) {
      throw new Error('CSV file is empty');
    }

    // Fill in missing State values from ZIP codes
    if (rows.length > 1) {
      const header = rows[0];
      const stateIdx = header.indexOf('NameState') >= 0 ? header.indexOf('NameState') : header.indexOf('State');
      const zipIdx = header.indexOf('NameZip') >= 0 ? header.indexOf('NameZip') : header.indexOf('Zip');

      if (stateIdx >= 0 && zipIdx >= 0) {
        let filledCount = 0;
        for (let i = 1; i < rows.length; i++) {
          const state = rows[i][stateIdx];
          const zip = rows[i][zipIdx];

          if (cd_isBrokenValue_(state) && zip) {
            const lookedUpState = cd_lookupStateFromZip_(zip);
            if (lookedUpState) {
              rows[i][stateIdx] = lookedUpState;
              filledCount++;
            }
          }
        }
        Logger.log('Filled in ' + filledCount + ' missing state values from ZIP codes');
      }
    }

    // Write to sheet
    sheet.getRange(1, 1, rows.length, rows[0].length).setValues(rows);
    
    // Format header row
    if (rows.length > 0) {
      const headerRange = sheet.getRange(1, 1, 1, rows[0].length);
      headerRange.setFontWeight('bold');
      headerRange.setBackground('#4285f4');
      headerRange.setFontColor('#ffffff');
      sheet.setFrozenRows(1);
    }
    
    // Auto-resize columns
    for (let i = 1; i <= rows[0].length; i++) {
      sheet.autoResizeColumn(i);
    }
    
    Logger.log('Campaign Deputy export uploaded: ' + fileName + ' (' + rows.length + ' rows)');
    
    return {
      success: true,
      message: 'Successfully imported ' + fileName,
      rowCount: rows.length - 1 // Exclude header
    };
    
  } catch (error) {
    Logger.log('Error uploading Campaign Deputy export: ' + error.message);
    throw new Error('Failed to process CSV: ' + error.message);
  }
}

// ========================================
// Campaign Deputy Matching Functions
// ========================================

/**
 * Prepares matching job for Campaign Deputy donors
 */
function cd_prepareMatchingJob_() {
  const ss = SpreadsheetApp.getActive();
  
  // Validate required sheets exist
  const mergeSheet = ss.getSheetByName('Merge output');
  const cdSheet = ss.getSheetByName('CD_donors');
  
  if (!mergeSheet) {
    SpreadsheetApp.getUi().alert('Merge output sheet not found. Please run the donor matching first.');
    return;
  }
  
  if (!cdSheet) {
    SpreadsheetApp.getUi().alert('CD_donors sheet not found. Please upload a Campaign Deputy export first.');
    return;
  }
  
  // Get web app URL
  const webAppUrl = dl_getExecUrlFromOptions_();
  if (!webAppUrl) {
    SpreadsheetApp.getUi().alert(
      'Put your Web App URL ending with /exec in Options!I2.\nExample:\nhttps://script.google.com/macros/s/AKfycb.../exec'
    );
    return;
  }
  
  // Export sheets to Drive as CSV
  const mergeData = mergeSheet.getDataRange().getValues();
  const cdData = cdSheet.getDataRange().getValues();

  const mergeFile = cd_createTempCsv_(mergeData, 'merge_output');
  const cdFile = cd_createTempCsv_(cdData, 'cd_donors');

  // Make files publicly readable (temporary) for direct download
  mergeFile.setSharing(DriveApp.Access.ANYONE_WITH_LINK, DriveApp.Permission.VIEW);
  cdFile.setSharing(DriveApp.Access.ANYONE_WITH_LINK, DriveApp.Permission.VIEW);

  // Get direct download URLs (bypassing the web app middleman)
  const mergeUrl = 'https://drive.google.com/uc?export=download&id=' + mergeFile.getId();
  const cdUrl = 'https://drive.google.com/uc?export=download&id=' + cdFile.getId();

  // Create token
  const token = dl_makeToken_();
  const until = Date.now() + 60 * 60 * 1000; // 1 hour

  // Store file IDs and token (for cleanup later)
  const props = PropertiesService.getDocumentProperties();
  props.setProperty('cd_token', token);
  props.setProperty('cd_token_until', String(until));
  props.setProperty('cd_merge_fileId', mergeFile.getId());
  props.setProperty('cd_cd_fileId', cdFile.getId());

  // Build terminal command - downloads DIRECTLY from Drive, not through web app
  const cmd =
    "curl -sSL '" + webAppUrl + "?cd_matcher=1' | " +
    "python3 - --merge '" + mergeUrl + "' " +
    "--cd '" + cdUrl + "' " +
    "--result '" + webAppUrl + "?cd_result=1&token=" + token + "' " +
    "--model '" + webAppUrl + "?model=1'";
  
  // Show command in dialog
  const html = HtmlService.createHtmlOutput(
    '<div style="font-family:monospace; padding:20px; word-wrap:break-word;">' +
    '<h3>Campaign Deputy Matching</h3>' +
    '<p>Copy and paste this command into Terminal:</p>' +
    '<textarea readonly style="width:100%; height:100px; font-family:monospace;">' + cmd + '</textarea>' +
    '<p style="margin-top:20px;"><strong>What this does:</strong></p>' +
    '<ul>' +
    '<li>Downloads data <strong>directly from Google Drive</strong> (no size limits!)</li>' +
    '<li>Matches donor clusters to Campaign Deputy PersonIDs</li>' +
    '<li>Aggregates donation history and calculates metrics</li>' +
    '<li>Creates CD_To_Upload sheet for import</li>' +
    '<li>Automatically cleans up temporary files</li>' +
    '</ul>' +
    '<p style="margin-top:10px; color:#666; font-size:12px;">Note: CSV files are temporarily shared with "anyone with link" for download, then deleted.</p>' +
    '</div>'
  ).setWidth(600).setHeight(480);
  
  SpreadsheetApp.getUi().showModalDialog(html, 'Campaign Deputy Matching');
}

/**
 * Creates a temporary CSV file in Drive
 */
function cd_createTempCsv_(data, name) {
  const csvContent = data.map(row => row.map(cell => {
    // Convert Date objects to simple M/D/YYYY format
    if (cell instanceof Date && !isNaN(cell.getTime())) {
      const month = cell.getMonth() + 1;
      const day = cell.getDate();
      const year = cell.getFullYear();
      cell = `${month}/${day}/${year}`;
    }

    const str = String(cell || '');
    if (str.includes(',') || str.includes('"') || str.includes('\n')) {
      return '"' + str.replace(/"/g, '""') + '"';
    }
    return str;
  }).join(',')).join('\n');

  const folder = DriveApp.getRootFolder();
  const file = folder.createFile(name + '_' + Date.now() + '.csv', csvContent, MimeType.CSV);
  return file;
}

/**
 * Serves CSV files for Campaign Deputy matching
 */
function cd_serveCsv_(type, token) {
  const chk = cd_validateToken_(token);
  if (!chk.ok) return ContentService.createTextOutput(chk.msg).setMimeType(ContentService.MimeType.TEXT);
  
  const props = PropertiesService.getDocumentProperties();
  const fileId = type === 'merge' ? 
    props.getProperty('cd_merge_fileId') : 
    props.getProperty('cd_cd_fileId');
    
  if (!fileId) return ContentService.createTextOutput('No CSV file id').setMimeType(ContentService.MimeType.TEXT);
  
  const file = DriveApp.getFileById(fileId);
  const out = ContentService.createTextOutput(file.getBlob().getDataAsString());
  out.setMimeType(ContentService.MimeType.CSV);
  return out;
}

/**
 * Validates Campaign Deputy matching token
 */
function cd_validateToken_(token) {
  const props = PropertiesService.getDocumentProperties();
  const t = props.getProperty('cd_token');
  const until = Number(props.getProperty('cd_token_until') || '0');
  if (!token || !t || token !== t) return {ok:false, msg:'Invalid or missing token'};
  if (Date.now() > until) return {ok:false, msg:'Token expired'};
  return {ok:true};
}

/**
 * Cleans up temporary CSV files from Drive
 */
function cd_cleanupTempFiles_() {
  try {
    const props = PropertiesService.getDocumentProperties();
    const mergeFileId = props.getProperty('cd_merge_fileId');
    const cdFileId = props.getProperty('cd_cd_fileId');

    if (mergeFileId) {
      try {
        const file = DriveApp.getFileById(mergeFileId);
        file.setTrashed(true);
        Logger.log('Deleted temporary merge file: ' + mergeFileId);
      } catch (e) {
        Logger.log('Could not delete merge file: ' + e.message);
      }
    }

    if (cdFileId) {
      try {
        const file = DriveApp.getFileById(cdFileId);
        file.setTrashed(true);
        Logger.log('Deleted temporary CD file: ' + cdFileId);
      } catch (e) {
        Logger.log('Could not delete CD file: ' + e.message);
      }
    }

    // Clear the stored file IDs
    props.deleteProperty('cd_merge_fileId');
    props.deleteProperty('cd_cd_fileId');
    props.deleteProperty('cd_token');
    props.deleteProperty('cd_token_until');

  } catch (error) {
    Logger.log('Error during cleanup: ' + error.message);
    // Don't throw - cleanup failures shouldn't break the main workflow
  }
}

// ========================================
// Diagnostic Functions
// ========================================

/**
 * Prepares diagnostic analysis job
 */
function diagnostic_analyzeMatching() {
  const ss = SpreadsheetApp.getActive();

  // Validate required sheets exist
  const krefSheet = ss.getSheetByName('KREF_Exports');
  const fecSheet = ss.getSheetByName('FEC_Exports');

  if (!krefSheet || !fecSheet) {
    SpreadsheetApp.getUi().alert('KREF_Exports or FEC_Exports sheet not found. Please ensure your data is loaded.');
    return;
  }

  // Get web app URL
  const webAppUrl = dl_getExecUrlFromOptions_();
  if (!webAppUrl) {
    SpreadsheetApp.getUi().alert(
      'Put your Web App URL ending with /exec in Options!I2.\nExample:\nhttps://script.google.com/macros/s/AKfycb.../exec'
    );
    return;
  }

  // Read match threshold from Options!J2
  const optionsSheet = ss.getSheetByName('Options');
  const threshold = optionsSheet ? Number(optionsSheet.getRange('J2').getValue() || 0.7) : 0.7;

  // Export sheets to Drive as CSV
  const krefData = krefSheet.getDataRange().getValues();
  const fecData = fecSheet.getDataRange().getValues();

  const krefFile = cd_createTempCsv_(krefData, 'kref_export');
  const fecFile = cd_createTempCsv_(fecData, 'fec_export');

  // Make files publicly readable (temporary) for direct download
  krefFile.setSharing(DriveApp.Access.ANYONE_WITH_LINK, DriveApp.Permission.VIEW);
  fecFile.setSharing(DriveApp.Access.ANYONE_WITH_LINK, DriveApp.Permission.VIEW);

  // Get direct download URLs
  const krefUrl = 'https://drive.google.com/uc?export=download&id=' + krefFile.getId();
  const fecUrl = 'https://drive.google.com/uc?export=download&id=' + fecFile.getId();

  // Create token
  const token = dl_makeToken_();
  const until = Date.now() + 30 * 60 * 1000; // 30 minutes

  // Create diagnostic job (same pattern as training)
  const job = {
    krefUrl: webAppUrl + '?csv=kref&token=' + encodeURIComponent(token),
    fecUrl: webAppUrl + '?csv=fec&token=' + encodeURIComponent(token),
    modelUrl: webAppUrl + '?model=1'
  };

  // Store file IDs, token, and job (use dl_ prefix for compatibility with existing endpoints)
  const props = PropertiesService.getDocumentProperties();
  props.setProperty('dl_token', token);
  props.setProperty('dl_token_until', String(until));
  props.setProperty('dl_kref_fileId', krefFile.getId());
  props.setProperty('dl_fec_fileId', fecFile.getId());
  props.setProperty('diag_job_json', JSON.stringify(job));

  // Build command using bundle (same pattern as training)
  const fullCmd =
    "curl -sSL '" + webAppUrl + "?diagnostic=1' | " +
    "python3 - --bundle '" + webAppUrl + "?diag_job=1&token=" + token + "' " +
    "--threshold " + threshold;

  // Show command in dialog
  const html = HtmlService.createHtmlOutput(
    '<div style="font-family:monospace; padding:20px; word-wrap:break-word;">' +
    '<h3> Matching Quality Diagnostic</h3>' +
    '<p><strong>Step 1:</strong> Ensure you have the diagnostic script:</p>' +
    '<pre style="background:#f5f5f5; padding:10px; overflow-x:auto;">' +
    'cd Donor-matching-coaDing\n' +
    'git pull  # Get latest version' +
    '</pre>' +
    '<p><strong>Step 2:</strong> Copy and paste this command into Terminal:</p>' +
    '<textarea readonly style="width:100%; height:120px; font-family:monospace; font-size:11px;">' + fullCmd + '</textarea>' +
    '<p style="margin-top:20px;"><strong>What this does:</strong></p>' +
    '<ul style="font-size:13px;">' +
    '<li>Downloads KREF and FEC data from Google Drive</li>' +
    '<li>Downloads your trained model weights</li>' +
    '<li>Runs diagnostic analysis</li>' +
    '</ul>' +
    '<p><strong>Expected Output:</strong></p>' +
    '<ul style="font-size:13px;">' +
    '<li> Distribution histogram (should show bimodal pattern)</li>' +
    '<li> List of obvious misses (same address + similar names)</li>' +
    '<li> Example pairs at different confidence levels</li>' +
    '<li> Recommendations for improving the model</li>' +
    '</ul>' +
    '<p style="color:#666; font-size:12px;"><em>Downloads expire in 30 minutes</em></p>' +
    '</div>'
  ).setWidth(650).setHeight(550);

  SpreadsheetApp.getUi().showModalDialog(html, 'Diagnostic Analysis');
}

/**
 * Returns the diagnostic Python script
 */
function diagnostic_getScript_() {
  const script = `#!/usr/bin/env python3
"""
Diagnostic tool for analyzing donor matching confidence distributions.

This script helps identify issues with the matching model by:
1. Analyzing the distribution of confidence scores
2. Identifying cases where obvious matches are missed
3. Finding high-confidence false positives
4. Providing insights for model improvement

Usage:
    python3 diagnostic_matching.py --bundle BUNDLE_URL [--threshold 0.7]
"""

import sys
import json
import csv
import io
import random
from collections import defaultdict
import math
import re
import urllib.request
import time

# HTTP helpers with retry logic (same as training/CD matcher)
def http_get(url):
    max_retries = 4
    delays = [2, 4, 8, 16]

    url_display = url if len(url) <= 100 else url[:97] + "..."
    print(f"\\nFetching: {url_display}")
    start_time = time.time()

    for attempt in range(max_retries + 1):
        try:
            with urllib.request.urlopen(url, timeout=300) as r:
                data = r.read()
                elapsed = time.time() - start_time
                size_mb = len(data) / (1024 * 1024)
                print(f" Received {size_mb:.2f} MB in {elapsed:.1f}s")
                return data
        except urllib.error.HTTPError as e:
            if e.code == 429 and attempt < max_retries:
                delay = delays[attempt]
                print(f"HTTP 429 rate limit. Retrying in {delay}s...")
                time.sleep(delay)
            else:
                raise
    raise Exception("Max retries exceeded")

def parse_csv(text):
    f = io.StringIO(text)
    rdr = csv.reader(f)
    rows = list(rdr)
    if not rows: return [], []
    header = rows[0]
    return header, rows[1:]

` + UNIFIED_MATCHING_MODULE + `

# Wrapper functions for diagnostic
def features(a, b):
    """Calculate 17 features using unified module."""
    return calculate_features(a, b)

def diagnostic_predict(a, b, weights):
    """Calculate match probability using unified predict."""
    return predict(a, b, weights)

def analyze_distribution(data, weights, sample_size=1000):
    """Analyze the confidence score distribution."""
    print("\\n" + "="*60)
    print("CONFIDENCE SCORE DISTRIBUTION ANALYSIS")
    print("="*60)

    # Sample pairs
    print(f"\\nSampling {sample_size} random pairs...")
    scores = []
    examples_low = []  # < 0.3
    examples_mid = []  # 0.4-0.6
    examples_high = []  # > 0.7

    for _ in range(sample_size):
        a = random.choice(data)
        b = random.choice(data)

        if a == b:
            continue

        score = predict(a, b, weights)
        scores.append(score)

        # Collect examples
        if score < 0.3 and len(examples_low) < 5:
            examples_low.append((a, b, score))
        elif 0.4 <= score <= 0.6 and len(examples_mid) < 5:
            examples_mid.append((a, b, score))
        elif score > 0.7 and len(examples_high) < 5:
            examples_high.append((a, b, score))

    # Statistics
    scores.sort()
    n = len(scores)
    mean = sum(scores) / n
    median = scores[n//2]

    # Distribution buckets
    buckets = {
        "0.0-0.1": 0,
        "0.1-0.2": 0,
        "0.2-0.3": 0,
        "0.3-0.4": 0,
        "0.4-0.5": 0,
        "0.5-0.6": 0,
        "0.6-0.7": 0,
        "0.7-0.8": 0,
        "0.8-0.9": 0,
        "0.9-1.0": 0
    }

    for score in scores:
        bucket_idx = min(int(score * 10), 9)
        bucket_key = f"{bucket_idx/10:.1f}-{(bucket_idx+1)/10:.1f}"
        buckets[bucket_key] += 1

    # Print statistics
    print(f"\\nStatistics:")
    print(f"  Mean:   {mean:.3f}")
    print(f"  Median: {median:.3f}")
    print(f"  Min:    {scores[0]:.3f}")
    print(f"  Max:    {scores[-1]:.3f}")

    print(f"\\nDistribution (expecting bimodal - peaks at <0.3 and >0.7):")
    max_count = max(buckets.values())
    for bucket, count in buckets.items():
        pct = count / n * 100
        bar_len = int(count / max_count * 50)
        bar = "" * bar_len
        print(f"  {bucket}: {bar} {count:4d} ({pct:5.1f}%)")

    # Check for bimodal
    low_conf = sum(1 for s in scores if s < 0.3)
    mid_conf = sum(1 for s in scores if 0.3 <= s <= 0.7)
    high_conf = sum(1 for s in scores if s > 0.7)

    print(f"\\nBimodal Analysis:")
    print(f"  Low confidence  (<0.3): {low_conf:4d} ({low_conf/n*100:5.1f}%)")
    print(f"  Mid confidence (0.3-0.7): {mid_conf:4d} ({mid_conf/n*100:5.1f}%)")
    print(f"  High confidence (>0.7): {high_conf:4d} ({high_conf/n*100:5.1f}%)")

    if mid_conf > low_conf and mid_conf > high_conf:
        print("\\n    WARNING: Distribution is NOT bimodal!")
        print("      Most scores are in the uncertain middle range.")
        print("      This suggests the model is not confident in its predictions.")
    else:
        print("\\n    Distribution appears bimodal (good!)")

    # Print example pairs
    print("\\n" + "="*60)
    print("EXAMPLE PAIRS")
    print("="*60)

    def print_pair(a, b, score, label):
        print(f"\\n{label} (confidence: {score:.3f})")
        print(f"  Person A: {a.get('DonorFirst','')} {a.get('DonorLast','')} | {a.get('Address1','')} | {a.get('City','')} {a.get('Zip','')}")
        print(f"  Person B: {b.get('DonorFirst','')} {b.get('DonorLast','')} | {b.get('Address1','')} | {b.get('City','')} {b.get('Zip','')}")

        # Show feature breakdown
        feat = features(a, b)
        print(f"  Features: name={feat[0]:.2f}, last={feat[1]:.2f}, addr={feat[2]:.2f}, emp={feat[3]:.2f}, occ={feat[4]:.2f}, zip={feat[5]:.0f}, city={feat[6]:.0f}")

    print("\\n--- LOW CONFIDENCE EXAMPLES (should be clear non-matches) ---")
    for a, b, score in examples_low:
        print_pair(a, b, score, "Low confidence")

    print("\\n--- MID CONFIDENCE EXAMPLES (uncertain) ---")
    for a, b, score in examples_mid:
        print_pair(a, b, score, "Mid confidence")

    print("\\n--- HIGH CONFIDENCE EXAMPLES (should be clear matches) ---")
    for a, b, score in examples_high:
        print_pair(a, b, score, "High confidence")

def find_obvious_misses(data, weights, threshold=0.7):
    """Find pairs that should obviously match but don't."""
    print("\\n" + "="*60)
    print("SEARCHING FOR OBVIOUS MISSES")
    print("="*60)

    # Look for same address + similar name
    print("\\nLooking for pairs with:")
    print("  - Exact same address")
    print("  - Similar last name (soundex match)")
    print(f"  - But confidence < {threshold}")

    address_index = defaultdict(list)
    for donor in data:
        addr = (donor.get("Address1","") or "").strip().upper()
        if addr:
            address_index[addr].append(donor)

    misses = []
    checked = 0

    for addr, donors in address_index.items():
        if len(donors) < 2:
            continue

        for i in range(len(donors)):
            for j in range(i+1, len(donors)):
                a = donors[i]
                b = donors[j]

                # Check if last names sound similar
                last_a = a.get("DonorLast","")
                last_b = b.get("DonorLast","")

                if soundex(last_a) == soundex(last_b):
                    score = predict(a, b, weights)
                    checked += 1

                    if score < threshold:
                        misses.append((a, b, score))

    print(f"\\nChecked {checked} pairs with same address + phonetic last name match")
    print(f"Found {len(misses)} with confidence < {threshold}")

    if misses:
        print(f"\\nShowing first 10 misses:")
        for a, b, score in misses[:10]:
            print(f"\\n  Confidence: {score:.3f} (should be >{threshold}!)")
            print(f"    Person A: {a.get('DonorFirst','')} {a.get('DonorLast','')} | {a.get('Address1','')}")
            print(f"    Person B: {b.get('DonorFirst','')} {b.get('DonorLast','')} | {b.get('Address1','')}")
            feat = features(a, b)
            print(f"    Features: name={feat[0]:.2f}, last={feat[1]:.2f}, addr={feat[2]:.2f}, zip={feat[5]:.0f}, city={feat[6]:.0f}")
    else:
        print("\\n   No obvious misses found!")

def main():
    import argparse
    parser = argparse.ArgumentParser(description='Diagnostic tool for donor matching')
    parser.add_argument('--bundle', required=True, help='Bundle URL from web app')
    parser.add_argument('--threshold', type=float, default=0.7, help='Match confidence threshold (default: 0.7)')

    args = parser.parse_args()

    # Fetch job configuration (same as training/CD matcher)
    job = json.loads(http_get(args.bundle).decode("utf-8"))

    # Load nickname database (automatic, like other scripts)
    load_nickname_db(job["modelUrl"])

    # Fetch data from web app (same sources as training)
    print("\\nLoading data from web app...")
    hdrK, rowsK = parse_csv(http_get(job["krefUrl"]).decode("utf-8"))
    hdrF, rowsF = parse_csv(http_get(job["fecUrl"]).decode("utf-8"))

    # Convert to dict format
    kref_data = [dict(zip(hdrK, row)) for row in rowsK]
    fec_data = [dict(zip(hdrF, row)) for row in rowsF]
    combined = kref_data + fec_data

    print(f"Loaded {len(kref_data)} KREF records")
    print(f"Loaded {len(fec_data)} FEC records")
    print(f"Total: {len(combined)} records")

    # Fetch model weights
    print("\\nLoading model weights...")
    model_data = json.loads(http_get(job["modelUrl"]).decode("utf-8"))
    weights = model_data.get("weights")

    if not weights:
        print("ERROR: No weights found in model!")
        sys.exit(1)

    print(f"Loaded model with {len(weights)} weights")
    print(f"Using match threshold: {args.threshold}")

    # Run diagnostics
    analyze_distribution(combined, weights, sample_size=2000)
    find_obvious_misses(combined, weights, threshold=args.threshold)

    print("\\n" + "="*60)
    print("DIAGNOSTIC COMPLETE")
    print("="*60)

if __name__ == "__main__":
    main()
`;

  return ContentService.createTextOutput(script).setMimeType(ContentService.MimeType.TEXT);
}

/**
 * Receives matching results and creates/appends to CD_To_Upload sheet
 * Supports batched uploads to handle large datasets
 */
function cd_receiveResults_(body) {
  try {
    const ss = SpreadsheetApp.getActive();
    const batchNum = body.batch_num || 1;
    const totalBatches = body.total_batches || 1;

    // Get or create CD_To_Upload sheet
    let uploadSheet = ss.getSheetByName('CD_To_Upload');
    if (!uploadSheet) {
      uploadSheet = ss.insertSheet('CD_To_Upload');
    }

    // Define header
    const header = [
      'PersonID', 'NameFirst', 'NameLast', 'Address', 'City', 'State', 'Zip',
      'TotalAmount', 'TotalWeighted', 'GaveLocally', 'NumberOfDonations', 'LargestYearAmount', 'LastYearAmount', 'Occupation', 'Employer', 'Notes'
    ];

    // First batch: Clear sheet and write header
    if (batchNum === 1) {
      uploadSheet.clear();
      uploadSheet.getRange(1, 1, 1, header.length).setValues([header]);
      uploadSheet.getRange(1, 1, 1, header.length).setFontWeight('bold').setBackground('#4285f4').setFontColor('#ffffff');
      uploadSheet.setFrozenRows(1);
    }

    // Write data (append to existing data)
    const rows = body.upload_rows || [];
    if (rows.length > 0) {
      const nextRow = uploadSheet.getLastRow() + 1;
      uploadSheet.getRange(nextRow, 1, rows.length, header.length).setValues(rows);
      Logger.log(`Wrote ${rows.length} rows starting at row ${nextRow}`);
    }

    const totalRows = uploadSheet.getLastRow() - 1; // Minus header
    Logger.log(`CD_To_Upload batch ${batchNum}/${totalBatches}: Added ${rows.length} rows (total: ${totalRows})`);

    // Auto-resize and cleanup AFTER confirming write succeeded
    if (batchNum === totalBatches) {
      try {
        // Auto-resize columns (non-critical, can fail)
        for (let i = 1; i <= header.length; i++) {
          uploadSheet.autoResizeColumn(i);
        }
        Logger.log('Columns auto-resized');
      } catch (e) {
        Logger.log('Auto-resize failed (non-critical): ' + e.message);
      }

      try {
        // Clean up temporary CSV files from Drive (non-critical, can fail)
        cd_cleanupTempFiles_();
        Logger.log('Temporary files cleaned up');
      } catch (e) {
        Logger.log('Cleanup failed (non-critical): ' + e.message);
      }
    }

    return {
      success: true,
      message: `Batch ${batchNum}/${totalBatches} uploaded`,
      batchRows: rows.length,
      totalRows: totalRows
    };

  } catch (error) {
    Logger.log('Error creating CD_To_Upload: ' + error.message);
    throw new Error('Failed to create upload sheet: ' + error.message);
  }
}

/**
 * Returns the Python script for Campaign Deputy matching
 */
function cd_getMatcherScript_() {
  const py = [
    '#!/usr/bin/env python3',
    'import sys, json, csv, io, urllib.request, argparse, math, re, time',
    'from collections import defaultdict',
    'from datetime import datetime',
    '',
    '# HTTP helpers with retry logic',
    'def http_get(url):',
    '    max_retries = 4',
    '    delays = [2, 4, 8, 16]  # exponential backoff in seconds',
    '    ',
    '    # Show which URL we\'re fetching (truncate for readability)',
    '    url_display = url if len(url) <= 100 else url[:97] + "..."',
    '    print(f"\\nFetching: {url_display}")',
    '    start_time = time.time()',
    '    ',
    '    for attempt in range(max_retries + 1):',
    '        try:',
    '            with urllib.request.urlopen(url, timeout=300) as r:',
    '                data = r.read()',
    '                elapsed = time.time() - start_time',
    '                size_mb = len(data) / (1024 * 1024)',
    '                print(f" Received {size_mb:.2f} MB in {elapsed:.1f}s")',
    '                return data',
    '        except urllib.error.HTTPError as e:',
    '            elapsed = time.time() - start_time',
    '            print(f" HTTP {e.code} error after {elapsed:.1f}s")',
    '            if e.code == 500:',
    '                print(f"   Server error - likely timeout or memory limit on server side")',
    '                print(f"   This often happens with very large datasets (100k+ rows)")',
    '            if e.code == 429 and attempt < max_retries:',
    '                delay = delays[attempt]',
    '                print(f"   Rate limit hit. Retrying in {delay}s... (attempt {attempt + 1}/{max_retries})")',
    '                time.sleep(delay)',
    '            else:',
    '                raise',
    '        except Exception as e:',
    '            elapsed = time.time() - start_time',
    '            print(f" Error after {elapsed:.1f}s: {type(e).__name__}: {e}")',
    '            raise',
    '    ',
    '    raise Exception("Max retries exceeded")',
    '',
    'def http_post(url, obj):',
    '    max_retries = 4',
    '    delays = [2, 4, 8, 16]  # exponential backoff in seconds',
    '    ',
    '    data = json.dumps(obj).encode("utf-8")',
    '    req = urllib.request.Request(url, data=data, headers={"Content-Type":"application/json"})',
    '    ',
    '    for attempt in range(max_retries + 1):',
    '        try:',
    '            with urllib.request.urlopen(req) as r:',
    '                return r.read().decode("utf-8")',
    '        except urllib.error.HTTPError as e:',
    '            if e.code == 429 and attempt < max_retries:',
    '                delay = delays[attempt]',
    '                print(f"HTTP 429 rate limit hit. Retrying in {delay}s... (attempt {attempt + 1}/{max_retries})")',
    '                time.sleep(delay)',
    '            else:',
    '                raise',
    '    ',
    '    raise Exception("Max retries exceeded")',
    '',
    'def parse_csv(text):',
    '    f = io.StringIO(text)',
    '    rdr = csv.reader(f)',
    '    rows = list(rdr)',
    '    if not rows: return [], []',
    '    return rows[0], rows[1:]',
    '',
    '# Parse args',
    'ap = argparse.ArgumentParser()',
    'ap.add_argument("--merge", required=True, help="URL to merge output CSV")',
    'ap.add_argument("--cd", required=True, help="URL to Campaign Deputy CSV")',
    'ap.add_argument("--result", required=True, help="URL to post results")',
    'ap.add_argument("--model", required=True, help="URL to get trained model")',
    'args = ap.parse_args()',
    '',
    'print("Downloading data from Google Drive...")',
    'print("(This downloads directly from Drive, bypassing Apps Script limits)")',
    'merge_hdr, merge_rows = parse_csv(http_get(args.merge).decode("utf-8"))',
    'cd_hdr, cd_rows = parse_csv(http_get(args.cd).decode("utf-8"))',
    '',
    'print(f"Loaded {len(merge_rows)} donation records")',
    'print(f"Loaded {len(cd_rows)} Campaign Deputy donors")',
    '',
    UNIFIED_MATCHING_MODULE,
    '',
    '# Load existing model weights',
    'print("\\nChecking for trained model...")',
    'model_data = json.loads(http_get(args.model).decode("utf-8"))',
    'old_w = model_data.get("weights")',
    '',
    'if not old_w:',
    '    print("ERROR: No trained model found. Please run donor matching first to train the model.")',
    '    sys.exit(1)',
    '',
    '# Handle model version compatibility',
    'if len(old_w) == 16:',
    '    print("  Detected old model format (15 features + bias)")',
    '    print("  Upgrading to new model format (17 features + bias)")',
    '    print("  Adding 2 new geographic features (zip, city)")',
    '    # Expand weights: old had 15 features + bias, new has 17 features + bias',
    '    # Insert zeros at positions 5 and 6 (zip and city features)',
    '    w = old_w[:5] + [0.0, 0.0] + old_w[5:]',
    '    print(f"  Expanded weights from {len(old_w)} to {len(w)} elements")',
    'elif len(old_w) == 18:',
    '    print("  Model already in new format (17 features + bias)")',
    '    w = old_w',
    'else:',
    '    print(f"  ERROR: Unexpected weight count: {len(old_w)}")',
    '    print(f"  Expected 16 (old format) or 18 (new format)")',
    '    sys.exit(1)',
    '',
    'print(f"Using model with {len(w)} weights")',
    '',
    '# Load nickname database',
    'load_nickname_db(args.model)',
    '',
    '# Load learned associations from training data',
    'print("\\nLoading learned associations from training data...")',
    'try:',
    '    training_url = args.model.replace("model=1", "training=1")',
    '    training_json = http_get(training_url).decode("utf-8")',
    '    training_data = json.loads(training_json)',
    '    training_rows = training_data.get("training_rows", [])',
    '    ',
    '    # Learn associations using same function as donor dedup',
    '    def learn_associations_cd(training_rows):',
    '        """Analyze matched pairs to learn which terms co-occur."""',
    '        first_cooccur = defaultdict(lambda: defaultdict(int))',
    '        last_cooccur = defaultdict(lambda: defaultdict(int))',
    '        address_cooccur = defaultdict(lambda: defaultdict(int))',
    '        employer_cooccur = defaultdict(lambda: defaultdict(int))',
    '        occupation_cooccur = defaultdict(lambda: defaultdict(int))',
    '        ',
    '        for row in training_rows:',
    '            if row.get("label") != 1:  # Only learn from matches',
    '                continue',
    '            ',
    '            def normalize(val):',
    '                if not val: return "EMPTY"',
    '                s = str(val).strip().upper()',
    '                if s in ["", "N/A", "NA", "NULL", "NONE", "UNKNOWN"]: return "EMPTY"',
    '                return s',
    '            ',
    '            first_a = normalize(row["a"].get("first", ""))',
    '            first_b = normalize(row["b"].get("first", ""))',
    '            last_a = normalize(row["a"].get("last", ""))',
    '            last_b = normalize(row["b"].get("last", ""))',
    '            addr_a = normalize(row["a"].get("addr", ""))',
    '            addr_b = normalize(row["b"].get("addr", ""))',
    '            emp_a = normalize(row["a"].get("employer", ""))',
    '            emp_b = normalize(row["b"].get("employer", ""))',
    '            occ_a = normalize(row["a"].get("occupation", ""))',
    '            occ_b = normalize(row["b"].get("occupation", ""))',
    '            ',
    '            if first_a != first_b:',
    '                first_cooccur[first_a][first_b] += 1',
    '                first_cooccur[first_b][first_a] += 1',
    '            if last_a != last_b:',
    '                last_cooccur[last_a][last_b] += 1',
    '                last_cooccur[last_b][last_a] += 1',
    '            if addr_a != addr_b:',
    '                address_cooccur[addr_a][addr_b] += 1',
    '                address_cooccur[addr_b][addr_a] += 1',
    '            if emp_a != emp_b:',
    '                employer_cooccur[emp_a][emp_b] += 1',
    '                employer_cooccur[emp_b][emp_a] += 1',
    '            if occ_a != occ_b:',
    '                occupation_cooccur[occ_a][occ_b] += 1',
    '                occupation_cooccur[occ_b][occ_a] += 1',
    '        ',
    '        return (dict(first_cooccur), dict(last_cooccur), dict(address_cooccur),',
    '                dict(employer_cooccur), dict(occupation_cooccur))',
    '    ',
    '    (first_associations, last_associations, address_associations,',
    '     employer_associations, occupation_associations) = learn_associations_cd(training_rows)',
    '    ',
    '    first_pairs = sum(len(v) for v in first_associations.values()) // 2',
    '    last_pairs = sum(len(v) for v in last_associations.values()) // 2',
    '    addr_pairs = sum(len(v) for v in address_associations.values()) // 2',
    '    emp_pairs = sum(len(v) for v in employer_associations.values()) // 2',
    '    occ_pairs = sum(len(v) for v in occupation_associations.values()) // 2',
    '    ',
    '    print(f"  Learned {first_pairs} first name associations")',
    '    print(f"  Learned {last_pairs} last name associations")',
    '    print(f"  Learned {addr_pairs} address associations")',
    '    print(f"  Learned {emp_pairs} employer associations")',
    '    print(f"  Learned {occ_pairs} occupation associations")',
    'except Exception as e:',
    '    print(f"  Warning: Could not load training data: {e}")',
    '    print(f"  Proceeding without learned associations")',
    '    first_associations = {}',
    '    last_associations = {}',
    '    address_associations = {}',
    '    employer_associations = {}',
    '    occupation_associations = {}',
    '',
    '# Parse merge output into donor groups',
    'print("\\nGrouping donations by DonorID...")',
    'donor_groups = defaultdict(list)',
    '',
    'for row in merge_rows:',
    '    record = dict(zip(merge_hdr, row))',
    '    donor_id = record.get("DonorID", "")',
    '    if donor_id:',
    '        donor_groups[donor_id].append(record)',
    '',
    'print(f"Found {len(donor_groups)} unique DonorIDs")',
    '',
    '# Field mappings for cross-dataset matching',
    'DONOR_FIELD_MAP = {',
    '    "first": "DonorFirst", "last": "DonorLast", "addr": "Address1",',
    '    "city": "City", "state": "State", "zip": "Zip",',
    '    "employer": "Employer", "occupation": "Occupation"',
    '}',
    '',
    'CD_FIELD_MAP = {',
    '    "first": "NameFirst", "last": "NameLast", "addr": "DeliveryLine1",',
    '    "city": "NameCity", "state": "NameState", "zip": "NameZip",',
    '    "employer": "Employer", "occupation": "Occupation"',
    '}',
    '',
    '# Features for cross-dataset matching',
    'def cd_features(donor_rec, cd_rec):',
    '    """Calculate features for donor-to-CD matching."""',
    '    return calculate_features(donor_rec, cd_rec, DONOR_FIELD_MAP, CD_FIELD_MAP)',
    '',
    '# Predict match probability for CD matching',
    'def cd_predict(donor_rec, cd_rec):',
    '    x = cd_features(donor_rec, cd_rec)',
    '    z = w[-1]',
    '    for i in range(len(x)):',
    '        z += w[i] * x[i]',
    '    return sigmoid(z)',
    '',
    '# Match DonorIDs to PersonIDs',
    'print("\\nMatching DonorIDs to Campaign Deputy PersonIDs...")',
    'print("Using blocking by last name to speed up matching...")',
    'donor_to_person = {}',
    'threshold = 0.7',
    '',
    '# Build blocking index for CD donors by last name',
    'cd_blocks = defaultdict(list)',
    'for i, cd_row in enumerate(cd_rows):',
    '    cd_rec = dict(zip(cd_hdr, cd_row))',
    '    last_name = (cd_rec.get("NameLast", "") or "").strip().upper()',
    '    if last_name:',
    '        # Block by first 4 chars of last name',
    '        block_key = last_name[:4]',
    '        cd_blocks[block_key].append((i, cd_rec))',
    '',
    'print(f"Created {len(cd_blocks)} blocks for matching")',
    '',
    '# Campaign Deputy matching strategy',
    'import time',
    '',
    'print("\\n" + "="*60)',
    'print("CAMPAIGN DEPUTY MATCHING STRATEGY")',
    'print("="*60)',
    'print("Pass 0: Exact matching (normalized name + address)")',
    'print("Pass 1: Limited matching (max 50 candidates per donor)")',
    'print("Pass 2: Thorough matching (max 200 candidates per donor)")',
    'print("Pass 3: Exhaustive matching (unlimited candidates)")',
    'print("="*60)',
    '',
    '# Build index for exact matching',
    'print("\\n" + "="*60)',
    'print("PASS 0: Exact Matching")',
    'print("="*60)',
    'print("Building exact match index...")',
    'cd_exact_index = {}',
    'for i, cd_row in enumerate(cd_rows):',
    '    cd_rec = dict(zip(cd_hdr, cd_row))',
    '    person_id = cd_rec.get("PersonID", "")',
    '    if person_id:',
    '        # Create exact match key from normalized fields',
    '        first = (cd_rec.get("NameFirst", "") or "").strip().upper()',
    '        last = (cd_rec.get("NameLast", "") or "").strip().upper()',
    '        # Use enhanced address normalization (handles abbreviations)',
    '        addr = normalize_address(cd_rec.get("DeliveryLine1", ""))',
    '        if first and last and addr:',
    '            exact_key = (first, last, addr)',
    '            if exact_key not in cd_exact_index:',
    '                cd_exact_index[exact_key] = (person_id, cd_rec)',
    '',
    'print(f"  Created index with {len(cd_exact_index)} unique combinations")',
    '',
    'start_time = time.time()',
    'total = len(donor_groups)',
    'matched_count = 0',
    'donor_to_person = {}',
    'used_person_ids = set()',
    'remaining_donors = []',
    '',
    'for idx, (donor_id, records) in enumerate(donor_groups.items(), 1):',
    '    if idx % 1000 == 0 or idx == 1:',
    '        print(f"  Progress: {idx}/{total} ({100*idx/total:.1f}%) - Matched: {matched_count}", flush=True)',
    '',
    '    donor_rec = records[0]',
    '    first = (donor_rec.get("DonorFirst", "") or "").strip().upper()',
    '    last = (donor_rec.get("DonorLast", "") or "").strip().upper()',
    '    # Use enhanced address normalization (handles abbreviations)',
    '    addr = normalize_address(donor_rec.get("Address1", ""))',
    '',
    '    exact_key = (first, last, addr)',
    '    if exact_key in cd_exact_index:',
    '        person_id, cd_rec = cd_exact_index[exact_key]',
    '        if person_id not in used_person_ids:',
    '            donor_to_person[donor_id] = person_id',
    '            used_person_ids.add(person_id)',
    '            matched_count += 1',
    '        else:',
    '            remaining_donors.append(donor_id)',
    '    else:',
    '        remaining_donors.append(donor_id)',
    '',
    'print(f"\\nPass 0 complete: {matched_count} matched, {len(remaining_donors)} remaining")',
    '',
    '# PASS 1: Limited matching',
    'print("\\n" + "="*60)',
    'print("PASS 1: Limited Matching")',
    'print("="*60)',
    'start_time = time.time()',
    'total = len(remaining_donors)',
    'matched_count_pass1 = 0',
    'large_block_count = 0',
    'unmatched_donors = []',
    '',
    'for idx, donor_id in enumerate(remaining_donors, 1):',
    '    # Progress reporting',
    '    if idx % 500 == 0 or idx == 1:',
    '        elapsed = time.time() - start_time',
    '        rate = idx / elapsed if elapsed > 0 else 0',
    '        remaining = (total - idx) / rate if rate > 0 else 0',
    '        mins = int(remaining / 60)',
    '        secs = int(remaining % 60)',
    '        print(f"  Progress: {idx}/{total} ({100*idx/total:.1f}%) - Matched: {matched_count_pass1} - Large blocks: {large_block_count} - ETA: {mins}m {secs}s", flush=True)',
    '    ',
    '    records = donor_groups[donor_id]',
    '    ',
    '    donor_rec = records[0]',
    '    donor_last = (donor_rec.get("DonorLast", "") or "").strip().upper()',
    '    ',
    '    best_match = None',
    '    best_prob = 0',
    '    ',
    '    # Get candidates from block',
    '    candidates = []',
    '    if donor_last:',
    '        block_key = donor_last[:4]',
    '        candidates = cd_blocks.get(block_key, [])',
    '    ',
    '    # Fallback: expand search for small blocks',
    '    if len(candidates) < 5:',
    '        for prefix_len in [3, 2, 1]:',
    '            if len(donor_last) >= prefix_len:',
    '                block_key = donor_last[:prefix_len]',
    '                for key in cd_blocks:',
    '                    if key.startswith(block_key):',
    '                        candidates.extend(cd_blocks[key])',
    '                if len(candidates) >= 20:',
    '                    break',
    '    ',
    '    # FIRST PASS: Limit to 50 candidates for speed',
    '    if len(candidates) > 50:',
    '        large_block_count += 1',
    '        candidates = candidates[:50]',
    '    ',
    '    # Find best match',
    '    for i, cd_rec in candidates:',
    '        prob = cd_predict(donor_rec, cd_rec)',
    '        if prob >= threshold and prob > best_prob:',
    '            best_prob = prob',
    '            best_match = cd_rec.get("PersonID", "")',
    '    ',
    '    if best_match:',
    '        donor_to_person[donor_id] = best_match',
    '        used_person_ids.add(best_match)',
    '        matched_count_pass1 += 1',
    '    else:',
    '        unmatched_donors.append(donor_id)',
    '',
    'print(f"\\nPass 1 complete: {matched_count_pass1} matched, {len(unmatched_donors)} remaining")',
    'print(f"  Large blocks encountered: {large_block_count}")',
    '',
    '# PASS 2: Thorough matching',
    'unmatched_pass2 = []',
    'if unmatched_donors:',
    '    print("\\n" + "="*60)',
    '    print("PASS 2: Thorough Matching")',
    '    print("="*60)',
    '    print(f"Processing {len(unmatched_donors)} remaining donors against {len(cd_rows) - len(used_person_ids)} remaining CD records")',
    '    ',
    '    # Rebuild blocks excluding already-matched PersonIDs',
    '    cd_blocks_filtered = defaultdict(list)',
    '    for i, cd_row in enumerate(cd_rows):',
    '        cd_rec = dict(zip(cd_hdr, cd_row))',
    '        person_id = cd_rec.get("PersonID", "")',
    '        if person_id and person_id not in used_person_ids:',
    '            last_name = (cd_rec.get("NameLast", "") or "").strip().upper()',
    '            if last_name:',
    '                block_key = last_name[:4]',
    '                cd_blocks_filtered[block_key].append((i, cd_rec))',
    '    ',
    '    print(f"Rebuilt {len(cd_blocks_filtered)} blocks for pass 2")',
    '    ',
    '    start_time_pass2 = time.time()',
    '    matched_count_pass2 = 0',
    '    total_pass2 = len(unmatched_donors)',
    '    ',
    '    for idx, donor_id in enumerate(unmatched_donors, 1):',
    '        records = donor_groups[donor_id]',
    '        # Progress reporting',
    '        if idx % 200 == 0 or idx == 1:',
    '            elapsed = time.time() - start_time_pass2',
    '            rate = idx / elapsed if elapsed > 0 else 0',
    '            remaining = (total_pass2 - idx) / rate if rate > 0 else 0',
    '            mins = int(remaining / 60)',
    '            secs = int(remaining % 60)',
    '            print(f"  Progress: {idx}/{total_pass2} ({100*idx/total_pass2:.1f}%) - Matched: {matched_count_pass2} - ETA: {mins}m {secs}s", flush=True)',
    '        ',
    '        donor_rec = records[0]',
    '        donor_last = (donor_rec.get("DonorLast", "") or "").strip().upper()',
    '        ',
    '        best_match = None',
    '        best_prob = 0',
    '        ',
    '        # Get candidates from filtered blocks',
    '        candidates = []',
    '        if donor_last:',
    '            block_key = donor_last[:4]',
    '            candidates = cd_blocks_filtered.get(block_key, [])',
    '        ',
    '        # Fallback: expand search',
    '        if len(candidates) < 5:',
    '            for prefix_len in [3, 2, 1]:',
    '                if len(donor_last) >= prefix_len:',
    '                    block_key = donor_last[:prefix_len]',
    '                    for key in cd_blocks_filtered:',
    '                        if key.startswith(block_key):',
    '                            candidates.extend(cd_blocks_filtered[key])',
    '                    if len(candidates) >= 20:',
    '                        break',
    '        ',
    '        # SECOND PASS: Higher limit (200) since pool is smaller',
    '        if len(candidates) > 200:',
    '            candidates = candidates[:200]',
    '        ',
    '        # Find best match',
    '        for i, cd_rec in candidates:',
    '            prob = cd_predict(donor_rec, cd_rec)',
    '            if prob >= threshold and prob > best_prob:',
    '                best_prob = prob',
    '                best_match = cd_rec.get("PersonID", "")',
    '        ',
    '        if best_match:',
    '            donor_to_person[donor_id] = best_match',
    '            used_person_ids.add(best_match)',
    '            matched_count_pass2 += 1',
    '        else:',
    '            unmatched_pass2.append(donor_id)',
    '    ',
    '    print(f"\\nPass 2 complete: {matched_count_pass2} matched, {len(unmatched_pass2)} remaining")',
    '',
    '# PASS 3: Exhaustive matching (unlimited candidates)',
    'matched_count_pass3 = 0',
    'if unmatched_pass2:',
    '    print("\\n" + "="*60)',
    '    print("PASS 3: Exhaustive Matching")',
    '    print("="*60)',
    '    print(f"Processing {len(unmatched_pass2)} remaining donors with unlimited candidate search")',
    '    ',
    '    # Rebuild blocks excluding already-matched PersonIDs',
    '    cd_blocks_exhaustive = defaultdict(list)',
    '    for i, cd_row in enumerate(cd_rows):',
    '        cd_rec = dict(zip(cd_hdr, cd_row))',
    '        person_id = cd_rec.get("PersonID", "")',
    '        if person_id and person_id not in used_person_ids:',
    '            last_name = (cd_rec.get("NameLast", "") or "").strip().upper()',
    '            if last_name:',
    '                block_key = last_name[:4]',
    '                cd_blocks_exhaustive[block_key].append((i, cd_rec))',
    '    ',
    '    print(f"  Rebuilt {len(cd_blocks_exhaustive)} blocks for pass 3")',
    '    ',
    '    start_time_pass3 = time.time()',
    '    total_pass3 = len(unmatched_pass2)',
    '    ',
    '    for idx, donor_id in enumerate(unmatched_pass2, 1):',
    '        records = donor_groups[donor_id]',
    '        # Progress reporting',
    '        if idx % 100 == 0 or idx == 1:',
    '            elapsed = time.time() - start_time_pass3',
    '            rate = idx / elapsed if elapsed > 0 else 0',
    '            remaining_time = (total_pass3 - idx) / rate if rate > 0 else 0',
    '            mins = int(remaining_time / 60)',
    '            secs = int(remaining_time % 60)',
    '            print(f"  Progress: {idx}/{total_pass3} ({100*idx/total_pass3:.1f}%) - Matched: {matched_count_pass3} - ETA: {mins}m {secs}s", flush=True)',
    '        ',
    '        donor_rec = records[0]',
    '        donor_last = (donor_rec.get("DonorLast", "") or "").strip().upper()',
    '        ',
    '        best_match = None',
    '        best_prob = 0',
    '        ',
    '        # Get candidates from filtered blocks',
    '        candidates = []',
    '        if donor_last:',
    '            block_key = donor_last[:4]',
    '            candidates = cd_blocks_exhaustive.get(block_key, [])',
    '        ',
    '        # Fallback: expand search',
    '        if len(candidates) < 5:',
    '            for prefix_len in [3, 2, 1]:',
    '                if len(donor_last) >= prefix_len:',
    '                    block_key = donor_last[:prefix_len]',
    '                    for key in cd_blocks_exhaustive:',
    '                        if key.startswith(block_key):',
    '                            candidates.extend(cd_blocks_exhaustive[key])',
    '                    if len(candidates) >= 20:',
    '                        break',
    '        ',
    '        # THIRD PASS: NO LIMIT - check all candidates',
    '        # Find best match',
    '        for i, cd_rec in candidates:',
    '            prob = cd_predict(donor_rec, cd_rec)',
    '            if prob >= threshold and prob > best_prob:',
    '                best_prob = prob',
    '                best_match = cd_rec.get("PersonID", "")',
    '        ',
    '        if best_match:',
    '            donor_to_person[donor_id] = best_match',
    '            used_person_ids.add(best_match)',
    '            matched_count_pass3 += 1',
    '    ',
    '    print(f"\\nPass 3 complete: {matched_count_pass3} matched")',
    '',
    '# Summary',
    'total_matched = len(donor_to_person)',
    'total_unmatched = len(donor_groups) - total_matched',
    'print("\\n" + "="*60)',
    'print("MATCHING COMPLETE")',
    'print("="*60)',
    'print(f"Pass 0 (Exact):      {matched_count} matches")',
    'print(f"Pass 1 (Limited):    {matched_count_pass1} matches")',
    'print(f"Pass 2 (Thorough):   {matched_count_pass2} matches")',
    'print(f"Pass 3 (Exhaustive): {matched_count_pass3} matches")',
    'print(f"TOTAL: {total_matched} matched ({100*total_matched/len(donor_groups):.1f}%)")',
    'print(f"UNMATCHED: {total_unmatched} ({100*total_unmatched/len(donor_groups):.1f}%)")',
    'print("="*60)',
    '',
    '# Helper to check if value is "broken" (empty or NA)',
    'def is_broken(val):',
    '    if not val: return True',
    '    s = str(val).strip().upper()',
    '    return s in ["", "NA", "N/A", "UNKNOWN", "NULL"]',
    '',
    '# Valid US state abbreviations (50 states + DC + territories)',
    'VALID_US_STATES = {',
    '    "AL", "AK", "AZ", "AR", "CA", "CO", "CT", "DE", "FL", "GA",',
    '    "HI", "ID", "IL", "IN", "IA", "KS", "KY", "LA", "ME", "MD",',
    '    "MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ",',
    '    "NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA", "RI", "SC",',
    '    "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY",',
    '    "DC", "PR", "VI", "GU", "AS", "MP"',
    '}',
    '',
    '# Check if a state value is broken or invalid',
    'def is_broken_state(val):',
    '    if not val: return True',
    '    s = str(val).strip().upper()',
    '    # Check if empty, NA, or other broken values',
    '    if s in ["", "NA", "N/A", "UNKNOWN", "NULL"]: return True',
    '    # Also check if it\'s not a valid US state',
    '    return s not in VALID_US_STATES',
    '',
    '# ZIP prefix to State lookup (covers most common ZIP ranges)',
    'ZIP_TO_STATE = {',
    '    "900": "CA", "901": "CA", "902": "CA", "903": "CA", "904": "CA", "905": "CA", "906": "CA", "907": "CA", "908": "CA",',
    '    "910": "CA", "911": "CA", "912": "CA", "913": "CA", "914": "CA", "915": "CA", "916": "CA", "917": "CA", "918": "CA",',
    '    "919": "CA", "920": "CA", "921": "CA", "922": "CA", "923": "CA", "924": "CA", "925": "CA", "926": "CA", "927": "CA",',
    '    "928": "CA", "930": "CA", "931": "CA", "932": "CA", "933": "CA", "934": "CA", "935": "CA", "936": "CA", "937": "CA",',
    '    "938": "CA", "939": "CA", "940": "CA", "941": "CA", "942": "CA", "943": "CA", "944": "CA", "945": "CA", "946": "CA",',
    '    "947": "CA", "948": "CA", "949": "CA", "950": "CA", "951": "CA", "952": "CA", "953": "CA", "954": "CA", "955": "CA",',
    '    "956": "CA", "957": "CA", "958": "CA", "959": "CA", "960": "CA", "961": "CA",',
    '    "100": "NY", "101": "NY", "102": "NY", "103": "NY", "104": "NY", "105": "NY", "106": "NY", "107": "NY", "108": "NY",',
    '    "109": "NY", "110": "NY", "111": "NY", "112": "NY", "113": "NY", "114": "NY", "115": "NY", "116": "NY", "117": "NY",',
    '    "118": "NY", "119": "NY", "120": "NY", "121": "NY", "122": "NY", "123": "NY", "124": "NY", "125": "NY", "126": "NY",',
    '    "127": "NY", "128": "NY", "129": "NY", "130": "NY", "131": "NY", "132": "NY", "133": "NY", "134": "NY", "135": "NY",',
    '    "136": "NY", "137": "NY", "138": "NY", "139": "NY", "140": "NY", "141": "NY", "142": "NY", "143": "NY", "144": "NY",',
    '    "145": "NY", "146": "NY", "147": "NY", "148": "NY", "149": "NY",',
    '    "600": "IL", "601": "IL", "602": "IL", "603": "IL", "604": "IL", "605": "IL", "606": "IL", "607": "IL", "608": "IL",',
    '    "609": "IL", "610": "IL", "611": "IL", "612": "IL", "613": "IL", "614": "IL", "615": "IL", "616": "IL", "617": "IL",',
    '    "618": "IL", "619": "IL", "620": "IL", "622": "IL", "623": "IL", "624": "IL", "625": "IL", "626": "IL", "627": "IL",',
    '    "628": "IL", "629": "IL",',
    '    "700": "LA", "701": "LA", "702": "LA", "703": "LA", "704": "LA", "705": "LA", "706": "LA", "707": "LA", "708": "LA",',
    '    "710": "LA", "711": "LA", "712": "LA", "713": "LA", "714": "LA",',
    '    "750": "TX", "751": "TX", "752": "TX", "753": "TX", "754": "TX", "755": "TX", "756": "TX", "757": "TX", "758": "TX",',
    '    "759": "TX", "760": "TX", "761": "TX", "762": "TX", "763": "TX", "764": "TX", "765": "TX", "766": "TX", "767": "TX",',
    '    "768": "TX", "769": "TX", "770": "TX", "771": "TX", "772": "TX", "773": "TX", "774": "TX", "775": "TX", "776": "TX",',
    '    "777": "TX", "778": "TX", "779": "TX", "780": "TX", "781": "TX", "782": "TX", "783": "TX", "784": "TX", "785": "TX",',
    '    "786": "TX", "787": "TX", "788": "TX", "789": "TX", "790": "TX", "791": "TX", "792": "TX", "793": "TX", "794": "TX",',
    '    "795": "TX", "796": "TX", "797": "TX", "798": "TX", "799": "TX",',
    '    "300": "FL", "301": "FL", "302": "FL", "303": "FL", "304": "FL", "305": "FL", "306": "FL", "307": "FL", "308": "FL",',
    '    "309": "FL", "310": "FL", "311": "FL", "312": "FL", "313": "FL", "314": "FL", "315": "FL", "316": "FL", "317": "FL",',
    '    "318": "FL", "319": "FL", "320": "FL", "321": "FL", "322": "FL", "323": "FL", "324": "FL", "325": "FL", "326": "FL",',
    '    "327": "FL", "328": "FL", "329": "FL", "330": "FL", "331": "FL", "332": "FL", "333": "FL", "334": "FL", "335": "FL",',
    '    "336": "FL", "337": "FL", "338": "FL", "339": "FL",',
    '    "150": "PA", "151": "PA", "152": "PA", "153": "PA", "154": "PA", "155": "PA", "156": "PA", "157": "PA", "158": "PA",',
    '    "159": "PA", "160": "PA", "161": "PA", "162": "PA", "163": "PA", "164": "PA", "165": "PA", "166": "PA", "167": "PA",',
    '    "168": "PA", "169": "PA", "170": "PA", "171": "PA", "172": "PA", "173": "PA", "174": "PA", "175": "PA", "176": "PA",',
    '    "177": "PA", "178": "PA", "179": "PA", "180": "PA", "181": "PA", "182": "PA", "183": "PA", "184": "PA", "185": "PA",',
    '    "186": "PA", "187": "PA", "188": "PA", "189": "PA", "190": "PA", "191": "PA", "192": "PA", "193": "PA", "194": "PA",',
    '    "195": "PA", "196": "PA",',
    '}',
    '',
    '# Lookup state from ZIP code',
    'def lookup_state_from_zip(zip_code):',
    '    if not zip_code: return ""',
    '    zip_str = str(zip_code).strip()[:3]',
    '    return ZIP_TO_STATE.get(zip_str, "")',
    '',
    '# Parse date with flexible formats',
    'def parse_date(date_str):',
    '    if not date_str: return None',
    '    s = str(date_str).strip()',
    '    if not s: return None',
    '    try:',
    '        # Try multiple formats to handle various date inputs',
    '        # Handles both "5/15/2018" and "05/15/2018"',
    '        for fmt in [',
    '            "%m/%d/%Y",    # 05/15/2018',
    '            "%-m/%-d/%Y",  # 5/15/2018 (Unix/Mac)',
    '            "%#m/%#d/%Y",  # 5/15/2018 (Windows)',
    '            "%Y-%m-%d",    # 2018-05-15',
    '            "%m-%d-%Y",    # 05-15-2018',
    '            "%-m-%-d-%Y",  # 5-15-2018',
    '        ]:',
    '            try: return datetime.strptime(s, fmt)',
    '            except: pass',
    '        ',
    '        # Fallback: try to handle single-digit dates manually',
    '        parts = s.replace("-", "/").split("/")',
    '        if len(parts) == 3:',
    '            try:',
    '                m, d, y = int(parts[0]), int(parts[1]), int(parts[2])',
    '                # Handle 2-digit years',
    '                if y < 100:',
    '                    y += 2000 if y < 50 else 1900',
    '                return datetime(y, m, d)',
    '            except: pass',
    '    except: pass',
    '    return None',
    '',
    '# Build upload rows',
    'print("\\nGenerating CD_To_Upload data...")',
    'upload_rows = []',
    '',
    'for donor_id, records in donor_groups.items():',
    '    person_id = donor_to_person.get(donor_id, "")',
    '    ',
    '    # Sort by date (newest first), preserve original date strings',
    '    dated_records = []',
    '    for rec in records:',
    '        date_str = rec.get("ReceiptDate", "")',
    '        dt = parse_date(date_str)',
    '        # Store: (parsed_date, original_string, record)',
    '        dated_records.append((dt if dt else datetime(1900, 1, 1), date_str, rec))',
    '    dated_records.sort(key=lambda x: x[0], reverse=True)',
    '    sorted_records = [r for _, _, r in dated_records]',
    '    ',
    '    # Smart field selection (most recent non-broken)',
    '    def pick_field(field_name):',
    '        for rec in sorted_records:',
    '            val = rec.get(field_name, "")',
    '            if not is_broken(val):',
    '                return val',
    '        return ""',
    '    ',
    '    first_name = pick_field("DonorFirst")',
    '    last_name = pick_field("DonorLast")',
    '    address = pick_field("Address1")',
    '    city = pick_field("City")',
    '    state = pick_field("State")',
    '    # Fallback to ZIP lookup if state is missing or invalid',
    '    if not state or is_broken_state(state):',
    '        zip_code = pick_field("Zip")',
    '        state = lookup_state_from_zip(zip_code)',
    '    else:',
    '        zip_code = pick_field("Zip")',
    '    occupation = pick_field("Occupation")',
    '    employer = pick_field("Employer")',
    '    ',
    '    # Calculate totals',
    '    total_amount = 0',
    '    total_weighted = 0',
    '    gave_locally = False  # Check if any donation is from KREF (origin "K")',
    '    num_donations = len(records)  # Total count of donations',
    '    ',
    '    # Group donations by year for annual calculations',
    '    year_amounts = defaultdict(float)',
    '    ',
    '    for rec in records:',
    '        try: total_amount += float(rec.get("Amount", 0) or 0)',
    '        except: pass',
    '        try: total_weighted += float(rec.get("WeightedAmount", 0) or 0)',
    '        except: pass',
    '        ',
    '        # Check if this donation is from KREF (origin starts with "K:")',
    '        donation_id = rec.get("DonationID", "")',
    '        if donation_id.startswith("K:"):',
    '            gave_locally = True',
    '        ',
    '        # Group by year for annual totals',
    '        date_str = rec.get("ReceiptDate", "")',
    '        dt = parse_date(date_str)',
    '        if dt and dt.year > 1900:  # Valid year',
    '            try:',
    '                amount = float(rec.get("Amount", 0) or 0)',
    '                year_amounts[dt.year] += amount',
    '            except:',
    '                pass',
    '    ',
    '    # Calculate largest year amount and last year amount',
    '    largest_year_amount = max(year_amounts.values()) if year_amounts else 0',
    '    current_year = datetime.now().year',
    '    last_year = current_year - 1',
    '    last_year_amount = year_amounts.get(last_year, 0)',
    '    ',
    '    # Build notes - Option 2 format: $amount to recipient on date',
    '    # Find oldest valid date for summary (use original string)',
    '    oldest_info = min(((dt, date_str) for dt, date_str, _ in dated_records if dt.year > 1900), default=(None, ""))',
    '    oldest_str = oldest_info[1] if oldest_info[0] and oldest_info[1] else "unknown date"',
    '    ',
    '    notes = f"Donor gave ${total_amount:.2f} since {oldest_str} with a weighted score of {total_weighted:.0f}."',
    '    ',
    '    # Add all donations with amounts and recipients (use original date strings)',
    '    for dt, date_str, rec in dated_records:',
    '        amt = rec.get("Amount", "")',
    '        recipient = rec.get("Recipient", "Unknown")',
    '        # Use original date string if available, otherwise "unknown date"',
    '        display_date = date_str.strip() if date_str and str(date_str).strip() else "unknown date"',
    '        notes += f"\\n${amt} to {recipient} on {display_date}"',
    '    ',
    '    upload_rows.append([',
    '        person_id,',
    '        first_name,',
    '        last_name,',
    '        address,',
    '        city,',
    '        state,',
    '        zip_code,',
    '        f"${total_amount:.2f}",',
    '        f"${total_weighted:.2f}",',
    '        "TRUE" if gave_locally else "FALSE",  # Gave Locally',
    '        str(num_donations),  # Number of Donations',
    '        f"${largest_year_amount:.2f}",  # Largest Year Amount',
    '        f"${last_year_amount:.2f}",  # Last Year Amount',
    '        occupation,',
    '        employer,',
    '        notes',
    '    ])',
    '',
    'print(f"Generated {len(upload_rows)} rows for upload")',
    '',
    '# Sort by weighted score (highest first)',
    'print("\\nSorting by weighted score...")',
    'def get_weighted_value(row):',
    '    try:',
    '        # Extract numeric value from "$123.45" format (index 8)',
    '        weighted_str = str(row[8]).replace("$", "").replace(",", "")',
    '        return float(weighted_str)',
    '    except:',
    '        return 0.0',
    '',
    'upload_rows.sort(key=get_weighted_value, reverse=True)',
    'print(f"Sorted {len(upload_rows)} rows by weighted score")',
    '',
    '# Post results in batches to avoid HTTP 500 errors',
    'print("\\nPosting results to Google Sheets...")',
    'batch_size = 1000',
    'total_rows = len(upload_rows)',
    'num_batches = (total_rows + batch_size - 1) // batch_size  # Ceiling division',
    '',
    'for batch_num in range(num_batches):',
    '    start_idx = batch_num * batch_size',
    '    end_idx = min(start_idx + batch_size, total_rows)',
    '    batch = upload_rows[start_idx:end_idx]',
    '    ',
    '    print(f"  Posting batch {batch_num + 1}/{num_batches} ({len(batch)} rows)...")',
    '    payload = {"upload_rows": batch, "batch_num": batch_num + 1, "total_batches": num_batches}',
    '    response = http_post(args.result, payload)',
    '    print(f"  Batch {batch_num + 1} posted successfully")',
    '',
    'print(f"\\nDone! Posted {total_rows} rows in {num_batches} batch(es). Check the CD_To_Upload sheet.")',
  ].join('\n');

  return ContentService.createTextOutput(py).setMimeType(ContentService.MimeType.TEXT);
}
